\documentclass[letter]{book}

%%%%%% Import Package %%%%%%
\usepackage{graphicx}
\usepackage[unicode]{hyperref}
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{indentfirst}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fontspec,xunicode,xltxtra}
\usepackage{xeCJK}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{epigraph}
\usepackage{amsmath}
\usepackage[xindy]{glossaries}
\usepackage{fancyhdr}
\usepackage{amsmath}
%\usepackage{TikZ}
\usepackage{ifthen}
\usepackage{longtable}

%Figure显示为中文的“图”
\renewcommand\figurename{图}

\setcounter{tocdepth}{4}

\renewcommand\thesection{\arabic{section}} 

%%%% 下面的命令设置行间距与段落间距 %%%%
\linespread{1.4}
% \setlength{\parskip}{1ex}
\setlength{\parskip}{0.5\baselineskip}

\lstdefinelanguage{JavaScript}{
	keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
	keywordstyle=\color{blue}\bfseries,
	ndkeywords={class, export, boolean, throw, implements, import, this},
	ndkeywordstyle=\color{darkgray}\bfseries,
	identifierstyle=\color{black},
	sensitive=false,
	comment=[l]{//},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{purple}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	morestring=[b]',
	morestring=[b]"
}

%Set Code Format%
\lstloadlanguages{C, csh, make,python,Java,JavaScript}
\lstset{	  
	alsolanguage= XML,  
	tabsize=4, %  
	frame=shadowbox, %把代码用带有阴影的框圈起来  
	commentstyle=\color{red!50!green!50!blue!50},%浅灰色的注释
	frameround=tttt,  
	rulesepcolor=\color{red!20!green!20!blue!20},%代码块边框为淡青色  
	keywordstyle=\color{blue!90}\bfseries, %代码关键字的颜色为蓝色，粗体  
	showstringspaces=false,%不显示代码字符串中间的空格标记  
	stringstyle=\ttfamily, % 代码字符串的特殊格式  
	keepspaces=true, %  
	breakindent=22pt, %  
	numbers=left,%左侧显示行号 往左靠,还可以为right，或none，即不加行号  
	stepnumber=1,%若设置为2，则显示行号为1,3,5，即stepnumber为公差,默认stepnumber=1  
	%numberstyle=\tiny, %行号字体用小号  
	numberstyle={\color[RGB]{0,192,192}\tiny} ,%设置行号的大小，大小有tiny,scriptsize,footnotesize,small,normalsize,large等  
	numbersep=8pt,  %设置行号与代码的距离，默认是5pt  
	basicstyle=\footnotesize, % 这句设置代码的大小  
	showspaces=false, % 
	escapechar=`,
	flexiblecolumns=true, %  
	breaklines=true, %对过长的代码自动换行  
	breakautoindent=true,%  
	breakindent=4em, %  	   
	aboveskip=1em, %代码块边框  
	tabsize=4,  
	showstringspaces=false, %不显示字符串中的空格  
	backgroundcolor=\color[RGB]{245,245,244},   %代码背景色  
	%backgroundcolor=\color[rgb]{0.91,0.91,0.91}    %添加背景色  
	escapeinside={``}{\_},  %在``里显示中文  
	%% added by http://bbs.ctex.org/viewthread.php?tid=53451  
	fontadjust,  
	captionpos=t,  
	framextopmargin=2pt,framexbottommargin=2pt,abovecaptionskip=-3pt,belowcaptionskip=3pt,  
	xleftmargin=4em,xrightmargin=4em, % 设定listing左右的空白  
	texcl=true
}

\graphicspath{{./Image/common/}{./Image/Api/}{./Image/InterfaceDesign/}{./Image/Attachment/}}

\begin{document}

\tableofcontents


\include{title}

\part{Framework}

\chapter{Spring}

\section{Common Sense}

\subsection{classpath}

理解classpath花了不短的时间，classpath到底是哪个目录？配置文件中指定的classpath程序到底在哪里找的？都困扰着我。其实有很好的方法来深深的理解classpath，如下代码片段打印出来当前项目在classpath中的文件。

\begin{lstlisting}[language=Java]
public class Application {
	public static void main(String[] args) {
		ClassLoader classLoader = ClassLoader.getSystemClassLoader();
			URL[] urls = ((URLClassLoader) classLoader).getURLs();
			for (URL url : urls) {
			System.out.println(url.getFile());
		}
	}
}
\end{lstlisting}

从代码的输出即可看出，classpath具体是哪些文件夹。

\begin{lstlisting}
# 与JDK相关的文件夹
/home/local/jdk1.8.0_111/jre/lib
/home/local/jdk1.8.0_111/jre/lib/ext
/home/local/jdk1.8.0_111/lib
# 与项目相关的文件夹，这里仅仅是一个module
/home/hldev/hldata/backend/credit-system/cc-data/build/classes/main/
/home/hldev/hldata/backend/credit-system/cc-data/build/resources/main/
# 与Gradle相关的文件夹
/home/hldev/.gradle/caches
\end{lstlisting}

所以在项目中看到如下的配置就不难理解了：

\begin{lstlisting}[language=Java]
bean.setMapperLocations(resolver.getResources("classpath:/mybatis/*.xml"));
\end{lstlisting}
它实际上就是映射的如图\ref{fig:classpathfile}中所示的XML文件。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{classpathfile.png}
	\caption{MyBatis映射classpath中的文件}
	\label{fig:classpathfile}
\end{figure}

classpath是指编译过后的WEB-INF文件夹下的的classes目录。配置文件一般放在 resources 目录下, 当编译之后会自动复制到 classes 目录下。classpath*:的使用是为了多个component(最终发布成不同的jar包)并行开发,各自的bean定义文件按照一定的规则:package+filename,而使用这些component的调用者可以把这些文件都加载进来。不仅包含 classes 路径，还包括 jar 文件 classes 路径进行查找。classpath:只能加载找到的第一个文件。然而，使用classpath*:需要遍历所有的classpath，加载速度很慢，因此您应该尽量避免使用classpath*。有时提示找不到资源文件，首先检查资源文件夹的名称，资源文件夹的名称默认为resources，注意有一个s，其次是要检查此文件夹是否定义成了资源文件夹，资源文件夹与普通文件夹不同，资源文件夹有一个层叠的标识，如图\ref{fig:javaresourcefolder}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{javaresourcefolder.png}
	\caption{Intellij Idea中的资源文件夹}
	\label{fig:javaresourcefolder}
\end{figure}

仅仅是建立一个resources文件夹是不够的，还需要将之定义成资源文件夹，在文件夹上右键，选择Mark Directionary As，将之定义为资源文件夹。定义为资源文件夹后，即可通过如下方式指定配置文件的位置：

\begin{lstlisting}[language=Java]
@Configuration
@Data
@Component
@PropertySource("classpath:application.properties")
@ConfigurationProperties(prefix = "spring.datasource")
public class DataSourceConfig {

	private String jdbcUrl;
	
	private String username;
	
	private String driverClassName;
	
	private String password;
}
\end{lstlisting}

在Intellij Idea中，如果要把配置文件加入classpath，可以将配置文件标记为资源文件。程序启动命令：

\begin{lstlisting}[language=Bash]
java -jar -Xmx2g credit-system-web-boot-1.1.28.jar --spring.config.location=application-jenkins.properties
\end{lstlisting}

启动的同时启动远程调试：

\begin{lstlisting}[language=Bash]
java -Xmx8192M -Xms4096M -jar -Xdebug -Xrunjdwp:transport=dt_socket,suspend=n,server=y,address=5005 /home/app/backend/credit-system-web-boot-1.1.28.jar --spring.config.location=application-jenkins.properties
\end{lstlisting}



\subsection{Spring Boot开发者工具}

spring-boot-devtools为应用提供一些开发时特性，包括默认值设置，自动重启，livereload等。在build.gradle文件里添加开发工具配置：

\begin{lstlisting}[language=Bash]
compile "org.springframework.boot:spring-boot-devtools"
\end{lstlisting}

在激活了开发者工具后，Classpath里对文件做任何修改都会触发应用程序重启。为了让重启速度够快，不会修改的类（比如第三方JAR文件里的类）都加载到了基础类加载器里，而应用程序的代码则会加载到一个单独的重启类加载器里。检测到变更时，只有重启类加载器重启。在Intellij Idea中，需要在设置中配置Compiler，勾选上build project automatically选项。在Mac中，按下 command + alt + shift + / 出现的界面中，选择Registry。找到 “compiler.automake.allow.when.app.running” 这个选项，并且勾选。遗憾的是在自己的Mac中，Intellij Idea按照如上配置后还是不能正常自动Reload，不过当在点击了Reload Changed Classes之后，Spring Boot会自动重启，可以配置Reload Changed Classes的快捷键来实现。

\subsection{请求消息}

在获取请求消息记录日志时，获取到的汉字乱码。获取请求参数的语句为：

\begin{lstlisting}[language=Java]
# 获取未经过解码的参数（汉字会显示乱码）
String queryString = request.getQueryString();
# 获取经过解码的参数
String qymc = request.getParameter("qymc");
\end{lstlisting}

使用request.getParameter方式获得的参数是已经经过web服务器解码的。使用request.getQueryString可以获得未解码的原始参数。对于tomcat解码造成的乱码问题可以通过2种途径解决，修改tomcat配置文件设置解码方式，服务器端对于获取到的参数进行new String(param.getBytes("ISO-8859-1"),"页面指定编码")转换，如下代码片段所示：

\begin{lstlisting}[language=Java]
# 将获取到的消息解码
String decodedQueryString=new String(queryString.getBytes("ISO-8859-1"),"UTF-8");
\end{lstlisting}


\subsection{返回消息封装}

一个完整的标准化服务描述性语言SOAP（Simple Object Access Protocol，简单对象访问协议）在某些环境下表现的还不错。尽管如此，SOAP产品的性能开销很大，它是一个巨大的性能杀手。在编写Restful接口时，需要将返回的内容用标准格式进行封装。通过编写统一的返回实体来实现:

\begin{lstlisting}[language=Java]
@RequestMapping("books")
public RestApiResponse<List<Book>> getUserShelfBooks(long userId){
	List<Book> userShelfBooks = bookShelfComposite.getUserShelfBooks(userId);
	return new RestApiResponse<>(ResponseCode.REQUEST_SUCCESS_MESSAGE, ResponseCode.REQUEST_SUCCESS, userShelfBooks);
}
\end{lstlisting}


\subsection{接口返回}

接口返回的格式如图所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{restresponse.png}
	\caption{Restful接口返回数据示例}
	\label{fig:restresponse}
\end{figure}

其中，message可能缩写为mesg、msg等，为了避免歧义，没有使用缩写，也没有命名为errorMessage，是返回时不仅仅只有错误消息。针对不同的接口返回不同的字段，可以使用MapperFacade的Mapper方法进行映射，如下代码片段所示：

\begin{lstlisting}[language=Java]
@Autowired
private MapperFacade corporationMapperFacade;

public XydmResponse mapToCorporationResponse(Corporation corporation) {
	return corporationMapperFacade.map(corporation, XydmResponse.class);
}
\end{lstlisting}

\subsubsection{获取Response返回内容}

在拦截器中记录日志时，有时需要获取HTTP Response中的内容，从图\ref{fig:outputchunkcontent}可以看出，只要获取到outputChunk的内容即可。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{outputchunkcontent.png}
	\caption{获取HTTP Response返回的内容}
	\label{fig:outputchunkcontent}
\end{figure}


可以通过反射来实现：

\begin{lstlisting}[language=Java]
public String getResponseContent(HttpServletResponse response) throws IOException, NoSuchFieldException, IllegalAccessException {
	String responseContent = null;
	CoyoteOutputStream outputStream = (CoyoteOutputStream) response.getOutputStream();
	Class<CoyoteOutputStream> coyoteOutputStreamClass = CoyoteOutputStream.class;
	Field obField = coyoteOutputStreamClass.getDeclaredField("ob");
	if (obField.getType().toString().endsWith("OutputBuffer")) {
		obField.setAccessible(true);
		OutputBuffer outputBuffer = (OutputBuffer) obField.get(outputStream);
		Class<OutputBuffer> opb = OutputBuffer.class;
		Field outputChunkField = opb.getDeclaredField("outputChunk");
		outputChunkField.setAccessible(true);
		if (outputChunkField.getType().toString().endsWith("ByteChunk")) {
			// 取到byte流
			ByteChunk bc = (ByteChunk) outputChunkField.get(outputBuffer);
			// 最终的值
			responseContent = new String(bc.getBytes(), "UTF-8");
		}
	}
	return responseContent;
}
\end{lstlisting}

OutputBuffer的命名空间为：org.apache.catalina.connector。

\subsection{Code Snippets}

有时想测试某段代码的运行效果，代码足够短也不太需要使用IDE来运行，相当于一个代码草稿本类似的功能。那么此时可以使用Visual Studio Code中使用Code Runner插件，此插件可以支持多种语言，测试草稿代码，如图\ref{fig:visualstudiocodesnippet}所示，想要查看对一个空字符串做toString()操作会有什么效果：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{visualstudiocodesnippet.png}
	\caption{Visual Studio Code代码草稿}
	\label{fig:visualstudiocodesnippet}
\end{figure}

ArrayList分页：

\begin{lstlisting}[language=Java]
return new Page<>(pageable, menuList.subList((page - 1) * size, page * size));
\end{lstlisting}

Optional避免空指针：

\begin{lstlisting}[language=Java]
public static String getChampionName(Competition comp) throws IllegalArgumentException {
	return Optional.ofNullable(comp)
			.map(c->c.getResult())
			.map(r->r.getChampion())
			.map(u->u.getName())
			.orElseThrow(()->new IllegalArgumentException("The value of param comp isn't available."));
}
\end{lstlisting}

Optional参数验证：

\begin{lstlisting}[language=Java]
public void setName(String name) throws IllegalArgumentException {
	this.name = Optional.ofNullable(name).filter(User::isNameValid)
						.orElseThrow(()->new IllegalArgumentException("Invalid username."));
}
\end{lstlisting}

\subsection{集合操作(Collection Operate)}

List移除空元素，移除重复元素：

\begin{lstlisting}[language=Java]
request.getParamList().stream().filter(StringUtils::isNotEmpty).distinct().forEach(param -> {
if (param.length() == 18 && CommonUtil.isLetterOrNumber(param)) {

});
\end{lstlisting}

对象做非空判断：

\begin{lstlisting}[language=Java]
f (!"".equals(name)) {
	//将""写在前头，这样，不管name是否为null，都不会出错
}

if (StringUtils.isNotBlank((String) pageable.getParams().get("qymc"))) {
	//用String强制转换也不会报空指针异常
} 
\end{lstlisting}

获取Json单个值：

\begin{lstlisting}[language=Java]
JSONObject json = new JSONObject(responseContent);
String total = json.getString("totalElementsCondition1");
\end{lstlisting}

去除List元素中的特殊字符：

\begin{lstlisting}[language=Java]
List<AccessLog> logs = operationLogs.stream()
						.peek(log -> log.setQueryResult(log.getQueryResult().replaceAll("\u0000", "")))
						.collect(Collectors.toList());
\end{lstlisting}

peek()-->对每个遇到的元素执行一些操作。如上方法会遍历List列表operationLogs中的每个元素，将特殊字符\textbackslash u0000去掉。

\paragraph{数组转换为ArrayList}

数组转List有如下方式：

\begin{lstlisting}[language=Java]
Integer[] integers = new Integer[] {1,2,3,4,5};
// Cannot modify returned list
List<Integer> list21 =  Arrays.asList(integers); 
// good
List<Integer> list22 = new ArrayList<>(Arrays.asList(integers)); 
//Java 8
List<Integer> list23 = Arrays.stream(integers).collect(Collectors.toList()); 
\end{lstlisting}

采用Arrays.asList转时，返回的List不能修改，因为asList()返回的列表是由原始数组支持的固定大小的列表。这种情况下，如果添加或删除列表中的元素，程序会抛出异常UnsupportedOperationException。


\subsection{jps}

jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令。jdk中的jps命令可以显示当前运行的java进程以及相关参数，它的实现机制如下：

java程序在启动以后，会在java.io.tmpdir指定的目录下，就是临时文件夹里，生成一个类似于hsperfdata\_User的文件夹，这个文件夹里（在Linux中为/tmp/hsperfdata\_\{userName\}/），有几个文件，名字就是java进程的pid，因此列出当前运行的java进程，只是把这个目录里的文件名列一下而已。 至于系统的参数什么，就可以解析这几个文件获得。hsperfdata的含义就是HotSpot Performance Data。

\begin{lstlisting}[language=Bash]
nohup java -jar -Xmx2g credit-system-web-boot-1.0.0.jar --spring.config.location=application-jenkins.properties &
\end{lstlisting}

\section{注解}

\subsection{ComponentScan}

\section{Shiro}

\subsection{新建表}



\section{数据库操作}

\subsection{Could not get JDBC connection}

接口有时会查询不出数据，总是报超时错误。查看程序日志，提示Could not get JDBC Connection。查看了数据库的Session数量(40)远远未到达最大限制数量(500)。

\begin{lstlisting}[language=SQL]
#达梦DB查看最大会话数量
select *
from v$dm_ini
where para_name ='MAX_SESSIONS'

#达梦DB查看当前会话数量
SELECT count(*)
FROM v$sessions;
\end{lstlisting}

经过分析，是由于连接池的默认设置数量问题，由于配置中对HikariCP的连接池数量未做指定，HikariCP默认的连接池数量(maximumPoolSize)是10，正是由于连接池数量较小，很容易出现获取不到连接而出现超时的现象。This property controls the maximum size that the pool is allowed to reach, including both idle and in-use connections. Basically this value will determine the maximum number of actual connections to the database backend. A reasonable value for this is best determined by your execution environment. When the pool reaches this size, and no idle connections are available, calls to getConnection() will block for up to connectionTimeout milliseconds before timing out. Default: 10.如下配置HikariCP的连接池数量：

\begin{lstlisting}[language=Bash]
#
# Pool数太少会出现连接超时,Hikari默认的大小是10
# 用户数200-3000连接池目前设置为200
# 调大时综合服务器内存、CPU负荷进行考虑
# 数据库目前最大Session数量为500
#
spring.datasource.maximumPoolSize=200
\end{lstlisting}


\subsection*{JDBC数据库重连}

在数据库重启后，程序无法重新连接到数据库。程序使用的是Hikari，找到了如下属性：

\begin{lstlisting}[language=Bash]
# 使用Hikari pool时，是否允许连接池暂停，默认为: false
spring.datasource.allow-pool-suspension=true
\end{lstlisting}

可以将连接池到c3p0，c3p0连接池本身具有数据库重连机制。目前常见到连接池如下表所示：

\begin{tabular}{|c|c|p{7cm}|}
	\hline
	\multirow{1}{*}{名称}
	%\multirow{1}{c|}{名称}  
	& \multicolumn{1}{c|}{协议} 
	& \multicolumn{1}{c|}{备注}\\			
	\cline{1-3}
	c3p0 &  LGPL v.2.1  & C3P0是一个开源数据连接池，Hibernate3.0默认自带的数据连接池，性能比较稳定。\\
	\hline
	HikariCP & Apache 2.0 & Fast, simple, reliable. HikariCP is a "zero-overhead" production ready JDBC connection pool. At roughly 130Kb, the library is very light. \\
	\hline
	Druid & Apache 2.0 & Druid是Java语言中最好的数据库连接池。Druid能够提供强大的监控和扩展功能。 \\
	\hline
	DBCP & Apache 2.0 & DBCP(DataBase connection pool),是 apache 上的一个 java 连接池项目，也是 tomcat 使用的连接池组件。单独使用dbcp需要2个包：commons-dbcp.jar,commons-pool.jar \\
	\hline
	BoneCP & Apache 2.0 & 使用HikariCP替代\\
	\hline
\end{tabular}

Tomcat 在 7.0 以前的版本都是使用 commons-dbcp 做为连接池的实现，但是 dbcp 饱受诟病，原因有：

\begin{itemize}
	\item{dbcp 是单线程的，为了保证线程安全会锁整个连接池}
	\item{dbcp 性能不佳}
	\item{dbcp 太复杂，超过 60 个类}
\end{itemize}

dbcp 使用静态接口，在 JDK 1.6 编译有问题
dbcp 发展滞后
因此很多人会选择一些第三方的连接池组件，例如 c3p0 , bonecp, druid。为此，Tomcat 从 7.0 开始引入一个新的模块：Tomcat jdbc pool。登陆服务器，使用命令查看正常的数据库连接：

\begin{lstlisting}[language=Bash]
lsof -i:5236
\end{lstlisting}

输出的结果如下：

\begin{lstlisting}
COMMAND   PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
java    20026   hl   38u  IPv4 2197171      0t0  TCP localhost:58220->localhost:padl2sim (ESTABLISHED)
java    20026   hl   42u  IPv4 2197527      0t0  TCP localhost:58222->localhost:padl2sim (ESTABLISHED)
java    20026   hl   43u  IPv4 2197176      0t0  TCP localhost:58224->localhost:padl2sim (ESTABLISHED)
java    20026   hl   44u  IPv4 2197177      0t0  TCP localhost:58226->localhost:padl2sim (ESTABLISHED)
java    20026   hl   45u  IPv4 2197178      0t0  TCP localhost:58228->localhost:padl2sim (ESTABLISHED)
java    20026   hl   46u  IPv4 2197179      0t0  TCP localhost:58230->localhost:padl2sim (ESTABLISHED)
java    20026   hl   47u  IPv4 2197180      0t0  TCP localhost:58232->localhost:padl2sim (ESTABLISHED)
java    20026   hl   48u  IPv4 2197181      0t0  TCP localhost:58234->localhost:padl2sim (ESTABLISHED)
java    20026   hl   49u  IPv4 2197182      0t0  TCP localhost:58236->localhost:padl2sim (ESTABLISHED)
java    20026   hl   50u  IPv4 2197536      0t0  TCP localhost:58238->localhost:padl2sim (ESTABLISHED)
\end{lstlisting}

在程序启动时，并没有真正建立TCP连接。只有真正的接受到请求查询数据库时，程序才与数据库建立TCP连接。此处建立了10个TCP连接。而Hikari默认的最大连接池数量也为10个。如下配置设启连接池连接池时，初始建立的连接数量为5个：

\begin{lstlisting}
spring.datasource.initial-size=5
\end{lstlisting}

在Spring中配置数据源类型如下：

\begin{lstlisting}
# 指定数据源类型
spring.datasource.type=com.zaxxer.hikari.HikariDataSource
\end{lstlisting}

Hikari默认的数据源默认的连接池数量为10个，默认的数量在HikariConfig类中进行的设置。为什么修改Spring的Initial Size数量会影响Hikari的连接数量呢？如果不配置数据源，Spring Boot默认的数据源是：

\begin{lstlisting}
org.apache.tomcat.jdbc.pool.DataSource
\end{lstlisting}

但是在实际开发中，可能需要使用自己是熟悉的数据源或者其他性能比较高的数据源。此时就可以通过制定Spring的数据源类型来实现。

停止数据库：

\begin{lstlisting}[language=Bash]
nohup sudo /opt/dmdbms/bin/dmserver /opt/dmdbms/data/DAMENG/dm.ini -noconsole &
\end{lstlisting}

在反复研究后发现，程序并未使用HikariCP连接池，而是使用的Spring JDBC连接池，所以在初始化Datasource时指定连接池，如下代码片段所示：

\begin{lstlisting}[language=Java]
@Configuration
@Data
@ConfigurationProperties(prefix = "spring.datasource")
public class DataSourceConfig {

	private String jdbcUrl;
	
	private String username;
	
	private String driverClassName;
	
	private String password;
	
	@Bean
	@Primary
	public DataSource primaryDataSource() {
		HikariConfig hikariConfig = new HikariConfig();
		hikariConfig.setDriverClassName(driverClassName);
		hikariConfig.setJdbcUrl(jdbcUrl);
		hikariConfig.setUsername(username);
		hikariConfig.setPassword(password);
		hikariConfig.setMaximumPoolSize(5);
		hikariConfig.setConnectionTestQuery("SELECT 1");
		hikariConfig.setPoolName("springHikariCP");
		hikariConfig.addDataSourceProperty("dataSource.cachePrepStmts", "true");
		hikariConfig.addDataSourceProperty("dataSource.prepStmtCacheSize", "250");
		hikariConfig.addDataSourceProperty("dataSource.prepStmtCacheSqlLimit", "2048");
		hikariConfig.addDataSourceProperty("dataSource.useServerPrepStmts", "true");
		HikariDataSource dataSource = new HikariDataSource(hikariConfig);
		return dataSource;
	}
}
\end{lstlisting}

\subsection{HikariCP数据库连接}

在配置HikariCP数据库连接时，总是提示如下错误：

\begin{lstlisting}
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.zaxxer.hikari.HikariDataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.NullPointerException
\end{lstlisting}

经过跟踪，发现是url为空导致的这个问题，后来增加了url的配置才得以成功连接数据库。所以除了要定义jdbc-url外，还需要定义url。经过查看HikariCP的文档，属性jdbc-url将使 HikariCP 使用“基于驱动管理器”（DriverManager-based）的配置。一般情况下，基于数据源（DataSource-based）的配置是更好的选择。但对许多部署实例来讲却也区别不大。当使用jdbc-url来配置“旧”的 JDBC 驱动时，你可能也需要设置 driverClassName 属性，但可以试一试不设置是否能行得通。所以，最终连接数据库的配置如下：

\begin{lstlisting}
spring.datasource.hikari.jdbc-url=jdbc:mysql://10.0.0.14:3306/dolphin
spring.datasource.hikari.url=jdbc:mysql://10.0.0.14:3306/dolphin
spring.datasource.hikari.username=root
spring.datasource.hikari.password=dolphin
spring.datasource.hikari.type=com.zaxxer.hikari.HikariDataSource
spring.datasource.hikari.driver-class-name=com.mysql.jdbc.Driver
\end{lstlisting}


\paragraph{DataSource跟DriverManager区别及联系}

In basic terms, a data source is a facility for storing data. It can be as sophisticated as a complex database for a large corporation or as simple as a file with rows and columns. A data source can reside on a remote server, or it can be on a local desktop machine. Applications access a data source using a connection, and aDataSource object can be thought of as a factory for connections to the particular data source that theDataSource instance represents.

两者相同点：
Using a DataSource object is the preferred alternative to using the DriverManager for establishing a connection to a data source. They are similar to the extent that the DriverManager class and DataSource interface both have methods for creating a connection, methods for getting and setting a timeout limit for making a connection, and methods for getting and setting a stream for logging.

两者不同点：
1.Unlike the DriverManager, a DataSource object has properties that identify and describe the data source it represents. Also, a DataSource object works with a JavaTM Naming and Directory InterfaceTM (JNDI) naming service and is created, deployed, and managed separately from the applications that use it. A driver vendor will provide a class that is a basic implementation of the DataSource interface as part of its JDBC 2.0 or 3.0 driver product.

2.The second major advantage is that the DataSource facility allows developers to implement a DataSourceclass to take advantage of features like connection pooling and distributed transactions.

传统的数据库连接方式（指通过DriverManager和基本实现DataSource进行连接）中，一个数据库连接对象均对应一个物理数据库连接，数据库连接的建立以及关闭对系统而言是耗费系统资源的操作，在多层结构的应用程序环境中这种耗费资源的动作对系统的性能影响尤为明显。

在多层结构的应用程序中通过连接池（connection pooling）技术可以使系统的性能明显得到提到，连接池意味着当应用程序需要调用一个数据库连接的时，数据库相关的接口通过返回一个通过重用数据库连接来代替重新创建一个数据库连接。通过这种方式，应用程序可以减少对数据库连接操作，尤其在多层环境中多个客户端可以通过共享少量的物理数据库连接来满足系统需求。通过连接池技术Java应用程序不仅可以提高系统性能同时也为系统提高了可测量性。

数据库连接池是运行在后台的而且应用程序的编码没有任何的影响。此中状况存在的前提是应用程序必须通过DataSource对象（一个实现javax.sql.DataSource接口的实例）的方式代替原有通过DriverManager类来获得数据库连接的方式。一个实现javax.sql.DataSource接口的类可以支持也可以不支持数据库连接池，但是两者获得数据库连接的代码基本是相同的。

\subsection{Spring Boot缓存}

SpringBoot支持很多种缓存方式：redis、guava、ehcahe、jcache等等。出现At least one cache should be provided per cache operation错误时，表示没有指定Cache缓存框架,需要在XML中配置。EhCache 是一个纯Java的进程内缓存框架，具有快速、精干等特点，是Hibernate中默认的CacheProvider。ehcache提供了多种缓存策略，主要分为内存和磁盘两级，所以无需担心容量问题。

@Cacheable : Spring在每次执行前都会检查Cache中是否存在相同key的缓存元素，如果存在就不再执行该方法，而是直接从缓存中获取结果进行返回，否则才会执行并将返回结果存入指定的缓存中。 
 
@CacheEvict : 清除缓存。 

@CachePut : @CachePut也可以声明一个方法支持缓存功能。使用@CachePut标注的方法在执行前不会去检查缓存中是否存在之前执行过的结果，而是每次都会执行该方法，并将执行结果以键值对的形式存入指定的缓存中。 

这三个方法中都有两个主要的属性：value 指的是 ehcache.xml 中的缓存策略空间；key 指的是缓存的标识，同时可以用 \# 来引用参数。
在ehcache.xml中配置缓存策略空间：

\begin{lstlisting}[language=XML]
<ehcache>
	<cache name="summaryXzxk" maxBytesLocalHeap="1m" timeToLiveSeconds="200"/>
</ehcache>
\end{lstlisting}

在方法上指定缓存，语法采用SpEL(Spring Express Language)，Spring表达式语言全称为Spring Expression Language(缩写为SpEL)，能在运行时构建复杂表达式、存取对象图属性、对象方法调用等等，并且能与Spring功能完美整合，如能用来配置Bean定义。 表达式语言给静态Java语言增加了动态功能。
SpEL是单独模块，只依赖于core模块，不依赖于其他模块，可以单独使用。：

\begin{lstlisting}[language=Java]
@Cacheable(value = "summaryXzxk",
			key = "'findPage'+#pageable.page+#pageable.params.get('xdr')",
			condition = "#pageable.params.get('xdr') eq null or #pageable.params.get('xdr') eq ''"
)
\end{lstlisting}

@Cacheable注解有三个参数，value是必须的，还有key和condition。第一个参数，也就是value指明了缓存将被存到什么地方。其中summaryXzxk为定义在配置文件中的缓存器名称。缓存的Key,当我们没有指定该属性时，Spring将使用默认策略生成key(表示使用方法的参数类型及参数值作为key),key属性是用来指定Spring缓存方法的返回结果时对应的key的。该属性支持SpringEL表达式。我们还可以自定义策略：自定义策略是指我们可以通过Spring的EL表达式来指定我们的key。这里的EL表达式可以使用方法参数及它们对应的属性。使用方法参数时我们可以直接使用“\#参数名”或者“\#p参数index”。key的生成策略有两种：一种是默认策略，一种是自定义策略。默认的key生成策略是通过KeyGenerator生成的，其默认策略如下：1.如果方法没有参数，则使用0作为key。2.如果只有一个参数的话则使用该参数作为key。3.如果参数多余一个的话则使用所有参数的hashCode作为key满足一定条件才缓存，写法如下：

\begin{lstlisting}[language=Java]
condition = "#pageable.params.get('xdr') eq null or #pageable.params.get('xdr') eq ''"
\end{lstlisting}

如上条件，取出xdr为空的数据进行缓存。如下情形缓存将会不起作用：

\begin{itemize}
	\item{同一个bean内部方法调用，这个问题遇到过，也是查了一些时间才发现}
	\item{子类调用父类中有缓存注解的方法}
\end{itemize}

\subsection{全局异常拦截器}

在class注解上@ControllerAdvice

\begin{lstlisting}[language=Java]
@ControllerAdvice(basePackages = "creditsystem.web.controllers")
public class CreditsystemControllerAdvice extends ResponseEntityExceptionHandler {

	@Override
	protected ResponseEntity<Object> handleExceptionInternal(Exception ex, Object body, HttpHeaders headers, HttpStatus status, WebRequest request) {
		if (HttpStatus.INTERNAL_SERVER_ERROR.equals(status)) {
			request.setAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE, ex, WebRequest.SCOPE_REQUEST);
		}
		logger.error("系统错误", ex);
		return new ResponseEntity<>(ApiResult.error(status.value(), "系统错误"), headers, HttpStatus.OK);
	}
}
\end{lstlisting}

\subsection{用户重复登录问题}

在服务端维护登录用户与其SessionID的对应关系。同一用户，后登陆者将前登陆者的SessionID替换，这样，前者如果再请求服务器资源，无法与新生成的Session ID匹配，此时服务端会返回一个错误码，前端捕获到此错误码，就会认为其未登录，要求重新登录。这样，即使出现断电等无法注销的情况，也会在他下次登陆时自动取代掉，同时也解决了用户登陆的唯一性问题。

\subsection{读取properties属性}

\subsubsection{读取自定义属性}

有时需要读取自定义属性，以配置HikariCP连接池为例，在application.properties文件指定指定配置：

\begin{lstlisting}
spring.datasource.hikari.jdbc-url=jdbc:dm://dn4:5236/DMSERVER
\end{lstlisting}

在类中获取配置相应的值，注意需要添加Data注解，否则需要手写get和set方法：

\begin{lstlisting}[language=Java]
@Configuration
@Data
@ConfigurationProperties(prefix = "spring.datasource.hikari")
public class DataSourceConfig {
	private String jdbcUrl;
}
\end{lstlisting}

由于此处注解是默认写在application.properties配置文件中，所以在ConfigurationProperties中可以不指定路径。否则需要使用locations指定配置文件路径。

\section{单元测试}

\subsection{Gradle中单元测试}

项目使用Gradle进行构建，写好单元测试类后，在Gradle中做如下配置:

\begin{lstlisting}[language=Java]
project(':cc-api') {	
	test{
		include('**/*Test.class')
	}
}
\end{lstlisting}

在每次构建时都会运行单元测试。Mock和Stub是两种测试代码功能的方法。Mock测重于对功能的模拟。Stub测重于对功能的测试重现。比如对于List接口，Mock会直接对List进行模拟，而Stub会新建一个实现了List的TestList，在其中编写测试的代码。强烈建议优先选择Mock方式，因为Mock方式下，模拟代码与测试代码放在一起，易读性好，而且扩展性、灵活性都比Stub好。

\subsection{TestNG单元测试}

项目经常涉及到修改，水平不够足，有些地方始终考虑的不够周到，会互相影响，所以考虑添加单元测试。系统前后端分离，分为各类接口，根据不同类的接口分组进行测试。在测试类上指定测试接口的分组：

\begin{lstlisting}[language=Java]
/**
* 组名是数组类型，，所以一个测试方法可以属于多个组
* group={"group name 1","group name 2"}
*/
@Test(groups = "xycq-api")
public void testGetXzcfList() throws Exception {
	TestPublicVariable.test("/xzcf?xdr=dolphin");
}
\end{lstlisting}

配置跑单元测试如图\ref{fig:testngunittestbygroup}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{testngunittestbygroup.png}
	\caption{TestNG单元测试}
	\label{fig:testngunittestbygroup}
\end{figure}

\subsection{Spring mockMvc测试}

MockMvc实现了对Http请求的模拟，能够直接使用网络的形式，转换到Controller的调用，这样可以使得测试速度快、不依赖网络环境，而且提供了一套验证的工具，这样可以使得请求的验证统一而且很方便。引入在Gradle构建脚本中增加Spring测试包:

\begin{lstlisting}
compile("org.springframework.boot:spring-boot-test:" + springBootVersion)
testCompile group: 'junit', name: 'junit', version: '4.12'
\end{lstlisting}

增加测试类：

\begin{lstlisting}[language=Java]
@RunWith(SpringRunner.class)
@SpringBootTest(classes = DolphinApplication.class)
@Commit
@Transactional
public class BookControllerTest {

	private MockMvc mockMvc;
	
	@Autowired
	private WebApplicationContext wac;
	
	@Before
	public void setUp() {
		this.mockMvc = webAppContextSetup(this.wac).build();
	}
	
	@Test
	public void getBooksByName() throws Exception {
		String responseString = mockMvc.perform(
					get("/api/book?name=小学生")
				).andExpect(status().isOk())    //返回的状态是200
				.andDo(print())         //打印出请求和相应的内容
				.andReturn().getResponse().getContentAsString(); //将相应的数据转换为字符串
		System.out.println("--------返回的json = " + responseString);
	}
}
\end{lstlisting}


\section{常见问题}

\paragraph{getAttribute: Session already invalidated}

当用户多次登录后，由于服务端缓存了所有的Session，在取出Session进行对比的时候，会出现此错误，此错误指示从Session中获取属性值的时候，Session已经无效。有两种可能导致Session无效：session timeout或者程序中调用了session.invalidate()方法。检查代码发现，在移除Session时的确使用了：

\begin{lstlisting}[language=Java]
#使Session无效的语句
getSessionMap().get(sessionId).invalidate();
#出错的语句
User user = (User) session.getAttribute("LOCAL_CLINET_USER");
\end{lstlisting}

当在无效的session中获取Attribute时，出现了上面的错误，解决办法就是不调用invalidate方法。

\paragraph{java.lang.NoClassDefFoundError: org/apache/juli/logging/LogFactory}

Tomcat 的内部日志使用 JULI 组件，这是一个 Apache Commons 日志的重命名的打包分支，默认被硬编码，使用 java.util.logging 架构。这能保证 Tomcat 内部日志与 Web 应用的日志保持独立，即使 Web 应用使用的是 Apache Commons Logging。在Gradle构建脚本中添加如下配置即可：

\begin{lstlisting}[language=Bash]
dependencies{
	compile group: 'org.apache.tomcat', name: 'tomcat-juli', version: property('tomcat.version')
}
\end{lstlisting}



\paragraph{Java.lang.NoClassDefFoundError: javax/transaction/TransactionManager}

\paragraph{NoClassDefFoundError}

连接时从内存找不到需要的class就出现NoClassDefFoundError，当Java虚拟机或 ClassLoader 实例试图在类的定义中加载（作为通常方法调用的一部分或者作为使用 new 表达式创建的新实例的一部分），但无法找到该类的定义时，抛出此异常。 

Java事务的类型有三种：JDBC事务、JTA(Java Transaction API)事务、容器事务。 常见的容器事务如Spring事务，容器事务主要是J2EE应用服务器提供的，容器事务大多是基于JTA完成，这是一个基于JNDI的，相当复杂的API实现。在本机请求会报此错误，但是部署到服务器上是OK的。在本地引入如下2个jar包即可修复此问题：

\begin{lstlisting}[language=Bash]
compile group: 'javax.transaction', name: 'jta', version: '1.1'
compile group: 'ma.glasnost.orika', name: 'orika-core', version: '1.5.1'
\end{lstlisting}

Orika是一个Java Bean映射框架递，递归复制数据从一个对象到另一个。对于开发多层应用程序时，非常有用。Java 事务编程接口（JTA：Java Transaction API）和 Java 事务服务 (JTS；Java Transaction Service) 为 J2EE 平台提供了分布式事务服务。分布式事务（Distributed Transaction）包括事务管理器（Transaction Manager）和一个或多个支持 XA 协议的资源管理器 ( Resource Manager )。我们可以将资源管理器看做任意类型的持久化数据存储；事务管理器承担着所有事务参与单元的协调与控制。JTA 事务有效的屏蔽了底层事务资源，使应用可以以透明的方式参入到事务处理中；但是与本地事务相比，XA 协议的系统开销大，在系统开发过程中应慎重考虑是否确实需要分布式事务。若确实需要分布式事务以协调多个事务资源，则应实现和配置所支持 XA 协议的事务资源，如 JMS、JDBC 数据库连接池等

\paragraph{Unregistering JMX-exposed beans on shutdown}

运行之后控制台输出“Unregistering JMX-exposed beans on shutdown”,原因为：SpringBoot内置Tomcat没有正常启动.原来是在引入时没有写版本号，在没有版本号的情况下此次引入其实是不成功的，难怪在类中始终不能够自动导入对应的命名空间。

\begin{lstlisting}[language=Bash]
compile('org.springframework.boot:spring-boot-starter-web:1.5.2.RELEASE')
\end{lstlisting}

忘记了写：1.5.2.RELEASE，其实这里的版本号可以省略(应该是不能省略才是)，但是目前还不清楚具体的省略机制，在什么情况下可以省略。

\paragraph{AES加密失败}

因为某些国家的进口管制限制，Java发布的运行环境包中的加解密有一定的限制。比如默认不允许256位密钥的AES加解密，解决方法就是修改策略文件。将下载的没有限制的文件替换掉原来的文件即可。替换的路径为：

\begin{lstlisting}[language=Bash]
/home/hl/local/jdk1.8.0_144/jre/lib/security/local_policy.jar
/home/hl/local/jdk1.8.0_144/jre/lib/security/US_export_policy.jar
\end{lstlisting}

由于信息安全在军事等方面极其重要，如在第二次世界大战期间，使用了无线电，若是能够成功解密敌方的机密情报，往往预示着战争的胜利，因此美国对加密解密等软件进行了出口限制，JDK中默认加密的密钥长度较短，加密强度较低，而UnlimitedJCEPolicyJDK7中的文件则没有这样的限制，因此为了获得更好的加密强度，需要替换掉那两个文件。如何知道文件是否已经替换成了允许256位加密的那个文件呢？只需要看文件的MD5即可，在Mac下：

\begin{lstlisting}[language=Bash]
# 切换到需要替换的文件所在目录
/Library/Java/JavaVirtualMachines/jdk1.8.0_112.jdk/Contents/Home/jre/lib/security/
# 查看文件的MD5编码
# 前提是需要知道替换后的那个文件的MD5编码
# 与当前生成的MD5编码的文件比对即可
MD5 local_policy.jar
# 在Ubuntu下使用此命令查看MD5
md5sum local_policy.jar
\end{lstlisting}




\section{Mybatis}

mybatis-spring-boot-starter 1.3.x版本对应Spring Boot 1.5.x版本。


\subsection{打印SQL}

开发中，配置日志中打印SQL语句，在application.properties文件中指定mybatis配置文件路径：

\begin{lstlisting}[language=Bash]
mybatis.config-location=classpath:mybatis-config.xml
\end{lstlisting}

在Mybatis配置文件mybatis-config.xml中，做如下配置：

\begin{lstlisting}[language=XML]
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE configuration
PUBLIC "-//mybatis.org//DTD Config 3.0//EN"
"http://mybatis.org/dtd/mybatis-3-config.dtd">
<configuration>
	<settings>
		<!-- 打印查询语句 -->
		<setting name="logImpl" value="STDOUT_LOGGING" />
	</settings>
</configuration>
\end{lstlisting}

打印SQL语句效果如图\ref{fig:mybatisprintsql}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{mybatisprintsql.png}
	\caption{MyBatis打印SQL}
	\label{fig:mybatisprintsql}
\end{figure}

\subsection{多参数传递}

在使用Mybatis的过程中，有时需要传递多个参数，例如既要传入机构ID列表,又要传入其他参数，那么此时Mapper如下：

\begin{lstlisting}[language=Java]
int batchUpdate(@Param("ids") List<String> ids,@Param("status") String status);
\end{lstlisting}

在XML中使用：

\begin{lstlisting}[language=XML]
<update id="batchUpdate" parameterType="java.util.List">
	UPDATE TD_S_ORG
	SET STATUS = #{status}
	WHERE ORG_ID IN
	<foreach collection="ids" item="item" index="index" open="(" separator="," close=")">
		#{item}
	</foreach>
</update>
\end{lstlisting}

\subsection{resultMap和resultType区别}

使用resultType进行输出映射，只有查询出来的列名和pojo中的属性名一致，该列才可以映射成功。如果查询出来的列名和pojo中的属性名全部不一致，没有创建pojo对象。只要查询出来的列名和pojo中的属性有一个一致，就会创建pojo对象。mybatis通过resultMap能帮助我们很好地进行高级映射。遇到一个问题是将分页的本应该写成resultMap的地方写成了resultType，在本地可以正常运行，但是打包成jar包放到服务器就无法运行了，提示org.apache.ibatis.type.TypeException: Could not resolve type alias 'AccessLog'：

\begin{lstlisting}[language=XML]
<!--这里的resultType应该写成resultMap-->
<select id="countAccessLog" parameterType="com.hualongdata.data.mybatis.Pageable" resultType="AccessLog">
</select>
\end{lstlisting}

如果改为resultMap又会出现如下错误：


\begin{lstlisting}[language=XML]
org.apache.ibatis.builder.IncompleteElementException: Could not find result map creditsystem.data.mapper.RedBalckLogMapper.RedBalckLog
\end{lstlisting}

出现这个错误的原因是在resultMap里面指定的是RedBalckLog(大写)，而Mapper中定义的是redBalckLog(小写)。这里resultMap指定的Bean需要和Mapper文件中定义的Bean的id一致。

\subsection{常见问题}

\paragraph{Invalid bound statement (not found)}

在配置MyBatis Mapper时，出现如下错误：

\begin{lstlisting}[language=Java]
Request processing failed; nested exception is org.apache.ibatis.binding.BindingException: Invalid bound statement (not found)
\end{lstlisting}

配置文件检查了没有问题，后来看网上说是在Intellij Idea中，需要在resource目录下新建一个文件夹，并设置成资源文件夹，放置Mapper.xml文件。还真能够解决问题，但是不明白为什么需要单独建立一个文件夹，按道理resource也是资源文件夹啊，应该可以读取才对。还有建立的文件夹也有讲究，文件夹的名称应该和放Mapper.java文件的名称一致，不能随意起名。问题虽然解决，但是其中原理始终没有弄个明白，最终文件夹结构如图\ref{fig:mapperreourcestructure}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{mapperreourcestructure.png}
	\caption{Mapper XML放置目录结构}
	\label{fig:mapperreourcestructure}
\end{figure}


\paragraph{Parameter 'keys' not found. Available parameters are [list]}

在使用MyBatis时，出现如下错误：

\begin{lstlisting}[language=Java]
org.apache.ibatis.binding.BindingException: Parameter 'studentNameList' not found. Available parameters are [list]
\end{lstlisting}

传递一个 List 实例或者数组作为参数对象传给 MyBatis。当你这么做的时 候,MyBatis 会自动将它包装在一个Map中,用名称在作为键。List 实例将会以“list” 作为键,而数组实例将会以“array”作为键。因为此时传的参数只有一个，而且传入的是一个List集合，所以mybatis会自动封装成Map<"list",keys>。在解析的时候会通过“list”作为Map的key值去寻找。但是我在xml中却声明成keys了，所以自然会报错找不到。解决办法就是修改Mapper.xml文件。或者将List封装到Map里面传递。第一种方式以后如果要扩展该方法，增加集合参数的时候，需要修改xml中的内容。

\newpage

\chapter{Scala}

\section{函数}

Scala有两种变量，val和var。val就不能再赋值了。与之对应的，var可以在它生命周期中被多次赋值。数据转换：

\begin{lstlisting}[language=Scala]
//字符串转整型
val errorCodeId: Int = singleErrorCode.toInt;
//split函数
val dstArray: Array[String] = errorCode.toString.split(',')
\end{lstlisting}


\begin{lstlisting}[language=Scala]
case class User(  
	id: Int,  
	firstName: String,  
	lastName: String,  
	age: Int,  
	gender: Option[String])  
	
	object UserRepository {  
	private val users = Map(1 -> User(1, "John", "Doe", 32, Some("male")),  
	2 -> User(2, "Johanna", "Doe", 30, None))  
	def findById(id: Int): Option[User] = users.get(id)  
	def findAll = users.values  
}  
\end{lstlisting}

\subsection{排序}

对Vector中的元素做倒序排序。

\begin{lstlisting}[language=Scala]
/**
* 对要排序的列做反向排序（reverse）
* 全文索引列必须要置于条件的开始
* 否则无法利用索引
* 行政许可XDR列有全文索引，需要排序在前
*/
val params = pageable.getParams.asScala.toVector.reverse  
\end{lstlisting}

\subsection{集合操作}

Scala集合操作如下：

\begin{lstlisting}[language=Scala]
processResult = errorCode.get.split(',')
	.filter(s => StringUtils.isNoneBlank(s))
	.map(code => etlErrorTypeService.findByIntegerId(code.toInt))
	.filter(_ != null)
	.map(eet => eet.getErrDesc)
	.mkString("；")
\end{lstlisting}

mkString把集合元素转化为字符串，可以添加分隔符，前缀，后缀。

\chapter{Python}

Python是FLOSS（自由/开放源码软件）之一。你可以自由地发布这个软件的拷贝、阅读它的源代码、对它做改动、把它的一部分用于新的自由软件中。Python希望看到一个更加优秀的人创造并经常改进。由于它的开源本质，Python已经被移植在许多平台上（经过改动使它能够工作在不同平台上）。如果你小心地避免使用依赖于系统的特性，那么你的所有Python程序无需修改就可以在下述任何平台上面运行。这些平台包括Linux、Windows、FreeBSD、Macintosh、Solaris、OS/2、Amiga、AROS、AS/400、BeOS、OS/390、z/OS、Palm OS、QNX、VMS、Psion、Acom RISC OS、VxWorks、PlayStation、Sharp Zaurus、Windows CE甚至还有PocketPC、Symbian以及Google基于linux开发的Android平台！

\subsection{基础}

查看Python已经安装了哪些模块，可以直接在Python环境中输入help('modules')命令。在Pycharm中设置Python路径在Preference->Project Interpreter中。

\begin{lstlisting}[language=Bash]
# 安装MySQL
sudo apt-get install mysql-server -y
# 安装pip
sudo apt-get install python-pip python-dev build-essential
# 安装MySQL驱动
sudo pip install mysql-python
# 安装beautifulsoup4
sudo pip install beautifulsoup4
# openpyxl来处理xml文件
sudo pip install openpyxl
# 安装Redis支持模块
sudo pip install redis
\end{lstlisting}

在Mac OS X上，提示No Module named requests，使用pip安装requests模块也无法解决，输入如下命令此问题消失：

\begin{lstlisting}[language=Bash]
sudo easy_install -U requests
\end{lstlisting}

pip 改善了不少 easy\_install 的缺點，如此说來 pip 應該是略勝一籌，不過它還不能完全取代對方，因为目前有很多套件還是得用 easy\_install 安裝。easy\_insall的作用和perl中的cpan，ruby中的gem类似，都提供了在线一键安装模块的傻瓜方便方式，而pip是easy\_install的改进版，提供更好的提示信息，删除package等功能。老版本的python中只有easy\_install，没有pip。

\subsection{Beautiful Soup}

Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树(parse tree)。 它提供简单又常用的导航（navigating），搜索以及修改剖析树的操作。Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。如下代码片段展示爬取豆瓣书籍详情页的书籍信息：

\begin{lstlisting}[language=Bash]
def get_single_book_detail_info(url):
	try:
		req = urllib2.Request(url, headers=hds[np.random.randint(0, len(hds))])
		source_code = urllib2.urlopen(req).read()
		plain_text = str(source_code)
	except (urllib2.HTTPError, urllib2.URLError), e:
		print e
	soup = BeautifulSoup(plain_text)
	list_soup = soup.find('div', {'id': 'info'}).findAll('span')
	return list_soup
\end{lstlisting}

find方法直接返回结果，findAll方法返回符合条件的结果。Python在使用前，可以不需要初始化变量，在外层可以直接使用内层函数的变量。

\subsection{爬取豆瓣书籍}

向数据库中保存数据：

\begin{lstlisting}[language=Python]
def save_to_mysql():
	try:
		conn = MySQLdb.connect(host='localhost', user='root', passwd='123456', port=3306)
		cur = conn.cursor()
		conn.select_db('dolphin')
		value = ['hi rollen']
		cur.execute('insert into book(name) values(%s)', value)
		conn.commit()
		cur.close()
		conn.close()
	
	except MySQLdb.Error, e:
		print "Mysql Error %d: %s" % (e.args[0], e.args[1])
\end{lstlisting}

在根据标签爬取的书籍列表中，信息不够全面，比如ISBN号码就没有。所以需要爬到每本书籍的详情页面。

\subsection{抓取技巧}

\paragraph{防止IP被封}在爬取豆瓣的页面时，如果一分钟请求超过一定次数(一说40次)，豆瓣会将请求的IP加入黑名单。所以一是爬取的频率不要太高，而是可以使用代理来爬取。服务端对IP访问对阈值也有限制，因为按照正常情况下，一个用户不会大批量对请求一个网站的，所以超过了规定的请求次数也会触发封锁。还可以让请求不那么有规律，让服务器无法通过请求的频率判断是机器还是人类，一般人类访问请求的频率是不固定的。

\paragraph{模仿浏览器}有些网站会检查你是不是真的浏览器访问，还是机器自动访问的。这种情况，加上User-Agent，表明你是浏览器访问即可。有时还会检查是否带Referer信息还会检查你的Referer是否合法，一般再加上Referer。在此次抓取的过程中，模仿浏览器：

\begin{lstlisting}[language=Python]
# Some User Agents
headers = [{'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}, \
{
'User-Agent': 'Mozilla/5.0 (Windows NT 6.2) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.12 Safari/535.11'}, \
{'User-Agent': 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)'}]
\end{lstlisting}

在HTTP请求头中，添加浏览器相关标识，让服务器认定为来自浏览器的请求。

\paragraph{避免重复抓取}豆瓣采用Restful风格来设计的API，最后的值就相当于每本书籍的唯一标识，在数据库中预先生成所有的ID集合，每抓取了一个ID的数据，都会更新ID的状态，表示此数据已经抓取。这样的方式来确保每本书籍只被抓取一次。

\paragraph{海量数据}

\subsection{读取Properties文件}

在项目中持续集成的Python部署脚本中，需要读取Java的Properties文件，读取的方法如下：

\begin{lstlisting}[language=Python]
def getVersion():
	#
	#动态从版本配置文件中读取当前最新版本号
	#
	file_path = VERSION_CONFIG_PATH
	props = parse(file_path)   #读取文件
	print('version:'+props.get('VERSION'))
	#根据key读取value
	return props.get('VERSION')            

def parse(file_name):
	return Properties(file_name)
\end{lstlisting}

Propertise类的代码如下：

\begin{lstlisting}[language=Python]
import re
import os
import tempfile

class Properties:

	def __init__(self, file_name):
		self.file_name = file_name
		self.properties = {}
		try:
		fopen = open(self.file_name, 'r')
		for line in fopen:
		line = line.strip()
		if line.find('=') > 0 and not line.startswith('#'):
		strs = line.split('=')
		self.properties[strs[0].strip()] = strs[1].strip()
		except Exception, e:
		raise e
		else:
		fopen.close()
	
	def has_key(self, key):
		return key in self.properties
	
	def get(self, key, default_value=''):
		if key in self.properties:
		return self.properties[key]
		return default_value
\end{lstlisting}


\subsection{sh: mysql\_config: command not found解决办法}

首先需要安装MySQL，安装完成后，打开终端，输入下方语句修改环境变量：

\begin{lstlisting}[language=Bash]
# 安装python-devel
sudo dnf install python-devel -y
# 解决fatal error: Python.h: No such file or directory
sudo dnf install python-devel --allowerasing
PATH="$PATH":/usr/local/mysql/bin
\end{lstlisting}

设置完成之后，输入如下语句安装MySQL Python：

\begin{lstlisting}[language=Bash]
sudo pip install mysql-python
\end{lstlisting}

\chapter{前端}

\section{React}

只有动画或视频达到60fps，人眼才不会感觉卡顿。

\subsection{组件生命周期(Life Cycle)}

页面的交互流程，页面加载或者用户操作界面，调用相关函数通过Ajax从服务端获取数据，触发dispatch，将action传递给reducer更改state(要赋给propss的值都存在state中)，然后再将state的新的值赋给props，重新渲染页面。connect中的函数当state改变时都会被执行，可以理解成subscribe函数或者监听函数。

\subsection{第一个React}

在Web开发中，我们总需要将变化的数据实时反应到UI上，这时就需要对DOM进行操作。而复杂或频繁的DOM操作通常是性能瓶颈产生的原因（如何进行高性能的复杂DOM操作通常是衡量一个前端开发人员技能的重要指标）。React为此引入了虚拟DOM（Virtual DOM）的机制：在浏览器端用Javascript实现了一套DOM API。基于React进行开发时所有的DOM构造都是通过虚拟DOM进行，每当数据变化时，React都会重新构建整个DOM树，然后React将当前整个DOM树和上一次的DOM树进行对比，得到DOM结构的区别，然后仅仅将需要变化的部分进行实际的浏览器DOM更新。如下定义一个最简单的React示例，首先写基本的一个程序入口的组件：

\begin{lstlisting}[language=Javascript]
import React from 'react';
import ReactDOM from 'react-dom';

ReactDOM.render(
	<div>Hello World!</div>,
	document.getElementById('root')
);
\end{lstlisting}

随后定义一个默认的HTML文件：

\begin{lstlisting}[language=HTML]
<body>
	<!-- 容器 -->
	<div id="root"></div>
	<!-- 引入编译好的js文件 -->
	<script type="text/javascript" src="./bundle.js"></script>
</body>
\end{lstlisting}

\subsection{React数据的流转过程}

通过Axios向服务端发送HTTP请求，获取到服务端返回端Json数据后，触发对应Type的dispatch(serviceCommon.js)去更新state。当 state 变化，或者 ownProps 变化的时候，mapStateToProps(connect方法的第一个参数)都会被调用，计算出一个新的 stateProps，（在与 ownProps merge 后）更新给对应的组件。


\subsection{规范}

React模块使用.jsx扩展名，如BookMain.jsx。对于JSX属性值总是使用双引号("), 其他均使用单引号。为什么? JSX属性 不能包括转译的引号, 因此在双引号里包括像 "don't" 的属性值更容易输入。HTML属性也是用双引号，所以JSX属性也遵循同样的语法。

\subsection{Router}

Web 服务并不会解析 hash，也就是说 \# 后的内容 Web 服务都会自动忽略，但是 JavaScript 是可以通过 window.location.hash 读取到的，读取到路径加以解析之后就可以响应不同路径的逻辑处理。history 是 HTML5 才有的新 API，可以用来操作浏览器的 session history (会话历史)。在HTML5的history API出现之前，前端的路由都是通过 hash 来实现的，hash 能兼容低版本的浏览器。React Router的设计思想来源于Ember的路由，如果原来有用过Ember的路由，那么应该对React Router不会陌生。React Router 4.0（以下简称 RR4） 已经正式发布，它遵循react的设计理念，即万物皆组件。所以 RR4 只是一堆提供了导航功能的组件（还有若干对象和方法），具有声明式（引入即用），可组合性的特点。添加一个Router组件如下：

\begin{lstlisting}[language=Javascript]
import React from "react";
import {BrowserRouter, Route} from 'react-router-dom'

const User = ({match}) => {
	return <h1>Hello {match.params.username}!</h1>
}

const routes = (
	<BrowserRouter>
		<Route path="/user/:username" component={User}/>
	</BrowserRouter>
);

export default routes;
\end{lstlisting}

<BrowserRouter>是一个使用了HTML5 history API的高阶路由组件，保证你的UI界面和URL保持同步。使用HTML5历史记录API（pushState，replaceState和popstate事件）的<Router>来保持UI与URL的同步。<Route> 也许是RR4中最重要的组件了。它最基本的职责就是当页面的访问地址与 Route 上的 path 匹配时，就渲染出对应的 UI 界面。<Route> 自带三个render method和三个props，props 分别是：match、location、history。所有的render method 无一例外都将被传入这些props。访问效果如图\ref{fig:firstroute}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{firstroute.png}
	\caption{React Route效果}
	\label{fig:firstroute}
\end{figure}

\subsection{Provider}

Provider方法会在react组件外边包一层名为provider的组件，使其所有子组件共享store提供的方法。connect方法生成容器组件以后，需要让容器组件拿到state对象，才能生成 UI 组件的参数。一种解决方法是将state对象作为参数，传入容器组件。但是，这样做比较麻烦，尤其是容器组件可能在很深的层级，一级级将state传下去就很麻烦。React-Redux 提供Provider组件，可以让容器组件拿到state。

\begin{lstlisting}[language=Javascript]
ReactDOM.render(
	<Provider store={store}>{routes}</Provider>,
	document.getElementById('root')
);
\end{lstlisting}

Provider在根组件外面包了一层，这样一来，App的所有子组件就默认都可以拿到state了。同理store只需要在最顶层导入一次，否则在下级的每一个组件都需要导入store，非常不利于组件复用，每个组件都导入显然不够优雅，它的原理是React组件的context属性。

\subsection{Connect}

React-Redux 提供connect方法，用于从 UI 组件生成容器组件。connect的意思，就是将这两种组件连起来。

\begin{lstlisting}[language=Javascript]
connect(mapStateToProps, mapDispatchToProps, mergeProps)(MyComponent)
\end{lstlisting}

项目中connect定义如下：

\begin{lstlisting}[language=Javascript]
/**
* mapStateToProps:这个函数的作用是确定哪些 Redux 全局的 state 是我们组件想要通过 props 获取
*/
const mapStateToProps = (state) => {
	return {
		book: state.book
	}
}

/**
* mapDispatchToProps:这个方法的作用是确定 哪些action创建函数是我们想要通过props获取
*/
const mapDispatchToProps = (dispatch) =>{
	return{
		bookService: bindActionCreators(bookService, dispatch)
	}
}

connect(mapStateToProps, mapDispatchToProps)
class BookContainer extends Component {
	render() {
		return React.cloneElement(this.props.children, {...this.props});
	}
}
\end{lstlisting}

\subsection{Actions}

Action向store派发指令，action 函数会返回一个带有 type 属性的 Javascript Plain Object，store将会根据不同的action.type来执行相应的方法。

\subsection{Store}

store 是应用的状态管理中心，保存着是应用的状态（state），当收到状态的更新时，会触发视觉组件进行更新。Provider是react-redux提供的组件，它的作用是把store和视图绑定在了一起，如下语句创建一个简单的store。

\begin{lstlisting}[language=Javascript]
import {createStore} from 'redux';
//创建store
const store = createStore(reducer, 1);
\end{lstlisting}

第一个参数为reducer，第二个参数是初始化的状态(initialState)，这里为1。

\subsection{Reducer}

如下语句创建一个简单的Reducer：

\begin{lstlisting}[language=Javascript]
const reducer = (state, action) => {
	switch (action.type) {
		case "ADD":
			state = state + action.payload;
			break;
		case "SUBTRACT":
			break;
	}
	return state;
};
\end{lstlisting}




\subsection{Dispatcher}

接收action，并且向注册的回调函数广播payloads。Facebook还提供了Dispatcher的开源库。你可以将Dispatcher认为是一种全局pub/sub系统的事件处理器，用于向所注册的回调函数广播payloads。通常情况下，如果你使用Flux架构，你可以借助于Facebook提供的这个Dispatcher实现，并且可以借助NodeJS中的EventEmitter模块来配置事件系统， 从而帮助管理应用程序的状态。Store中包括了注册给Dispatcher的回调函数。actionCreator还有很简洁的用法：对actionCreator做dispatch级别的封装，这个过程我们可以使用 redux 提供的 bindActionCreators 函数自动完成。然后就可以直接调用action，而不需要使用dispatch方法去调用actionCreator。dispatch调用如下代码片段所示。

\begin{lstlisting}[language=Javascript]
store.dispatch({
	type: "ADD",
	payload: 10
});
\end{lstlisting}


\paragraph{redux-thunk}

解决了dispatch不好获取的问题。Redux官网说，action就是Plain JavaScript Object。Thunk允许action creator返回一个函数，而且这个函数第一个参数是dispatch。在异步Action时也需要使用thunder中间件。

\subsection{state}

State (也称为 state tree) 是一个宽泛的概念，但是在 Redux API 中，通常是指一个唯一的 state 值，由 store 管理且由 getState() 方法获得。它表示了 Redux 应用的全部状态，通常为一个多层嵌套的对象。约定俗成，顶层 state 或为一个对象，或像 Map 那样的键-值集合，也可以是任意的数据类型。然而你应尽可能确保 state 可以被序列化，而且不要把什么数据都放进去，导致无法轻松地把 state 转换成 JSON\footnote{来源：\url{http://cn.redux.js.org/docs/Glossary.html}}。state是React中组件的一个对象.React把用户界面当做是状态机,想象它有不同的状态然后渲染这些状态,可以轻松让用户界面与数据保持一致.React中,更新组件的state,会导致重新渲染用户界面(不要操作DOM).简单来说,就是用户界面会随着state变化而变化.而Action不会引起state的变化，引起state变化的是reducer。如果用 redux，就没有 state 和 props 的区分了。组件都应该用 props，不应该有任何组件私有的 state。尽量使用props当作数据源，state用来存放状态值。

\paragraph{初始化state}

有2种方法来初始化应用的state，一种是在createStore方法中的第二个可选参数preloadState。也可以在reducer中为undefined的state参数指定默认初始值。state的结构如图\ref{fig:statedemo}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{statedemo.png}
	\caption{State状态示例}
	\label{fig:statedemo}
\end{figure}



\paragraph{redux logger}

Redux Logger是一个中间件，它会打印出当前应用的state变化，配置如下代码所示。

\begin{lstlisting}[language=Javascript]
/**
* ES6 with npm, you can write import React from 'react'. 
* ES5 with npm, you can write var React = require('react').
*/
import {createLogger} from 'redux-logger';

if (process.env.NODE_ENV === 'development') {
	const logger = createLogger({
		stateTransformer: (state) => {
			let newState = {};
			for (let i of Object.keys(state)) {
				if (Iterable.isIterable(state[i])) {
					newState[i] = state[i].toJS();
				} else {
					newState[i] = state[i];
				}
			}
			return newState;
		}
	});
	middleware.push(logger);
}
\end{lstlisting}

打印出action的效果如图\ref{fig:reduxloggerexample}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{reduxloggerexample.png}
	\caption{Redux Logger打印效果}
	\label{fig:reduxloggerexample}
\end{figure}

从图中看出，state的状态已经改变。说明是state改变后的某些动作还未触发。

\subsection{Props}

当你用React Redux的connect进行封装的时候，connect方法会把dispatch放到props中。

\paragraph{React Router父子Props传递}

在React中需要将bookService由父组件中传递到子组件中，写法如下：

\begin{lstlisting}[language=Javascript]
const routes = (
	<BrowserRouter>
		<BookContainer>
			<switch>
				<Route path="/book/:id" render={() => <Book bookService={bookService}/>}/>
			</switch>
		</BookContainer>
	</BrowserRouter>
);
\end{lstlisting}

其中Book处于内层，负责渲染界面，所以叫做展示组件(Presentational Container)。BookContainer与Book组件属于父子组件的关系，BookContainer负责和Redux Store打交道，处于外层，称作容器组件(Container Component)。容器组件的定义如下：

\begin{lstlisting}[language=Javascript]
/**
* 容器组件，负责与Redux Store打交道
* 通过props把状态传递给内层组件
*/
class BookContainer extends Component {
	render() {
		return React.cloneElement(this.props.children, {...this.props});
	}
}
\end{lstlisting}

在浏览器中，切换到相应组件上，可以看到对应的属性，如图\ref{fig:bookserviceprops}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{bookserviceprops.png}
	\caption{React 组件Props传递}
	\label{fig:bookserviceprops}
\end{figure}


\subsection{Reducers}

Redux有且只有一个State状态树，为了避免这个状态树变得越来越复杂，Redux通过 Reducers来负责管理整个应用的State树，而Reducers可以被分成一个个Reducer。


\paragraph{onEnter}

Route 的 onEnter 钩子将用于在渲染对象的组件前做拦截操作，比如验证权限。单页应用路由的权限控制的基本思路是：监听路由的改变，每当路由将要发生改变，我们就使用一个中间服务(该服务介于上一级路由和将要到达路由之间启动)，来判断我们是否有进入这个路由的权限，有的话直接进入，没有的话就redirect。在React中，为某个路由进行权限监听的方式是：

\begin{lstlisting}[language=HTML]
<Route path="page" component={Page} onEnter={requireCredentials}/>
\end{lstlisting}

该onEnter属性对应连着一个具有判断权限的中间服务。

\subsection{Immutable}

JavaScript 中的对象一般是可变的（Mutable），因为使用了引用赋值，新的对象简单的引用了原始对象，改变新的对象将影响到原始对象。如 foo={a: 1}; bar=foo; bar.a=2 你会发现此时 foo.a 也被改成了 2。虽然这样做可以节约内存，但当应用复杂后，这就造成了非常大的隐患，Mutable 带来的优点变得得不偿失。为了解决这个问题，一般的做法是使用 shallowCopy（浅拷贝）或 deepCopy（深拷贝）来避免被修改，但这样做造成了 CPU 和内存的浪费。Immutable 可以很好地解决这些问题。

\paragraph{IMMUTABLE DATA}

Immutable Data 就是一旦创建，就不能再被更改的数据。对 Immutable 对象的任何修改或添加删除操作都会返回一个新的 Immutable 对象。Immutable 实现的原理是 Persistent Data Structure（持久化数据结构），也就是使用旧数据创建新数据时，要保证旧数据同时可用且不变。同时为了避免 deepCopy 把所有节点都复制一遍带来的性能损耗，Immutable 使用了 Structural Sharing（结构共享），即如果对象树中一个节点发生变化，只修改这个节点和受它影响的父节点，其它节点则进行共享。

\subsection{前端Mock}

有的时候前端没必要等待后端接口写好，可以直接Mock接口的返回数据。这里使用json-server\footnote{\url{https://github.com/typicode/json-server}}来实现。

\section{MobX}

\subsection{简介}

mobx 是一个库 (library)，不是一个框架 (framework)。可以和React组合使用，MobX is a battle tested library that makes state management simple and scalable by transparently applying functional reactive programming (TFRP). React and MobX together are a powerful combination. redux 管理的是 (STORE -> VIEW -> ACTION) 的整个闭环，而 mobx 只关心 STORE -> VIEW 的部分。

\section{gulp}

Gulp基于Node.js的前端构建工具，通过Gulp的插件可以实现前端代码的编译（sass、less）、压缩、测试；图片的压缩；浏览器自动刷新。比起Grunt不仅配置简单而且更容易阅读和维护。gulpjs是一个前端构建工具，与gruntjs相比，gulpjs无需写一大堆繁杂的配置参数，API也非常简单，学习起来很容易，而且gulpjs使用的是nodejs中stream来读取和操作数据，其速度更快。使用 browserify 进行打包， gulp 进行任务构建。由于使用了 ES2015 和 JSX 语法，因此使用 Babel 进行转换。

\begin{itemize}
	\item{压缩js代码（gulp-uglify）}
	\item{压缩图片（gulp-imagemin）}
\end{itemize}

\subsection{压缩图片}

首先，定义一个压缩图片的任务：

\begin{lstlisting}[language=JavaScript]
gulp.task('images', function() {
	return gulp.src('src/images/**/*')
			.pipe(imagemin({ optimizationLevel: 3, progressive: true, interlaced: true }))
			.pipe(gulp.dest('dist/assets/img'))
			.pipe(notify({ message: 'Images task complete' }));
});
\end{lstlisting}

定义一个名称为images的任务，这个任务使用imagemin插件把所有在src/images/目录以及其子目录下的所有图片（文件）进行压缩，可以进一步优化，利用缓存保存已经压缩过的图片。在终端中运行如下命令执行此任务。

\begin{lstlisting}[language=Bash]
gulp images
\end{lstlisting}

\subsection{清除文件}

在再次生成文件前，最好先清除之前生成的文件：

\begin{lstlisting}[language=JavaScript]
gulp.task('clean', function(cb) {
	del(['dist/assets/css', 'dist/assets/js', 'dist/assets/img'], cb)
});
\end{lstlisting}

在这里没有必要使用Gulp插件了，可以使用NPM提供的插件。用一个回调函数（cb）确保在退出前完成任务。

\subsection{设置默认任务}

\begin{lstlisting}[language=JavaScript]
gulp.task('default', ['clean'], function() {
	gulp.start('styles', 'scripts', 'images');
});
\end{lstlisting}

clean任务执行完成了才会去运行其他的任务，在gulp.start()里的任务执行的顺序是不确定的，所以将要在它们之前执行的任务写在数组里面。

\section{webpack}

使用install --save时，将默认在package.json中添加小尖尖\^而不是小波浪\~作为依赖版本的前缀。

\section{Library}

\subsection{lodash}

lodash是一个具有一致接口、模块化、高性能等特性的 JavaScript 工具库。

\paragraph{去除重复数据}

去除重复数据：

\begin{lstlisting}[language=JavaScript]
import lodashUniqBy from "lodash/uniqBy";
const uniqOrgList = lodashUniqBy(orgList, 'dfName');
\end{lstlisting}

\paragraph{分组}

将数组中的元素按照新的方式返回。

\begin{lstlisting}[language=JavaScript]
/**
* 将部门按照10个1组组成一个新数组返回
*/
return lodashChunk(dfbmList, 10)
		.map((dfbms, rowNo) => {
			const items = this.renderItems(groups, dfbms);
			return (
		});
\end{lstlisting}


\subsection{Axios}

axios是一个基于Promise用于浏览器和nodejs的HTTP客户端。ES6 原生提供了 Promise 对象。所谓 Promise，就是一个对象，用来传递异步操作的消息。它代表了某个未来才会知道结果的事件（通常是一个异步操作），并且这个事件提供统一的 API，可供进一步处理。

Promise 对象有以下两个特点。

（1）对象的状态不受外界影响。Promise 对象代表一个异步操作，有三种状态：Pending（进行中）、Resolved（已完成，又称 Fulfilled）和 Rejected（已失败）。只有异步操作的结果，可以决定当前是哪一种状态，任何其他操作都无法改变这个状态。这也是 Promise 这个名字的由来，它的英语意思就是「承诺」，表示其他手段无法改变。

（2）一旦状态改变，就不会再变，任何时候都可以得到这个结果。Promise 对象的状态改变，只有两种可能：从 Pending 变为 Resolved 和从 Pending 变为 Rejected。只要这两种情况发生，状态就凝固了，不会再变了，会一直保持这个结果。就算改变已经发生了，你再对 Promise 对象添加回调函数，也会立即得到这个结果。这与事件（Event）完全不同，事件的特点是，如果你错过了它，再去监听，是得不到结果的。

有了 Promise 对象，就可以将异步操作以同步操作的流程表达出来，避免了层层嵌套的回调函数。此外，Promise 对象提供统一的接口，使得控制异步操作更加容易。

Promise 也有一些缺点。首先，无法取消 Promise，一旦新建它就会立即执行，无法中途取消。其次，如果不设置回调函数，Promise 内部抛出的错误，不会反应到外部。第三，当处于 Pending 状态时，无法得知目前进展到哪一个阶段（刚刚开始还是即将完成）。

\subsection{Axios请求}

一个简单的Axios请求示例如下：

\begin{lstlisting}[language=JavaScript]
import axios from 'axios';

export function request() {
	return axios({
		method: 'get', url: 'http://localhost:8011/api/book/111'
	}).then(
		response => {
			console.log(response)
		}
	).catch(
		error => {
			console.error(error);
		}
	);
}
\end{lstlisting}


这里需要解决跨域问题，通过Node或Nginx等做Proxy，或者在服务器端配置CORS。

\subsection{Axios拦截响应}

拦截响应可以对服务端抛出的错误做统一的handle:

\begin{lstlisting}[language=JavaScript]
/**
* axios响应拦截器
*/
axios.interceptors.response.use(
	response =>{
		return response;
	},
	error => {
		if(error.response){
			switch (error.response.status){
				case 401:
					responseError401PreHandle(error.response.data);
					break;
				case 403:
			}
		}
	}
);
\end{lstlisting}

\subsection{Promise}

为了降低异步编程的复杂性，开发人员一直寻找简便的方法来处理异步操作。其中一种处理模式称为promise，它代表了一种可能会长时间运行而且不一定必须完整的操作的结果。这种模式不会阻塞和等待长时间的操作完成，而是返回一个代表了承诺的（promised）结果的对象。promise模式通常会实现一种称为then的方法，用来注册状态变化时对应的回调函数。promise模式在任何时刻都处于以下三种状态之一：未完成（unfulfilled）、已完成（resolved）和拒绝（rejected）。Promise.reject返回了一个拒绝状态的Promise对象. 对于这样的Promise对象, 如果其后续then | catch中都没有声明onRejected回调, 它将会抛出一个 “Uncaught (in promise) …”的错误. Promise之间泾渭分明, 内部Promise抛出的任何错误,外部Promise对象都无法感知并捕获. 同时, 由于promise是异步的, try catch语句也无法捕获其错误.因此养成良好习惯, promise记得写上catch.

\section{Cookie}

The browser is expected to support 20 cookies for each Web server, 300 cookies total, and may limit cookie size to 4 KB each.在Chrome中查看Cookie可以直接在Console中输入：

\begin{lstlisting}[language=Bash]
document.cookie
\end{lstlisting}

服务器端像客户端发送Cookie是通过HTTP响应报文实现的，在Set-Cookie中设置需要像客户端发送的cookie，cookie格式如下：

\begin{lstlisting}
Set-Cookie: “name=value;domain=.domain.com;path=/;expires=Sat, 11 Jun 2016 11:29:42 GMT;HttpOnly;secure”
\end{lstlisting}

其中name=value是必选项，其它都是可选项。如果在Cookie中设置了"HttpOnly"属性，那么通过程序(JS脚本、Applet等)将无法读取到Cookie信息，这样能有效的防止XSS攻击。当secure设置为true时，表示创建的 Cookie 会被以安全的形式向服务器传输，也就是只能在 HTTPS 连接中被浏览器传递到服务器端进行会话验证，如果是 HTTP 连接则不会传递该信息，所以不会被盗取到Cookie 的具体内容。Cookie的主要构成如下：

\begin{lstlisting}
name:一个唯一确定的cookie名称。通常来讲cookie的名称是不区分大小写的。
value:存储在cookie中的字符串值。最好为cookie的name和value进行url编码
domain:cookie对于哪个域是有效的。所有向该域发送的请求中都会包含这个cookie信息。这个值可以包含子域(如：yq.aliyun.com)，也可以不包含它(如：.aliyun.com，则对于aliyun.com的所有子域都有效).
path: 表示这个cookie影响到的路径，浏览器跟会根据这项配置，像指定域中匹配的路径发送cookie。
expires:失效时间，表示cookie何时应该被删除的时间戳(也就是，何时应该停止向服务器发送这个cookie)。如果不设置这个时间戳，浏览器会在页面关闭时即将删除所有cookie；不过也可以自己设置删除时间。这个值是GMT时间格式，如果客户端和服务器端时间不一致，使用expires就会存在偏差。
max-age: 与expires作用相同，用来告诉浏览器此cookie多久过期（单位是秒），而不是一个固定的时间点。正常情况下，max-age的优先级高于expires。
HttpOnly: 告知浏览器不允许通过脚本document.cookie去更改这个值，同样这个值在document.cookie中也不可见。但在http请求张仍然会携带这个cookie。注意这个值虽然在脚本中不可获取，但仍然在浏览器安装目录中以文件形式存在。这项设置通常在服务器端设置。
secure: 安全标志，指定后，只有在使用SSL链接时候才能发送到服务器，如果是http链接则不会传递该信息。就算设置了secure 属性也并不代表他人不能看到你机器本地保存的 cookie 信息，所以不要把重要信息放cookie就对了
\end{lstlisting}

\paragraph{Max Age}查看Cookie类的setMaxAge(int expiry)方法说明：这个类用于给Cookie设置最大的年龄，以秒为单位。expiry - an integer specifying the maximum age of the cookie in seconds; if negative, means the cookie is not stored; if zero, deletes the cookie.如果这个方法的参数给的是正值，表明cookie在超过指定的年龄时间后会消亡。负值意味着cookie不是永久性存储的，在浏览器关闭的时候将会被删除。这个方法的参数如果是零值将会导致cookie被删除。Expire指定了cookie的生存期，默认情况下cookie是暂时存在的，他们存储的值只在浏览器会话期间存在，当用户推出浏览器后这些值也会丢失，如果想让cookie存在一段时间，就要为expires属性设置为未来的一个过期日期。现在已经被max-age属性所取代，max-age用秒来设置cookie的生存期。

\section{Javascript}

\begin{tabular}{cp{8cm}c}
	\hline
	\multirow{1}{*}{快捷键}
	& \multicolumn{1}{c}{说明}  \\
	\hline			
	nbsp & " " \\
	\hline
\end{tabular}


\subsection{prototype}

prototype是用于解决实例对象之间无法共享属性和方法的。

\begin{lstlisting}[language=Javascript]
function Tiger(name){
	this.name = name;
	this.species = "猫科";
}
var tigerA = new Tiger("大虎");
var tigerB = new Tiger("二虎");
tigerA.species="犬科";
console.log(tigerB.species);//猫科
\end{lstlisting}

此处无法做到数据共享，也是极大的资源浪费。其实species共享是符合设计的。使用prototype来实现共享：

\begin{lstlisting}[language=Javascript]
function Tiger(name) {
	this.name = name;
}
Tiger.prototype = {
	species: '犬科'
};
var tigerA = new Tiger("大虎");
var tigerB = new Tiger("二虎");
console.log(tigerA.species);//犬科
console.log(tigerB.species);//犬科
Tiger.prototype.species = '猫科';
console.log(tigerA.species);//猫科
console.log(tigerB.species);//猫科
\end{lstlisting}

注意这里是设置Tiger的prototype而不是Tiger实例的prototype。一个未初始化的变量的值为undefined,一个没有传入实参的形参变量的值为undefined,如果一个函数什么都不返回,则该函数默认返回undefined.所以这里在控制台最后会打印一行undefined。普通对象设置prototype使用如下语句：

\begin{lstlisting}[language=Javascript]
tigerA.__proto__.species = '犬科';
\end{lstlisting}

\subsection{常用函数}

数学函数：

\begin{lstlisting}[language=Javascript]
f = Math.round(x / 10000 * Math.pow(10, y)) / Math.pow(10, y);
\end{lstlisting}

Math.pow计算10的y次幂，Math.round计算与round函数最接近的整数。

\begin{lstlisting}[language=HTML]
#默认会执行一次
<button onClick={this.getBookInfo()}>查询</button>
#默认不会执行
<button onClick={() => this.getBookInfo()}>查询</button>
\end{lstlisting}

对象转数组：

\begin{lstlisting}[language=Javascript]
const books = this.props.books;
let arr = [];
for(let i in books){
	arr.push(books[i]);
}
\end{lstlisting}

判断值是不是空：

\begin{lstlisting}[language=Javascript]
if (!value || (value + ' ').trim().length === 0) {
	/**
	* 值为空的字段在界面上不显示
	* 如果value是整数，不能使用trim()，所以加字符串手动转换成String类型
	*/
	return;
}

/**
* 如果需要判断字符串为空，直接if判断即可
*/
if(value){
	/**
	* 此种写法等价于if(value===null||value===undefined||value==='')
	*/
}
\end{lstlisting}

\subsection{日期处理}

Moment.js是一个JavaScript日期处理类库，如下代码片段使用moment格式化日期。

\begin{lstlisting}[language=Javascript]
import moment from "moment";

tableDataSource = (logList, orgList) => {
	const content = logList.content || [];
	return content.map((item, idx) => {
		const org = orgList.find(org => org.orgId == item.ORG_ID);
		return {
			...item,
			//格式化日期
			QUERY_DATE: moment(item.QUERY_DATE).format("YYYY-MM-DD hh:mm:ss"),
		};
	})
};
\end{lstlisting}

获取本周的第一天，moment中可以用如下代码实现：

\begin{lstlisting}[language=Javascript]
moment.lang('zh-cn', {
	week: {
		dow: 1 // Monday is the first day of the week
	}
});
// date now is the first day of the week, (i.e., Monday)
const date = moment().weekday(0); 
\end{lstlisting}

\section{常见问题}

\paragraph{index.js:83 TypeError: Cannot read property 'get' of undefined}

出現此錯誤的原因是獲取到的參數爲空。

\begin{lstlisting}
TypeError: Cannot read property 'get' of undefined
	at Object.getToJS (Utils.js:171)
	at Object.getToArray (Utils.js:166)
	at CreditDataPeopleMain.render (CreditDataPeopleMain.js:299)
	at eval (ReactCompositeComponent.js:796)
	at measureLifeCyclePerf (ReactCompositeComponent.js:75)
	at ReactCompositeComponentWrapper._renderValidatedComponentWithoutOwnerOrContext (ReactCompositeComponent.js:795)
	at ReactCompositeComponentWrapper._renderValidatedComponent (ReactCompositeComponent.js:822)
	at ReactCompositeComponentWrapper.performInitialMount (ReactCompositeComponent.js:362)
	at ReactCompositeComponentWrapper.mountComponent (ReactCompositeComponent.js:258)
	at Object.mountComponent (ReactReconciler.js:46)
\end{lstlisting}

獲取到的空數據如圖\ref{fig:getundefineerror}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{getundefineerror.png}
	\caption{undefined錯誤}
	\label{fig:getundefineerror}
\end{figure}

\paragraph{Each child in an array or iterator should have a unique "key" prop. Check the render method of `CreditDataPeopleMain`}

提示在數組和可枚舉中需要給定一個唯一的key屬性：

\begin{lstlisting}
Warning: Each child in an array or iterator should have a unique "key" prop. Check the render method of `CreditDataPeopleMain`. See https://fb.me/react-warning-keys for more information.
in a (created by CreditDataPeopleMain)
in CreditDataPeopleMain (created by RouterContext)
in SharingExchangeContainer (created by Connect(SharingExchangeContainer))
in Connect(SharingExchangeContainer) (created by RouterContext)
in div (created by AppContainer)
in div (created by AppContainer)
in div (created by AppContainer)
in AppContainer (created by RouterContext)
in RouterContext (created by Router)
in Router
in Provider
\end{lstlisting}

key屬性幫助react確定項目是否已經更新(Keys help React identify which items have changed, are added, or are removed. Keys should be given to the elements inside the array to give the elements a stable identity)。

\paragraph{Cannot read property 'apply' of undefined}

\paragraph{Module build failed: SyntaxError: Unexpected token ...}

...是ES6中的扩展运算符(Spread Operator)，需要将之转化为ES5语法，转换使用babel来完成，presets字段设定转码规则，检查是否安装包：

\begin{lstlisting}[language=Javascript]
"devDependencies": {
	"babel-core": "^6.25.0",
	"babel-loader": "^7.1.0",
	"babel-preset-es2015": "^6.24.1",//ES2015转码规则
	"babel-preset-react": "^6.24.1",
	//ES7不同阶段语法提案的转码规则（共有4个阶段）
	//从stage-0到stage-3
	"babel-preset-stage-0": "6.22.0"
}
\end{lstlisting}

检查Webpack中的配置：

\begin{lstlisting}[language=Javascript]
module: {
	loaders: [{
		test: /\.(jsx|js)?$/,   // 匹配jsx语法规则
		exclude: /node_modules/,   // 排除的模块
		loaders: 'babel-loader',   // loader名
		query: {
			// 将react预置为es5
			presets: ['es2015', 'react','stage-0']
		}
	}]
}
\end{lstlisting}

\paragraph{Store does not have a valid reducer}

提示错误：Store does not have a valid reducer. Make sure the argument passed to combineReducers is an object whose values are reducers.store在index.js文件中引入，在store.js中添加打印语句将reducer对象打印出来，看到实际是有reducer对象的，所以怀疑是reducer的定义有些问题。最后发现原来是require的使用有问题。

\begin{lstlisting}[language=Javascript]
const state = {};
for (const name of nameList) {
state[name] = require(`./${name}/${name}Reducer`);

}
\end{lstlisting}

这里是用Requre动态加载，但是实际有点问题，暂时还找不到原因。暂时采用如下的写法：


\begin{lstlisting}[language=Javascript]
import bookReducer from './book/bookReducer';
state[0] = bookReducer;
\end{lstlisting}

\part{OS}

\chapter{Linux}

在使用apt-get时，有时可能会遇到依赖损坏的情况，那么可以执行如下命令，尝试修复依赖：

\begin{lstlisting}[language=Bash]
sudo apt-get -f install 
\end{lstlisting}

f参数表示尝试修正系统依赖损坏处。

\subsection{shell}

\paragraph{快捷键}

Shell中常用的快捷键如下：

\begin{tabular}{cp{8cm}c}
	\hline
	\multirow{1}{*}{快捷键}
	& \multicolumn{1}{c}{说明}  \\
	\hline			
	Alt + F & 向前移动一个单词 \\
	Alt + B & 向后移动一个单词 \\	
	\hline
\end{tabular}

\paragraph{source}在使用source命令时，提示找不到文件。如果filename不包含斜杠（/），那么从PATH环境变量指定的那些路径搜索filename，这个文件不必是可执行 的。（If filename does not contain a slash, file names in  PATH  are used  to  find the directory containing filename.  The file searched for in PATH need not be executable.）如果在PATH中找不到指定文件，当bash不是posix模式时，将在当前目录下搜索该文件。（When bash is not in posix mode, the current directory is searched if no file is found in PATH.）如果shopt里的sourcepath关闭，则不在PATH中搜索指定文件。（If  the sourcepath  option  to  the shopt builtin command is turned off, the PATH is not searched.）直接写文件的绝对路径解决。source filename其实只是简单地读取脚本里面的语句依次在当前shell里面执行，没有建立新的子shell。那么脚本里面所有新建、改变变量的语句都会保存在当前shell里面。所以为什么修改了.bashrc文件之后都会使用source命令加文件名来让设置生效。

\paragraph{修改登录路径}每次登录后都要切换到项目路径，直接登录时切换到项目路径，在当前用户的.bashrc中添加如下设置：

\begin{lstlisting}[language=Bash]
if [ -d /home/app/ ];then
	cd /home/app/
fi
\end{lstlisting}

\paragraph{输入长命令}有时在shell中，会输入比较长的命令，直接在shell中输入可能不太方便。可以直接使用快捷键Ctrl+X,E，即可调出默认编辑器。因为自己习惯使用vim，所以使用如下命令设置vim为默认的编辑器:

\begin{lstlisting}[language=Bash]
export EDITOR=vim
\end{lstlisting}

在vim中，编辑好了命令之后，直接按下ZZ命令，即可退出vim，shell即会执行编辑完成后的命令。

\paragraph{创建路径}创建目录及目录树可以批量进行，一个命令，即可创建整个目录树。

\begin{lstlisting}[language=Bash]
#新建后端开发目录
mkdir -p /home/dolphin/hldata/backend/credit-system
#新建开发目录树
mkdir -p /home/dolphin/hldata/{backend/credit-system,frontend/credit-system-frontend,doc/{html,info,pdf},demo/}
\end{lstlisting}

\paragraph{与控制操作符组合使用}||控制操作符分隔两个命令，并且仅当第一个命令返回非零退出状态时才运行第二个命令。换句话说，如果第一个命令成功，则第二个命令不会运行。如果第一个命令失败，则第二个命令才会运行。

\begin{lstlisting}[language=Bash]
#如果目录不存在，则创建相应的目录
cd /home/hldev/app-soft || mkdir -p /home/hldev/app-soft
\end{lstlisting}

\paragraph{source文件路径}在使用source命令时，提示找不到文件。如果filename不包含斜杠（/），那么从PATH环境变量指定的那些路径搜索filename，这个文件不必是可执行 的。（If filename does not contain a slash, file names in  PATH  are used  to  find the directory containing filename.  The file searched for in PATH need not be executable.）如果在PATH中找不到指定文件，当bash不是posix模式时，将在当前目录下搜索该文件。（When bash is not in posix mode, the current directory is searched if no file is found in PATH.）如果shopt里的sourcepath关闭，则不在PATH中搜索指定文件。（If  the sourcepath  option  to  the shopt builtin command is turned off, the PATH is not searched.）直接写文件的绝对路径解决。

\paragraph{读取配置文件}在编写自动化部署脚本的时候需要读取配置文件version.properties中的版本号，直接使用source命令即可：

\begin{lstlisting}[language=Bash]
# version.properties文件中直接写VERSION\_CODE=1.0.2即可
source /Users/dolphin/source/credit-system/gradle/version.properties
# 输出1.0.2
echo $VERSION_CODE
\end{lstlisting}

在Gradle中也可以直接读取version.properties中的内容，这样编译打包发布就可以通过脚本自动化完成。

\paragraph{文件是否存在}

Shell下判斷文件是否存在：

\begin{lstlisting}[language=Bash]
myFile="/opt/app/backend/app.pid"

if [ -f "$myFile" ]; then
	kill `cat /opt/app/backend/app.pid`
fi
\end{lstlisting}

\paragraph{大小写不敏感}

有时文件前缀是大写字母，在使用Tab按键自动提示时就必须首先输入大写字母匹配才有效果，总体来说不是非常方便。所以可以设置shell大小写不敏感，就不需要再重复做一次大小写转换操作了：

\begin{lstlisting}[language=Bash]
echo "set completion-ignore-case on">>~/.inputrc
\end{lstlisting}

设置完毕后重新打开终端即可。

\paragraph{统计代码行数}

统计项目代码行数使用如下命令：

\begin{lstlisting}[language=Bash]
# Java行数：35400
find . -name "*.java"|xargs cat|grep -v ^$|wc -l
# 42178，会显示每个Java文件的行数
find . -name '*.java' |xargs wc -l

# Scala行数：1500
find . -name "*.scala"|xargs cat|grep -v ^$|wc -l
# XML行数：7336
find . -name "*.xml"|xargs cat|grep -v ^$|wc -l
# js行数：151312
find . -name "*.jsr"|xargs cat|grep -v ^$|wc -l

# 统计代码行数
find sourcecode "(" -name "*.js" -or -name "*.java" -or -name "*.scala" -or -name "*.xml" ")"|xargs cat|grep -v ^$|wc -l
\end{lstlisting}

符号$\textasciicircum$表示行首，\$表示行尾，使用grep -v可以实现NOT操作。-v选项用来实现反选匹配的（ invert match）。xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。

\paragraph{查看端口情况}

查看某个端口是否被占用：

\begin{lstlisting}[language=Bash]
lsof -i:18080
\end{lstlisting}

lsof（list open files）是一个列出当前系统打开文件的工具。FD(File Descriptor)：文件描述符，应用程序通过文件描述符识别该文件。在Linux系统中一切皆可以看成是文件，文件又可分为：普通文件、目录文件、链接文件和设备文件。文件描述符（file descriptor）是内核为了高效管理已被打开的文件所创建的索引，其是一个非负整数（通常是小整数），用于指代被打开的文件，所有执行I/O操作的系统调用都通过文件描述符。程序刚刚启动的时候，0是标准输入，1是标准输出，2是标准错误。如果此时去打开一个新的文件，它的文件描述符会是3。

\begin{tabular}{cp{8cm}c}
	\hline
	\multirow{1}{*}{文件描述符}
	& \multicolumn{1}{c}{说明}  \\
	\hline			
	cwd & 表示 current  work dirctory,即：应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改 \\
	txt & 该类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的/sbin/init程序 \\
	lnn & library references （AIX） \\
	ltx & shared  library text（code and  data） \\
	er & FD  information  error （see  NAME  column） \\
	jld & jail  directory（FreeBSD） \\	
	\hline
\end{tabular}

（7）mxx： hex  memory-mapped  type number  xx.

（8）m86：DOS  Merge  mapped  file

（9） mem: memory-mapped  file 

（10）mmap: memory-mapped device

（11）pd: parent  directory

（12）rtd: root  directory

（13）tr: kernel  trace file （OpenBSD）

（14）v86  VP/ix  mapped  file

（15）0：表示标准输出

（16）1：表示标准输入

（17）2：表示标准错误。
根据具体操作系统的不同，将文件和目录称为 REG 和 DIR（在 Solaris 中，称为 VREG 和 VDIR）。其他可能的取值为 CHR 和 BLK，分别表示字符和块设备；或者 UNIX、FIFO 和 IPv4，分别表示 UNIX 域套接字、先进先出 (FIFO) 队列和网际协议 (IP) 套接字。
进而查看被哪个程序占用：

\begin{lstlisting}[language=Bash]
ps pid
\end{lstlisting}

\subsection{命令}

\paragraph{ls}

有时一个文件夹里面的文件很多，为了避免从太多文件里面去找目标文件，而只想查看一个文件的属性：

\begin{lstlisting}[language=Bash]
#列出指定文件的信息
ls -lh authorized_keys
#只显示当前文件夹下的目录
ls -al | grep "^d"
\end{lstlisting}


\paragraph{find}

在当前目录下查找大文件：

\begin{lstlisting}[language=Bash]
find . -type f -size +1000k
#查找大于100MB的文件
find . -type f -size +100M
\end{lstlisting}

size默认单位是b，而它代表的是512字节，所以2表示1K，1M则是2048，如果不想自己转换，可以使用其他单位，如c、K、M等。如何查找指定大小的文件，并列出实际文件的大小。

\paragraph{kill}

\paragraph{tar}

创建压缩文档：

\begin{lstlisting}[language=Bash]
#打包
tar -czvf mybatisDemo.tar.gz mybatisDemo
\end{lstlisting}

解压tar.xz压缩文件，先 xz -d xxx.tar.xz 将 xxx.tar.xz解压成 xxx.tar 然后，再用 tar xvf xxx.tar来解包。


\begin{lstlisting}[language=Bash]
xz -d xxx.tar.xz
tar xvf xxx.tar
\end{lstlisting}

\paragraph{grep}

有时需要从返回的列表中，查看某一个字段的返回结果，如果列表太长或者字段太多，单凭肉眼分辨非常吃力，此时可以通过grep来查看数组返回数据：

\begin{lstlisting}[language=Bash]
curl -H "APPID:" -H "TIMESTAMP:2016-12-19 16 (tel:2016121916):58:02" -H "ECHOSTR:" -H "TOKEN:" http://10.10.1.12:28080/api/xzxk?xdr= | jq '.' |grep jdrq
\end{lstlisting}

效果如图\ref{fig:grepfilterreturnvalue}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{grepfilterreturnvalue.png}
	\caption{Grep过滤Json结果}
	\label{fig:grepfilterreturnvalue}
\end{figure}

这样就可以轻松的看出来某个关键字段的返回结果，不需要再一个个比对。

\paragraph{sed}

sed全名叫stream editor，流编辑器，用程序的方式来编辑文本，相当的hacker啊。sed基本上就是玩正则模式匹配，所以，玩sed的人，正则表达式一般都比较强。過濾某一個時間段的日志：

\begin{lstlisting}[language=Bash]
sed -n '/^2017-08-22 18:50/','/^2017-08-22 18:59/p' cc-web.2017-08-22.0.log >> filter.log
\end{lstlisting}

\paragraph{ag}

ag\footnote{\url{https://github.com/ggreer/the_silver_searcher}}可以递归搜索文件内容，输入如下命令安装：

\begin{lstlisting}[language=Bash]
apt-get install silversearcher-ag
dnf install the_silver_searcher
\end{lstlisting}

\paragraph{apt}

apt(Advance Package Tool)是Ubuntu的包管理工具：

\begin{lstlisting}[language=Bash]
# Ubuntu显示包的详细信息
sudo apt show wget
# 移除安装包
sudo apt remove wget
# CentOS显示包信息
yum info wget
\end{lstlisting}

\subsection{自动部署}

\begin{lstlisting}[language=Bash]
#!/usr/bin/env bash

#
# 一键部署后端项目到远程1台或多台服务器
# 发布流程为：编译构建-打包-发布包-拷贝包到指定目录-解压包
# 注：本机需要安装Ansible并与服务器做免密登录
#
# 2017-05-25 dolphin 增加动态读取版本号，根据版本自动发布程序
#

# 当使用未初始化的变量时，程序自动退出
set -u

# 当任何一行命令执行失败时，自动退出脚本
set -e

# send=`date '+%Y-%m-%d %H:%M:%S'`
CURRENT\_TIME=`date '+%Y%m%d%H%M%S'`

echo "$CURRENT_TIME"

PROGRAM_PATH_INFORMATION_CENTER="/opt/app/backend"
SERVER_IP="192.168.1.101"
LOGIN_USER="root"
LOCAL_PATH="./cc-web-boot/build/libs"

# Read program version number
source version.properties
echo "$VERSION"

PROGRAM_NAME="credit-system-web-boot-$VERSION.jar"
echo "$PROGRAM_NAME"

echo "开始构建..."
# Build project
./gradlew -p cc-web-boot -x test build
./gradlew -p cc-etl-sechedule -x test build

echo "开始拷贝..."
# publish project
scp $LOCAL_PATH/$PROGRAM_NAME $LOGIN_USER@$SERVER_IP:~/app-soft/
scp start.sh $LOGIN_USER@$SERVER_IP:$PROGRAM_PATH_INFORMATION_CENTER
scp version.properies $LOGIN_USER@$SERVER_IP:$PROGRAM_PATH_INFORMATION_CENTER

ansible webservers -m command -a "date"

echo "停止站点..."
ansible webservers -m command -a "chdir=$PROGRAM_PATH_INFORMATION_CENTER bash ./stop.sh"

echo "等待站点停止..."
sleep 10

echo "备份站点..."
# 备份当前站点
ansible webservers -m command -a "mv $PROGRAM_PATH_INFORMATION_CENTER/$PROGRAM_NAME $PROGRAM_PATH_INFORMATION_CENTER/$PROGRAM_NAME-$CURRENT_TIME"

echo "拷贝新文件..."
ansible webservers -a "mv ~/app-soft/$PROGRAM_NAME $PROGRAM_PATH_INFORMATION_CENTER"

echo "启动站点..."
ansible webservers -m shell -a "bash ./start.sh chdir=$PROGRAM_PATH_INFORMATION_CENTER"

echo "部署完成"
\end{lstlisting}

\subsection{环境变量}

Linux 分 shell变量(set)，用户变量(env)， shell变量包含用户变量，export是一种命令工具，是显示那些通过export命令把shell变量中包含的用户变量导入给用户变量的那些变量.删除Linux环境变量：

\begin{lstlisting}[language=Bash]
# 查看环境变量
echo $http_proxy
env
# 删除环境变量
unset http_proxy
\end{lstlisting}

set命令显示(设置)shell变量 包括的私有变量以及用户变量，不同类的shell有不同的私有变量 bash,ksh,csh每中shell私有变量都不一样。env命令显示(设置)用户变量变量。export命令显示(设置)当前导出成用户变量的shell变量。

\begin{tabular}{cp{8cm}c}
	\hline
	\multirow{1}{*}{序号}
	& \multicolumn{1}{c}{名称}  \\
	\hline			
	/etc/profile  & 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行.并从/etc/profile.d目录的配置文件中搜集shell的设置 \\
	\hline	
	/etc/environment & 在登录时操作系统使用的第二个文件,系统在读取你自己的profile前,设置环境文件的环境变量 \\
	\hline
	/etc/bashrc & 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取\\
	\hline
	~/.profile & 每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下,它设置一些环境变量,执行用户的.bashrc文件\\
	\hline
	~/.bashrc &  该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取\\
	\hline
\end{tabular}

有时在Bash记录历史命令时，需要忽略掉重复的命令等。可以做如下设置：

\begin{lstlisting}[language=Bash]
# 忽略[连续]重复命令
HISTCONTROL=ignoredups
# 清除重复命令
# HISTCONTROL=erasedups
# 忽略特定命令
HISTIGNORE="[   ]*:ls:ll:cd:vi:pwd:sync:exit:history*"
# 命令历史文件大小10M
HISTFILESIZE=1000000000
# 保存历史命令条数10W
HISTSIZE=1000000
\end{lstlisting}

以上配置可以通过 set | grep HIST 查看可选项.当打开多个终端，关闭其中一个终端时，会覆盖其他终端的命令历史， 这里我们采用追加的方式避免命令历史文件 .bash\_history 文件被覆盖。 再次打开 ~/.bashrc 文件添加下面这一句.

\begin{lstlisting}[language=Bash]
shopt -s histappend
\end{lstlisting}

更多 shopt 可选项可以通过 echo \$SHELLOPTS 命令查看。



\subsubsection{screen}

GNU Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。在使用nohup时，有一个缺点就是交互性很低，例如后台启动OpenVPN时，需要输入预先设置的认证密码。但是使用nohup命令启动时，无法显示密码输入入口。此时就需要使用screen命令了。输入如下的命令：

\begin{lstlisting}[language=Bash]
screen sudo openvpn client.conf
\end{lstlisting}

接着在跳出的窗口中，输入密码。按下Ctrl + A后再按下d键，暂时断开screen会话即可。只要Screen本身没有终止，在其内部运行的会话都可以恢复。这一点对于远程登录的用户特别有用——即使网络连接中断，用户也不会失去对已经打开的命令行会话的控制。只要再次登录到主机上执行screen -r就可以恢复会话的运行。同样在暂时离开的时候，也可以执行分离命令detach，在保证里面的程序正常运行的情况下让Screen挂起（切换到后台）。这一点和图形界面下的VNC很相似。查看当前系统中有哪些screen会话：

\begin{lstlisting}[language=Bash]
screen -ls
\end{lstlisting}

该命令会列出当前系统中所有的screen会话，重新打开会话：

\begin{lstlisting}[language=Bash]
screen -r 12865
\end{lstlisting}

正常情况下，当你退出一个窗口中最后一个程序（通常是bash）后，这个窗口就关闭了。另一个关闭窗口的方法是使用C-a k，这个快捷键杀死当前的窗口，同时也将杀死这个窗口中正在运行的进程。如果一个Screen会话中最后一个窗口被关闭了，那么整个Screen会话也就退出了，screen进程会被终止。除了依次退出/杀死当前Screen会话中所有窗口这种方法之外，还可以使用快捷键C-a :，然后输入quit命令退出Screen会话。需要注意的是，这样退出会杀死所有窗口并退出其中运行的所有程序。其实C-a :这个快捷键允许用户直接输入的命令有很多，包括分屏可以输入split等，这也是实现Screen功能的一个途径

\subsubsection{查看Linux信息}

\begin{lstlisting}[language=Bash]
# Linux查看版本当前操作系统内核信息
uname －a 
\end{lstlisting}

\subsection{XFCE桌面}

占用资源较GNOME ，KDE较少。适合老机器，轻量级桌面。与windows界面环境类似。许多不习惯GNOME 3 ，Unity新桌面的同学，很多选择了XFCE 4.8，包括Linus大神同学。

\subsection{Fedora访问Windows共享文件夹}

在局域网里，有时需要使用Fedora 24访问Windows共享文件夹。在Fedora中输入命令：

\begin{lstlisting}[language=Bash]
#密码aaa321
sudo mount -t cifs -o username="aaa",uid="dolphin",gid="dolphin" //10.55.10.2/data /home/dolphin/software
\end{lstlisting}

一般提示权限错误是由于用户名密码输入错误。aaa为Windows机器的用户名，uid为当前Fedora账户的名称，gid为当前Fedora用户组的名称。10.55.10.2为Windows账户的IP，data为共享文件夹的名称，与盘符无关。/home/dolphin/software本地Fedora机器的目录。cifs为通用网络文件系统，The Common Internet File System (CIFS) is the standard way that computer users share files across corporate intranets and the Internet. An enhanced version of the Microsoft open, cross-platform Server Message Block (SMB) protocol, CIFS is a native file-sharing protocol in Windows 2000.

\subsection{Ubuntu访问Windows共享文件夹}

在文件管理器中，点击左侧的“Connect to Server”选项卡，填写Windows的IP，示例如下：

\begin{lstlisting}[language=HTML]
smb://10.55.10.2
\end{lstlisting}

点击确定后，在弹出的对话框中输入相应的用户名和密码即可。

\subsection{软件包管理}

根据软件包的名称模糊匹配，查看系统已经安装了哪些软件包：

\begin{lstlisting}[language=Bash]
dpkg -l|grep pcre
yum list installed|grep openssl
# 查看openssl版本
openssl version -a
# yum查看已经安装的软件包
yum list installed
\end{lstlisting}

在升级软件包的时候，可以注意软件包管理器下方的提示，在网络速度不是很理想的情况下，可以优先升级已经下载完毕后的软件包。

\subsection{输入法(Input Method)}

有时在Fedora 24中，突然中文输入法切换后无法输入中文了。可以到系统区域和语言(Region\&Language)设置界面，重新删除后再添加即可修复,如图\ref{fig:regionandlanguagesetting}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{regionandlanguagesetting.jpg}
	\caption{Fedora区域和语言设置}
	\label{fig:regionandlanguagesetting}
\end{figure}

在使用Ubuntu 16.04的搜狗拼音时，有时也会出现各种问题。可以尝试的解决方案包括，登录系统再次重新登入。删除输入法配置后，重新添加输入法，或者使用Google拼音输入法，输入如下命令进行安装：

\begin{lstlisting}[language=Bash]
sudo apt install fcitx-googlepinyin
\end{lstlisting}

安装好后需要重新登入系统，添加输入法配置时才可以选择Google输入法。

\part{Tool}

\newpage

\chapter{Intellij Idea}

\subsection{Intellij Idea远程调试}

输入如下命令，启动远程调试：

\begin{lstlisting}[language=Bash]
nohup /home/app/local/jdk1.8.0_111/bin/java -Xmx8192M -Xms4096M -jar -Xdebug -Xrunjdwp:transport=dt_socket,suspend=n,server=y,address=5005 /home/app/backend/credit-system-web-boot-1.1.9.jar --spring.config.location=application.properties>/dev/null &
\end{lstlisting}


\subsection{Intellij Idea调试慢}

在使用Intellij Idea时调试Spring Boot项目时，启动时突然变慢了，原来启动10几秒，后来启动莫名其妙需要1分钟左右。经过排查是由于打了过多的断点的缘故，删除断点后，启动立即变快了。所以在项目中不能标记太多断点，调试后立即将断点删除。

\subsection{Tips}


\begin{tabular}{cp{6cm}c}
	\hline
	\multirow{1}{*}{快捷键}
	& \multicolumn{1}{c}{作用}  \\
	\hline			
	Ctrl + Left/Right & 到行首/尾 \\
	Command + Shift + A(Mac) & Find Action \\
	Ctrl + Left/Right(Ubuntu) & 左移/右移一个单词\\
	Home/End(Ubuntu) & 光标移动到行首或者行尾\\
	Shift + Ctrl + Alt + J & Select all occurrence，选中当前单词所有出现的地方\\
	Alt + J & Next Occurence，上下左右搜寻，选择离当前光标最近的下一个出现的地方，按下Shift就是反向选择\\	
	\hline
\end{tabular}


\paragraph{Class JavaLaunchHelper is implemented}


启动时提示错误：

\begin{lstlisting}[language=Bash]
objc[3648]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home/bin/java (0x10d19c4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10ea194e0). One of the two will be used. Which one is undefined.
\end{lstlisting}


检查启动配置里启动模块是否选择正确。

\paragraph{列编辑模式}

最近要做一些数据抓取转换的工作，需要编辑一些SQL脚本，突然发现可以根据SQL的注释，通过列编辑模式轻轻松松生成SQL的Insert语句。在Mac下，按住Command + Shift + Alt组合键之后，再用鼠标选择相应的区域即可。在列编辑的过程中，需要好好利用Command + Left到行首，Command + Right到行尾的功能，有时候单个字符移动光标之后，也许不是想要的光标位置，此时可以整个单词的移动，利用Alt + Left和Alt + Right快捷键。


\paragraph{自动换行}有时候，代码太长，滚动滑块不是非常方便。需要设置代码自动换行。


\paragraph{匹配文件}有时需要使用Intellij Idea的全局搜索，在匹配文件时，如果想匹配某几类文件，可以输入如下的过滤条件。

\begin{lstlisting}[language=Bash]
#在Java文件和log文件中进行匹配
*.java,*.log
\end{lstlisting}

\paragraph{智能提示}

在使用Intellij Idea的过程中，有时总是不出现智能提示，不出现智能提示大致氛围3种情况。其一是勾选了省电模式，在省电模式下没有智能提示。第二是缓存问题，解决办法删除缓存 file -> invalidate caches，根据提示重启intellij idea。 情况三 配色问题。解决方法是，切换到其他配色，比如内置配色darchla。自己的是属于勾选了省电模式。

\paragraph{Template}

类模版：

\begin{lstlisting}[language=Java]
#if (${PACKAGE_NAME} && ${PACKAGE_NAME} != "")package ${PACKAGE_NAME};#end
#parse("File Header.java")
/**
* @author jiangtingqiang@gmail.com
* @create ${YEAR}-${MONTH}-${DAY}-${TIME}
*/
public class ${NAME} {
}
\end{lstlisting}



\chapter{Gradle}

一般在Mac下Gradle的安装路径为：/usr/local/Cellar/gradle/3.2.1/libexec，在Mac的Intellij Idea指定Gradle路径时，即用此路径。

\begin{tabular}{cp{10cm}c}
	\hline
	\multirow{1}{*}{快捷键}
	& \multicolumn{1}{c}{作用}  \\
	\hline			
	fastjson & compile('com.alibaba:fastjson:1.2.35') \\
	httpclient & compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5'\\
	\hline
\end{tabular}

\section{基础}

\subsection{初始化项目}

输入如下命令来初始化一个项目：

\begin{lstlisting}[language=Java]
gradle init
\end{lstlisting}

运行命令后，会在项目中新建gradlew/gradlew.bat/settings.gradle/build.gradle文件。

\subsection{引用本地文件}

有时候项目依赖的可能不是公布在网络上的一个公共库，可能只是一个闭源的jar包，只是一个本地文件而已。Gradle中引用本地文件写法如下：

\begin{lstlisting}[language=Java]
def ccDataLibs = fileTree(dir: "$rootDir/cc-data/lib", include: '*.jar')

dependencies {
	compile ccDataLibs
}
\end{lstlisting}

dir表示文件所在目录，include表示包含哪些文件。

\subsection{执行流程}

There is a one-to-one relationship between a Project and a "build.gradle" file. During build initialisation, Gradle assembles a Project object for each project which is to participate in the build, as follows:

Create a Settings instance for the build.
Evaluate the "settings.gradle" script, if present, against the Settings object to configure it.
Use the configured Settings object to create the hierarchy of Project instances.
Finally, evaluate each Project by executing its "build.gradle" file, if present, against the project. The projects are evaluated in breadth-wise order, such that a project is evaluated before its child projects. This order can be overridden by calling evaluationDependsOnChildren() or by adding an explicit evaluation dependency using evaluationDependsOn(String).

\subsection{默认JVM}

在Intellij Idea中导入Gradle项目时，可能提示未设置Gradle JVM，此时选择select Configure > Project Defaults > Project Structure，在项目Structure中设置JVM即可。

\subsection{repositories}

在Gradle构建标本build.gradle里，经常会看到如下脚本：

\begin{lstlisting}[language=Java]
repositories {
	maven {
		url 'http://www.eveoh.nl/files/maven2'
	}
	maven {
		url 'http://repox.gtan.com:8078'
	}
	mavenCentral()
	jcenter()
	maven { url 'http://repo.spring.io/plugins-release' }
}
\end{lstlisting}

总的来说，只有两个标准的Android library文件服务器：Jcenter 和 Maven Central。起初，Android Studio 选择Maven Central作为默认仓库。如果你使用老版本的Android Studio创建一个新项目，mavenCentral()会自动的定义在build.gradle中。但是Maven Central的最大问题是对开发者不够友好。上传library异常困难。上传上去的开发者都是某种程度的极客。同时还因为诸如安全方面的其他原因，Android Studio团队决定把默认的仓库替换成jcenter。正如你看到的，一旦使用最新版本的Android Studio创建一个项目，jcenter()自动被定义，而不是mavenCentral()。mavenCentral()表示依赖是从Central Maven 2 仓库中获取的，库的地址是\url{https://repo1.maven.org/maven2}。jcenter表示依赖是从Bintary’s JCenter Maven 仓库中获取的，仓库的地址是\url{https://jcenter.bintray.com}，bintray是一家提供全球企业软件开发包托管的商业公司。

\subsection{常见问题}

\paragraph{could not find method compile()}

gradle项目配置中引用java插件。


\section{Gradle对象}

\subsection{属性(Properties)}

\subsubsection{Extra Properties}

extra属性一般用于定义常量，All extra properties\footnote{\url{https://docs.gradle.org/current/javadoc/org/gradle/api/Project.html\#extraproperties}} must be defined through the "ext" namespace. Once an extra property has been defined, it is available directly on the owning object (in the below case the Project, Task, and sub-projects respectively) and can be read and updated. Only the initial declaration that needs to be done via the namespace.

\begin{lstlisting}[language=Java]
buildscript {
	ext {
		springBootVersion = '1.4.5.RELEASE'
		jacksonVersion = '2.8.7'
		springfoxVersion = '2.6.1'
		poiVersion = "3.14"
		aspectjVersion = '1.7.4'
	}
}
\end{lstlisting}

Reading extra properties is done through the "ext" or through the owning object.


ext.isSnapshot = version.endsWith("-SNAPSHOT")
if (isSnapshot) {
	// do snapshot stuff
}

\paragraph{定义目标jar包名}

有时在构建后需要定义目标jar包的名称。

\begin{lstlisting}[language=Java]
project(":web"){

	description = "web"
	
	jar{
		baseName = "dolphin-web"
	}
}
\end{lstlisting}

编译打包后的jar包名称即为dolphin-web.jar。

\subsection{Task}

对于build.gradle配置文件，当运行Gradle <Task> 时，Gradle会为我们创建一个Project的对象，来映射build.gradle中的内容。其中呢，对于不属于任何Task范畴的代码，Gradle会创建一个Script类的对象，来执行这些代码；对于Task的定义，Gradle会创建Task对象，并将它会作为project的属性存在(实际上是通过getTaskName完成的)。


\paragraph{拷贝文件}

在项目中需要根据不同的用途生成不同的文件，比如api需要单独打包，平台需要单独打包，但是api和平台的代码都是一套代码，只是配置不同罢了。此时需要Gradle在构建之后根据不同的用途生成不同的包名称，使用copy功能来实现。首先定义项目的动态属性：

\begin{lstlisting}[language=Java]
buildscript {
	ext {
		projectVersion = '1.1.11'
	}
}
\end{lstlisting}

然后在task中使用此属性即可。

\begin{lstlisting}[language=Java]
task copyfile(){
	println(projectVersion)
}
\end{lstlisting}

一个简单的拷贝任务，在生成完毕文件后，目的文件拷贝一个接口部署包副本，接口与应用程序单独部署，并重新命名。

\begin{lstlisting}[language=Java]
def ccCommonBuildScript = file("$rootDir/gradle/common.gradle")
apply from: ccCommonBuildScript
/**
* 复制一份接口的部署包
* 接口与平台单独部署，避免互相影响
*/
task copyTask(type: Copy) {
	from 'build/libs/credit-system-web-boot-' + version + '.jar'
	into 'build/libs/api'
	rename 'credit-system-web-boot-' + version + '.jar','credit-system-web-api-' + version + '.jar'
}
\end{lstlisting}

在这里，version变量是来自另外一个gradle文件。version变量在common.gradle中定义，在build.gradle文件中使用。

\subsection{同时连接内外网}

在工作中，有时需要同时连接封闭的网络和互联网，开发环境需要依赖封闭的内网，与远程的同事交流需要依赖互联网，而封闭的内网与互联网切换是一大痛点，不仅影响效率，同时也影响心情，宝贵的时间就在这样无意义的开关中白白浪费掉了。制造障碍很容易，清除障碍很艰难。

解决双网卡问题，既能保证内网访问的安全，又能避免频繁切换网卡带来的时间开销，可谓一举两得。一般的计算机有有线网卡，也有无线网卡。一般情况下要么使用有线网卡，要么使用无线网卡，在使用无线时，连接上了有线，因为有线网卡的优先级高，故此时仅有有线能够工作，无线网卡可连接但是无法传送数据。实现双网卡的基本思路是删除默认网关，配置各自的网关即可。

\subsubsection{Mac同时连内网外网}

输入如下命令查看Mac的所有网络连接方式：

\begin{lstlisting}[language=Bash]
networksetup -listallnetworkservices
\end{lstlisting}

输出的结果如下：

\begin{lstlisting}
An asterisk (*) denotes that a network service is disabled.
Apple USB Ethernet Adapter
Wi-Fi
Bluetooth PAN
Thunderbolt Bridge
\end{lstlisting}

可以看出Mac可以通过Wi-Fi联网，也可以通过USB有线联网。给指定的网络连接方式设定DNS服务器代码如下：

\begin{lstlisting}[language=Bash]
sudo networksetup -setdnsservers AirPort 192.168.10.200
\end{lstlisting}

清空DNS缓存代码如下：

\begin{lstlisting}[language=Bash]
dscacheutil –flushcache
\end{lstlisting}

输入如下命令查看MacBook的路由表：

\begin{lstlisting}[language=Bash]
netstat -nr
\end{lstlisting}

\subsubsection{Fedora同时连接内外网}

给有线网卡配置地址信息时，内网网卡不要加默认网关，外网网卡加默认网关，并查看无线网卡分配到的地址信息中是否有默认网关。在这里内网是有线网络，外网是无线网络。使用route命令查看默认路由，输出如图\ref{fig:routetable}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{route-table.jpg}
	\caption{查看Kernel路由表}
	\label{fig:routetable}
\end{figure}

其中wlp3s0是无线网卡的名称，此命名规范为v197\footnote{https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/}，说明此时的默认路由是无线网卡的路由。metric设置路由跳数，metric值越高，优先级越低。添加-p(permanent)参数永久保存此条路由信息，系统重启后路由也规则也不会丢失。flag的含义如下：

\begin{tabular}{cp{10cm}c}
	\hline
	\multirow{1}{*}{序号}
	& \multicolumn{1}{c}{名称}  \\
	\hline			
	U  & Up表示此路由当前为启动状态 \\	
	H & Host，表示此网关为一主机 \\
	G & Gateway，表示此网关为一路由器\\
	R & Reinstate Route，使用动态路由重新初始化的路由\\
	D & Dynamically,此路由是动态性地写入\\
	M & Modified，此路由是由路由守护程序或导向器动态修改\\
	! & 表示此路由当前为关闭状态\\
	\hline
\end{tabular}

删除默认网关的命令如下：

\begin{lstlisting}[language=Bash]
sudo route del default
\end{lstlisting}

删除一条路由：

\begin{lstlisting}[language=Bash]
sudo route del -net 192.168.122.0 netmask 255.255.255.0
\end{lstlisting}

如果默认路由是外网网关。那么只需要单独为内网设置转发特例，所有59.214.*.*开头的，全部走enp0s26u1u2:

\begin{lstlisting}[language=Bash]
sudo route add -net 59.214.0.0 netmask 255.255.255.0 gw 10.55.10.1 dev enp0s26u1u2
\end{lstlisting}

其中59.214.0.0为内网IP的起始网段，255.255.0.0为内网的子网掩码，内网网关是10.36.40.12，eth0为内网网卡的名称。路由添加最好是添加到开机启动中：

\begin{lstlisting}[language=Bash]
vim /etc/rc.local
\end{lstlisting}

添加删除默认网关：

\begin{lstlisting}[language=Bash]
#添加默认网关
sudo route add default gw 10.55.10.1
#删除默认网关
sudo route del default gw 10.55.10.1
\end{lstlisting}

重启网络服务：

\begin{lstlisting}[language=Bash]
/etc/init.d/networking restart
\end{lstlisting}

此时可以同时访问互联网和局域网中的主机，但是无法访问A网络。访问A网络可通过局域网代理的方式访问，将局域网的一台主机作为代理服务器，本机访问局域网中的代理服务器，代理服务器再将请求转发给A网络。在本机只需要在浏览器中配置好代理服务器即可。以FireFox浏览器为例，在Preference->Advance->Network->Connection->Settings中，增加手动代理配置，地址填写代理主机的地址，端口填写8888，代理服务器运行的是Fiddler。以后，访问内网时就使用FireFox浏览器，访问外网时就使用Google Chrome浏览器。


\subsubsection{Ubuntu同时连接内外网}

\subsubsection{Windows同时连接内外网}

\chapter{youtube-dl}

\chapter{Fiddler}

\subsection{Fiddler替换Host}

\chapter{Charles}

Charles是Mac端的一款截取与分析网络请求的工具，在网络开发中使用其作分析，可以大大提高我们的开发效率。Charles是收费软件，一般可以试用三十天。Charles is an HTTP proxy / HTTP monitor / Reverse Proxy that enables a developer to view all of the HTTP and SSL / HTTPS traffic between their machine and the Internet. This includes requests, responses and the HTTP headers (which contain the cookies and caching information).

\subsection{title}


\chapter{OpenVPN}

OpenVPN从2001年开始开发，使用的是C语言。此处使用的OpenVPN版本是2.4.1。如果使用Mac下的brew工具安装，则OpenVPN目录在：/usr/local/Cellar/openvpn/2.4.1，OpenVPN的配置文件在：/usr/local/etc/openvpn。目前OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Microsoft Windows以及Android和iOS上运行，并包含了许多安全性的功能。此处的服务器使用的是CentOS 7.3，客户端包含Fedora 24、Ubuntu 14.04、Ubuntu 16.04、Window 7、Windows 10。在OpenVPN网络中查看存活的主机：

\begin{lstlisting}[language=bash]
nmap -A -T4 10.0.0.*
\end{lstlisting}

\subsection{安装}

安装基础包：

\begin{lstlisting}[language=bash]
sudo yum -y install openssl openssl-devel lzo openvpn easy-rsa --allowerasing
#手动安装
wget -c https://swupdate.openvpn.org/community/releases/openvpn-2.4.1.tar.gz
tar -zxvf openvpn-2.4.1.tar.gz
./configure
make && make install
\end{lstlisting}

LZO 是致力于解压速度的一种数据压缩算法，LZO 是 Lempel-Ziv-Oberhumer 的缩写。这个算法是无损算法，参考实现程序是线程安全的。 实现它的一个自由软件工具是lzop。最初的库是用 ANSI C 编写、并且遵从 GNU通用公共许可证发布的。现在 LZO 有用于 Perl、Python 以及 Java 的各种版本。代码版权的所有者是 Markus F. X. J. Oberhumer。

\subsection{生成客户端证书}

生成客户端证书端步骤如下：

\begin{lstlisting}[language=Bash]
#建立一个空的pki结构，生成一系列的文件和目录
./easyrsa init-pki
\end{lstlisting}

PKI：Public Key Infrastructure公钥基础设施。生成请求：

\begin{lstlisting}[language=Bash]
./easyrsa gen-req dolphinfedora
\end{lstlisting}

输入PEM验证码。PEM - Privacy Enhanced Mail,打开看文本格式,以"-----BEGIN..."开头, "-----END..."结尾,内容是BASE64编码。
Apache和*NIX服务器偏向于使用这种编码格式.签约：

\begin{lstlisting}[language=Bash]
#切换到服务端生成rsa的目录
#导入req
./easyrsa import-req ~/client/easyrsa/easy-rsa-master/easyrsa3/pki/reqs/dolphinfedora.req dolphinfedora
#用户签约，根据提示输入服务端的ca密码
./easyrsa sign client dolphinfedora
\end{lstlisting}

PKI：Public Key Infrastructure公钥基础设施。输入PEM验证码。PEM - Privacy Enhanced Mail,打开看文本格式,以"-----BEGIN..."开头, "-----END..."结尾,内容是BASE64编码.查看PEM格式证书的信息:openssl x509 -in certificate.pem -text -noout。Apache和*NIX服务器偏向于使用这种编码格式.服务端生成的文件有：

\begin{tabular}{|c|p{5cm}|c|}
	\hline
	\multirow{1}{*}{文件名称}
	& \multicolumn{1}{c|}{说明(Purpose)} 
	& \multicolumn{1}{c|}{位置} \\			
	\cline{1-3}
	ca.crt  & 根证书(Root CA certificate)件 & Server+All Clients	\\
	\hline
	reqs/server.req  & &\\
	\hline
	reqs/dolphin.req  & &\\
	\hline
	private/ca.key & 根证书私钥(Root CA key) & key signing machine only\\
	\hline
	private/server.key && \\
	\hline
	issued/server.crt & 服务器证书Server Certificate & server only\\
	\hline
	issued/dolphin.crt && \\
	\hline
	dh.pem & Diffie Hellman parameters & server only \\
	\hline
\end{tabular}

客户端生成的文件有：

\begin{tabular}{|c|p{8cm}|c|}
	\hline
	\multirow{1}{*}{序号}
	& \multicolumn{1}{c|}{名称}  \\			
	\cline{1-2}
	private/dolphinclient.key  & \\
	\hline
	reqs/sdolphinclient.req & \\
	\hline
\end{tabular}

拷贝出客户端证书文件：

\begin{lstlisting}[language=Bash]
cp easyrsa/easy-rsa-master/easyrsa3/pki/ca.crt ~/dolphinfedora/
cp easyrsa/easy-rsa-master/easyrsa3/pki/issued/dolphinfedora.crt ~/dolphinfedora/
cp ~/client/easyrsa/easy-rsa-master/easyrsa3/pki/private/dolphinfedora.key ~/dolphinfedora/
\end{lstlisting}


启动OpenVPN：

\begin{lstlisting}[language=Bash]
sudo openvpn server.conf
# Mac下启动OpenVPN
sudo /usr/local/Cellar/openvpn/2.4.1/sbin/openvpn /usr/local/etc/openvpn/client.conf
# 需要以后台交互方式启动时
screen sudo openvpn client.conf
\end{lstlisting}

客户端端配置如下：

\begin{lstlisting}[language=Bash]
client         #指定当前VPN是客户端
dev tun        #必须与服务器端的保持一致
proto udp      #必须与服务器端的保持一致
#指定连接的远程服务器的实际IP地址和端口号
remote 192.168.1.106 1194      
#断线自动重新连接
#在网络不稳定的情况下(例如：笔记本电脑无线网络)非常有用
resolv-retry infinite
nobind         #不绑定特定的本地端口号
persist-key
persist-tun
ca ca.crt      #指定CA证书的文件路径
cert client1.crt       #指定当前客户端的证书文件路径
key client1.key    #指定当前客户端的私钥文件路径
ns-cert-type server      #指定采用服务器校验方式
#如果服务器设置了防御DoS等攻击的ta.key
#则必须每个客户端开启；如果未设置，则注释掉这一行；
tls-auth ta.key 1     
comp-lzo              #与服务器保持一致
#指定日志文件的记录详细级别，可选0-9，等级越高日志内容越详细
verb 3                
\end{lstlisting}

配置ns-cert-type(Netscape Cert Type)指定为server主要是防止中间人攻击(Man-in-the-Middle Attack)。在服务端做如下配置：

\begin{lstlisting}[language=Bash]
nsCertType server
\end{lstlisting}

\subsection{Mac Book客户端配置}

在Mac下连接OpenVPN需要安装Tunnelblick，Tunnelblick是Mac OS X上的一个OpenVPN的图形化前端界面。Tunnelblick is free software licensed under the GNU General Public License, version 2 and may be distributed only in accordance with the terms of that license.安装好Tunnelblick之后，直接导入相应的设置即可。如下是Mac Book下一个OpenVPN客户端可用配置的示例。

\begin{lstlisting}[language=Bash]
# 定义一个客户端
client
# 和服务器保持一致
dev tun
# 用TCP协议
proto tcp
# 指定服务器的IP地址和端口，可以用多行指定多台服务器，实现负载均衡
remote 106.14.30.1 1194
;remote-random
resolv-retry infinite
# 客户端不需要绑定端口
nobind
persist-key
persist-tun
mute-replay-warnings
comp-lzo
verb 3
;mute 20
ca ca.crt
cert dolphin.crt
key dolphinclient.key
\end{lstlisting}


\subsection{Ubuntu远程Raspberry}

OpenVPN的一个应用就是可以方便的在不同局域网之间的计算机进行远程。在Raspberry安装VNC服务端：

\begin{lstlisting}[language=Bash]
sudo apt-get install tightvncserver
\end{lstlisting}

设置密码：

\begin{lstlisting}[language=Bash]
tightvncserver
\end{lstlisting}

启动：

\begin{lstlisting}[language=Bash]
vncserver :2 -geometry 800x600 -depth 24
\end{lstlisting}

在Ubuntu下远程配置如图\label{raspberryremtoeconfig}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{raspberryremtoeconfig.png}
	\caption{Ubuntu远程连接配置}
	\label{fig:raspberryremtoeconfig}
\end{figure}

\subsection{Ubuntu远程Ubuntu}

这里的两台机器都是Ubuntu 16.04 LTS系统，xrdp支持不了13.10的GNOME了，解决办法是安装xfce界面。被远程的机器上安装必须的组件：

\begin{lstlisting}[language=Bash]
#安装xrdp
sudo apt-get install xrdp -y
#安装vnc4server
sudo apt-get install vnc4server tightvncserver -y
#安装xfce4
sudo apt-get install xubuntu-desktop -y
sudo /etc/init.d/xrdp restart
＃　指定桌面管理器
echo "gnome-session --session=gnome-classic" > ~/.xsession
\end{lstlisting}

最后一个命令的作用是由于安装了gnome桌面，ubuntu1６.04中同时存在unity、GNOME多个桌面管理器，需要启动的时候指定一个，不然即使远程登录验证成功以后，也只是背景。在Ubuntu上使用Remmina Remote Desktop Client即可链接远程桌面。

\subsection{Mac Book远程Fedora}

在Mac Book远程Linux主机选择的是XQuartz，安装好XQuartz后，直接在从XQuartz打开的终端中运行：

\begin{lstlisting}[language=Bash]
ssh -X dolphin@10.0.0.10
\end{lstlisting}

直接在命令行中启动图形化界面，会在Mac上显示Linux主机上的软件原生GUI界面，就像软件在本机运行一样。X11是X Window System主版本11的缩写，它不光是一个基本的GUI软件，X11也被定义为一个网络协议，因为X11提供了非常灵活的网络访问接口。SSH 的 X11 forwarding 特性可以使 X client 和 X server 安全地通讯。使用 X11 forwarding 后，从 X client 到 X Server 方向的数据先被送至 SSH server，SSH server 利用和 SSH client 的安全通道转发给 SSH client，再由 SSH client 转发给 X server，从 X server 到 X client 的数据流同理。这里 SSH server 和 SSH client 充当了 X client 和 X server 间数据的转发器，由于 SSH server 和 X client、SSH client 和 X server 一般在同一台机器上，它们之间是一种安全的进程间通讯，而 SSH server 和 SSH client 间的通讯也是安全的，所以 X client 和 X server 间的通讯就是安全的。X在Unix-like系统上几乎完全占据统治地位。但是仍然有人尝试提供替代品和更多的选择，过去曾经有 Sun Microsystems 的 NeWS ，但它遭到市场失败；还有 NeXT 的 Display PostScript ，它最终转变为苹果电脑的 Quartz for Mac OS X 。

\subsection{常见问题}

得到了Initialization Sequence Completed消息，但是ping失败了 -- 这通常是服务端或客户端上的防火墙过滤了TUN/TAP网络接口从而阻止了VPN网络的流量。  

\subsubsection{ssl3\_get\_server\_certificate:certificate verify failed}

在Mac中，连接好了并没有显示IP的变化。This computer's apparent public IP address was not different after connecting to client. It is still 45.76.205.201.This may mean that your VPN is not configured correctly.


\paragraph{OpenVPN is connected to the server but your IP address does not change}

If OpenVPN connects to the server properly but your IP address does not change, you are probably missing the "--redirect-gateway" option. Add the following line:
--redirect-gateway def1 to your configuration file.

By default, OpenVPN only sends some traffic through the VPN — traffic that is specifically destined for the VPN network itself. The "--redirect-gateway" option tells OpenVPN to send all traffic through the VPN. (Leave out the "--" when it is put in the configuration file.)

An alternative to putting "redirect gateway def1" in the configuration file is to "push" it from the VPN server to the client.

\chapter{Nmap(GNU General Public License)}

Nmap(Network Mapper)，网络映射器，是一款开放源代码的网络探测和安全审核的工具。
它的设计目标是快速地扫描大型网络，当然用它扫描单个主机也没有问题。
Nmap以新颖的方式使用原始IP报文来发现网络上有哪些主机，
那些主机提供什么服务(应用程序名和版本)，那些服务运行在什么操作系统(包括版本信息)，
它们使用什么类型的报文过滤器/防火墙，以及一堆其它功能。虽然Nmap通常用于安全审核，
许多系统管理员和网络管理员也用它来做一些日常的工作，比如查看整个网络的信息，
管理服务升级计划，以及监视主机和服务的运行。
Nmap脚本引擎(Nmap Script Engine)是Nmap最有力灵活的的一个特性。
它允许用户撰写和分享一些简单的脚本来一些较大的网络进行扫描任务。
基本上这些脚本是用Lua编程语言来完成的。通常Nmap的脚本引擎可以完成很多事情。

\subsection{主机发现（Host Discovery）}

主机发现顾名思义就是发现所要扫描的主机是否是正在运行的状态。
输入如下命令开始主机扫描

\begin{lstlisting}[language=Bash]
nmap -F -sT -v nmap.org
# 主机发现，生成存活主机列表
nmap -sn -T4 -oG Discovery.gnmap 192.168.24.0/24
grep "Status: Up" Discovery.gnmap | cut -f 2 -d ' ' > LiveHosts.txt
\end{lstlisting}

\begin{itemize}
	\item{-F}扫描100个最有可能开放的端口,F表示快速模式(Fast Model)，
	比默认模式扫描更少的端口（Scan fewer ports than the default scan），
	如果需要扫描更多的端口，指定p参数，如下命令行片段所示：
	
	\begin{lstlisting}[language=Bash]
	nmap -p 0-10000 -A -v <Host>
	\end{lstlisting}
	
	端口号最好分段指定，否则扫描的速度会非常缓慢，
	例如需要扫描0-65535端口，可以先扫描0-10000端口，
	再扫描10001-20000，依次类推。
	
	\item{-v}获取扫描的信息
	\item{-sT(Scan using TCP)}采用的是TCP扫描，不写也是可以的，默认采用的就是TCP扫描
	\item{-sn参数表示ping扫描，禁用端口扫描}
	\item{-oG表示输出Grepable格式}
\end{itemize}

Nmap输出的是扫描目标的列表，以及每个目标的补充信息，至于是哪些信息则依赖于所使用的选项。
“所感兴趣的端口表格”是其中的关键。那张表列出端口号，协议，服务名称和状态。
状态可能是 open(开放的)，filtered(被过滤的)， closed(关闭的)，或者unfiltered(未被过滤的)。
Open(开放的)意味着目标机器上的应用程序正在该端口监听连接/报文。 
filtered(被过滤的) 意味着防火墙，过滤器或者其它网络障碍阻止了该端口被访问，
Nmap无法得知 它是 open(开放的) 还是 closed(关闭的)。 
closed(关闭的) 端口没有应用程序在它上面监听，但是他们随时可能开放。 
当端口对Nmap的探测做出响应，但是Nmap无法确定它们是关闭还是开放时，
这些端口就被认为是 unfiltered(未被过滤的) 如果Nmap报告状态组合 
open|filtered 和 closed|filtered时，那说明Nmap无法确定该端口处于两个状态中的哪一个状态。
当要求进行版本探测时，端口表也可以包含软件的版本信息。
当要求进行IP协议扫描时 (-sO)，Nmap提供关于所支持的IP协议而不是正在监听的端口的信息。
获取远程主机的系统类型及开放端口：

\begin{lstlisting}[language=Bash]
nmap -sS -P0 -sV -O <target>
\end{lstlisting}

\subsection{端口扫描（Port Scanning）}

端口扫描是Nmap最基本最核心的功能，用于确定目标主机的TCP/UDP端口的开放情况。
端口扫描简单示例如下代码片段所示：

\begin{lstlisting}[language=Bash]
#服务扫描
nmap-T4 -sV targetip 

#扫描淘宝IP
nmap -p 0-1000 -v 218.201.46.124
nmap -T4 -A -v 218.201.46.124

# 扫描指定主机的3306端口
nmap -sS -p 3306 -v 192.168.31.25
\end{lstlisting}

通过扫描可以发现一些主机信息，比如淘宝的数字证书用的是GlobalSign，
加密位数是2048，签名算法是sha256WithRSAEncryption，
操作系统猜测是Linux系列的，内核版本可能是3.X或者2.6.X，
具体信息如下。

\begin{lstlisting}
443/tcp open  ssl/http Tengine httpd
|_http-server-header: Tengine
|_http-title: 501 Not Implemented
| ssl-cert: Subject: commonName=*.tmall.com/organizationName=Alibaba (China) Technology Co., Ltd./stateOrProvinceName=ZheJiang/countryName=CN
| Issuer: commonName=GlobalSign Organization Validation CA - SHA256 - G2/organizationName=GlobalSign nv-sa/countryName=BE
| Public Key type: rsa
| Public Key bits: 2048
| Signature Algorithm: sha256WithRSAEncryption
| Not valid before: 2015-12-14T10:38:38
| Not valid after:  2016-12-14T10:38:38
| MD5:   5d67 3ca3 7f0e 2ab7 7b4f 59d5 0700 7223
|_SHA-1: d048 e9cf 5487 5030 9f26 e638 7d3f 94ad a2b3 e6fa
\end{lstlisting}

默认情况下，Nmap会扫描1000个最有可能开放的TCP端口。

1）公认端口(0～1023)，又称常用端口，为已经公认定义或为将要公认定义的软件保留的。
这些端口紧密绑定一些服务且明确表示了某种服务协议。如80端口表示HTTP协议。
2）注册端口(1024～49151)，又称保留端口,这些端口松散绑定一些服务。
3）动态/私有端口（49152～65535)。理论上不应为服务器分配这些端口。
按协议类型可以将端口划分为TCP和UDP端口。
1）TCP端口是指传输控制协议端口，需要在客户端和服务器之间建立连接，
提供可靠的数据传输。如Telnet服务的23端口。
2）UDP端口是指用户数据包协议端口，不需要在客户端和服务器之间建立连接。
常见的端口有DNS服务的53端口。
有的服务器为了安全会修改默认端口，
比如将ssh默认的22端口修改为50000，
不过通过Nmap扫描也会很容易的发现。
详细的扫描指令如下所示：

\begin{lstlisting}[language=Bash]
nmap -sS -sU -T4 -A -v -PE -PP -PS80,443 -PA3389 -PU40125 -PY -g 53 --script "default or (discovery and safe)" 12.26.32.14 
\end{lstlisting}

\begin{itemize}
	\item{-sS参数(TCP SYN Scan)可以利用基本的SYN扫描方式探测其端口开放状态}
	\item{-sU参数代表UDP Scan}
	\item{-T<0-5>: Set timing template (higher is faster)}
	\item{-A: Enable OS detection, version detection, script scanning, and traceroute}
	\item{-v: Increase verbosity level (use -vv or more for greater effect)}
	\item{-PE/PP/PM: ICMP echo, timestamp, and netmask request discovery probes}
	\item{-PS/PA/PU/PY[portlist]: TCP SYN/ACK, UDP or SCTP discovery to given ports}
	\item{-g/--source-port <portnum>: Use given port number}
\end{itemize}


\paragraph{"ppp0" is not an ethernet device}在使用nmap扫描时候，提示 

\begin{lstlisting}
Only ethernet devices can be used for raw scans on Windows, and "ppp0" is not an ethernet device. Use the --unprivileged option for this scan. QUITTING!
\end{lstlisting}

因为用Nmap扫描时，不能用pppoe的拨号口，只能用以太网口。
根据提示带参数--unprivileged就可以了,注意扫描的命令不能带-A参数，
否则会要求root权限，无法启动扫描。 


\subsection{版本侦测（Version Detection）}

版本侦测在后面添加sV(Service/Version)参数即可。

\begin{lstlisting}[language=Bash]
-sV: Probe open ports to determine service/version info
\end{lstlisting}

\subsection{Zenmap}

Zenmap是Nmap的官方GUI，可以运行在Linux, Windows, Mac OS X, BSD等操作系统上。

\begin{lstlisting}[language=Bash]
# Mac下获取Zenmap安装文件
wget -c https://nmap.org/dist/nmap-7.31.dmg
\end{lstlisting}


\section{tmux}

tmux是一个优秀的终端复用软件，类似GNU Screen，但来自于OpenBSD，采用BSD授权。使用tmux的好处之一就是可以保存会话，避免网络不稳定带来的影响。

\begin{tabular}{|c|p{8cm}|c|}
	\hline
	\multirow{1}{*}{键}
	& \multicolumn{1}{c|}{作用}  \\			
	\cline{1-2}
	Ctrl + b d  & 返回主 shell ，detach, tmux 依旧在后台运行，里面的命令也保持运行状态\\
	\hline
	C-b ? & 显示快捷键帮助\\
	\hline
\end{tabular}


\chapter{Raspberry Pi}

\subsection{Raspberry Pi远程}

安装步骤：

\begin{lstlisting}[language=Bash]
＃　安装tightvncserver
sudo apt-get install tightvncserver　－ｙ
＃　设置密码
vncpasswd
\end{lstlisting}

\chapter{Widgets}

\section{Little Tool}

\subsection{Pandoc}

在编写文档时习惯使用\LaTeX{}来编写，\LaTeX{}生成的pdf文档美观，Word格式在不同平台(Mac OS X/Linux/Windows)不同的软件(Microsft Office/金山/Libre Office)打开格式可能会不一致，但是有许多同事习惯使用Word，有时发给对方时也需要作一些编辑和调整，Pandoc就可以满足既可以用\LaTeX{}来编辑文档，也可以将Tex文档转换为Word方便同事：

\begin{lstlisting}[language=Bash]
pandoc -s interface.tex -o example30.docx
\end{lstlisting}

另外使用\LaTeX{}还有一个比较方便的地方是可以容易的以模块来管理文档，比如接口文档分为公共部分，所有调用方都可以看到，但是针对每个调用方可能会有一些定制的内容，当要修改公共部分时，如果是采用Word文件分散管理会出现不一致的情况，有的文件修改了，有的没有改。使用\LaTeX{}就只需要改动一个公共模块即可。其他引用公共的地方自动更新。

\subsection{Zeal}

Zeal是一个离线的文档浏览工具。查看Nginx Stream相关文档信息，示例如图\ref{fig:zealsearch}所示。搜索以nginx和冒号开始，表示搜索与Nignx相关的信息。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{zealsearch.png}
	\caption{Zeal文档搜索示例}
	\label{fig:zealsearch}
\end{figure}

\subsection{jq}

项目里同事根据接口返回的数据建立数据库表格时，需要看格式化后的Json，由于只能在终端中请求接口数据，未格式化的Json返回根本没法阅读。由于目前还未发现Json格式化工具，所以只能时粘贴Json到网络上的在线网页里面进行格式化，由于网络用的是物理隔离的内网，甭提有多麻烦了，而且都是重复劳动，而jq采用C语言编写(开始还以为是用Python写的)，没有运行时依赖(jq is written in portable C, and it has zero runtime dependencies.)，直接在服务器上源码编译安装即可(服务器物理隔离，只能源码安装)。没有它，估计耗费的时间不下几个小时。jq直接在终端中就可以查看格式化好的Json。需要说明的是 jq 只能接受 well form 的 JSON 字符串作为输入内容。也就是说输入内容必须严格遵循 JSON 格式的标准。所有的属性名必须是以双引号包括的字符串。对象的最后一个属性的末尾或者数组的最后一个元素的末尾不能有逗号。否则 jq 会抛出无法解析 JSON 的错误。


\paragraph{常用模式}

获取所有的key：

\begin{lstlisting}[language=Bash]
echo '{"foo": 42, "bar": "less interesting data"}' | jq 'keys'
# 获取某一个value的值
echo '{"foo": 42, "bar": "less interesting data"}' | jq '.bar'
\end{lstlisting}

嵌套数据查看：

\begin{lstlisting}[language=Bash]
curl -H "APPID:" -H "TIMESTAMP:2016-12-19 16 (tel:2016121916):58:02" -H "ECHOSTR:" -H "TOKEN:" http://10.10.1.12:28080/api/xzxk?xdr= | jq '.data.totalElements'
\end{lstlisting}

\section{Nginx}

安装配置好nginx服务器后默认目录是/usr/share/nginx/html。nginx模块一般被分成三大类：handler、filter和upstream。Mac下Nginx配置文件的路径为：/usr/local/etc/nginx/nginx.conf。

\subsection{X-Forwarded-For}

X-Forwarded-For是一个HTTP扩展头部。HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP。如今它已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 RFC 7239（Forwarded HTTP Extension）标准之中。在默认情况下，Nginx并不会对X-Forwarded-For头做任何的处理，除非用户使用proxy\_set\_header参数设置：

\begin{lstlisting}[language=bash]
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
\end{lstlisting}

X-Forwarded-For 请求头格式:

\begin{lstlisting}[language=bash]
X-Forwarded-For: IP0, IP1, IP2
\end{lstlisting}

XFF 的内容由「英文逗号 + 空格」隔开的多个部分组成，最开始的是离服务端最远的设备 IP，然后是每一级代理设备的 IP。

\subsection{Nginx获取真实IP}

做异地登陆的判断，或者统计ip访问次数等，通常情况下我们使用request.getRemoteAddr()就可以获取到客户端ip，但是当我们使用了nginx作为反向代理后，使用request.getRemoteAddr()获取到的就一直是nginx服务器的ip的地址。Nginx作为HTTP代理转发前端时，后端服务无法获知前端访问客户的IP地址。要获取客户端IP，需要将http\_realip\_module编译进入Nginx中。查看Nginx安装了哪些模块，使用如下命令：

\begin{lstlisting}[language=bash]
nginx -V
\end{lstlisting}

在命令的输出结果中，可以看到Nginx的版本，Nginx的编译参数，和Nginx已经包含有哪些模块。此处显示已经编译了http\_realip\_module模块。在服务器上的项目中使用的tengine，经过查看是没有编译http\_realip\_module模块，从tengine官网上下载好源码，输入如下命令进行编译：

\begin{lstlisting}[language=bash]
# 在Ubuntu 14.04 LTS下安装pcre依赖
# pcre:Perl Compatible Regular Expressions
sudo apt-get install libpcre3 libpcre3-dev
# 指定预编译参数，仅仅指定路径
sudo ./configure --prefix=/opt/tengine
# 将realip模块编译进入
sudo ./configure --prefix=/opt/tengine --with-http_realip_module
# 编译
sudo make
# 安装
sudo make install
./configure --prefix=/usr/local/tengine-2.1.2 --with-openssl=/Users/dolphin/source/openssl
\end{lstlisting}

不指定prefix，则可执行文件默认放在/usr/local/bin，库文件默认放在/usr/local/lib，配置文件默认放在/usr/local/etc。其它的资源文件放在/usr /local/share。
你要卸载这个程序，要么在原来的make目录下用一次make uninstall（前提是make文件指定过uninstall）,要么去上述目录里面把相关的文件一个个手工删掉。
指定prefix，直接删掉一个文件夹就够了，这也是制定prefix一个比较方便的地方。在编译时，还需要指定pcre的版本，因为Ubuntu 16.04 LTS里有时安装有pcre 3，而部署的电脑上不一定安装有pcre 3，而且pcre 3的源码不容易找到。所以编译命令如下：

\begin{lstlisting}[language=bash]
# 将realip模块编译进入，并指定pcre
sudo ./configure --prefix=/opt/tengine --with-http_realip_module --with-pcre=/root/software/pcre-8.40 --with-openssl=/root/software/openssl-OpenSSL_1_1_0e --without-http_gzip_module

./configure --prefix=/opt/tengine --with-http_realip_module --with-pcre=/root/software/pcre-8.40  --without-http_gzip_module

#本机编译（不成功）
./configure --prefix=/opt/tengine --with-http_realip_module --with-pcre=/home/hldev/Downloads/pcre-8.40 --with-openssl=/home/hldev/Downloads/openssl-OpenSSL_1_0_1e --without-http_gzip_module

#本机编译（成功）
-prefix=/opt/tengine --with-http_realip_module --with-pcre=/home/hldev/Downloads/pcre-8.40 --with-openssl=/home/hldev/software/openssl-OpenSSL_1_0_2g --without-http_gzip_module

# 构建程序
make

#安装程序
make install

#服务器
./configure --prefix=/opt/tengine

./configure: error: SSL modules require the OpenSSL library.
You can either do not enable the modules, or install the OpenSSL library
into the system, or build the OpenSSL library statically from the source
with nginx by using --with-openssl=<path> option.


\end{lstlisting}

其中--with-pcre表示pcre(Perl Compatible Regular Expressions)的源代码目录。在编译时，还需要注意openssl的版本，本地电脑版本是1.0.2g，服务端的版本是1.0.1e。服务器端编译Nginx的一个命令：

\begin{lstlisting}[language=bash]
./configure --prefix=/opt/tengine --with-openssl=/usr/local/src/openssl-OpenSSL_1_0_2g/ --without-http_gzip_module --with-pcre=/usr/local/src/pcre-8.40/
\end{lstlisting}

这里编译的时候有一个小细节需要注意，需要将源码包存放到/usr/local/src目录下。一直没有编译成功也许是这个小小的细节，开始时是将源码包随意存放的一个目录。以上编译的命令没有包含Zlib，也没有包含http\_realip\_module模块。最终的编译命令如下：

\begin{lstlisting}[language=bash]
./configure --prefix=/opt/tengine --with-openssl=/usr/local/src/openssl-OpenSSL_1_0_2g/ --with-pcre=/usr/local/src/pcre-8.40/ --with-zlib=/usr/local/src/zlib-1.2.11 --with-http_realip_module 
\end{lstlisting}

编译完毕之后，输入如下命令：

\begin{lstlisting}[language=bash]
# 查看Tengine版本、模块信息，编译时所使用的参数
./nginx -V
\end{lstlisting}

即可看到http\_realip\_module已经编译到Nginx中了。Nginx的http realip module等于Apache的mod\_rpaf，用于接受前端发来的IP head信息，从获取到真实的用户IP。将获取真实IP的模块编译进入Nginx后，还需要在对应的http、server、location中加入以下参数：

\begin{lstlisting}[language=bash]
#指令是告诉nginx，10.10.1.11是我们的反向代服务器，不是真实的用户IP
set_real_ip_from 10.10.1.11;
#告诉nginx真正的用户IP是存在X-Forwarded-For请求头中
real_ip_header X-Real-IP;
\end{lstlisting}

其中set\_real\_ip\_from可以指定某个网段。这个指令指定信任的代理IP，它们将会以精确的替换IP转发。0.8.22后可以指定Unix sockets。real\_ip\_header设置需要使用哪个头来确定替换的IP地址。由于项目中使用的Nginx作为反向代理，所以配置如下：

\begin{lstlisting}[language=bash]
location /inapi {
	proxy_pass http://localhost:28080;
	proxy_set_header X-Real-IP $remote_addr;
	proxy_redirect off;
}
\end{lstlisting}

在另一台电脑上访问服务端，在后端根据新添加的Header获取到的IP地址如图\ref{fig:nginxgetrealip}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{nginxgetrealip.png}
	\caption{Nginx获取用户真实访问IP}
	\label{fig:nginxgetrealip}
\end{figure}

此处获取到的IP是用户的真实IP，而不是反向代理服务器的IP。

\subsection{Nginx并发}

如何设置能限制某个IP某一时间段的访问次数是一个让人头疼的问题，特别面对恶意的ddos攻击的时候。其中CC攻击（Challenge Collapsar）是DDOS（分布式拒绝服务）的一种，也是一种常见的网站攻击方法，攻击者通过代理服务器或者肉鸡向向受害主机不停地发大量数据包，造成对方服务器资源耗尽，一直到宕机崩溃。

cc攻击一般就是使用有限的ip数对服务器频繁发送数据来达到攻击的目的，nginx可以通过HttpLimitReqModul和HttpLimitZoneModule配置来限制ip在同一时间段的访问次数来防cc攻击。

HttpLimitReqModule用来限制连单位时间内连接数的模块，使用limit\_req\_zone和limit\_req指令配合使用来达到限制。一旦并发连接超过指定数量，就会返回503错误。limit\_req\_zone 用来限制单位时间内的请求数，即速率限制,采用的漏桶算法 “leaky bucket”。

\begin{lstlisting}[language=bash]
http {
	limit_conn_log_level error;
	limit_conn_status 503;
	#Zone=one或allips表示设置了名为“one”或“allips”的存储区
	#大小为10兆字节
	limit_conn_zone $binary_remote_addr zone=one:10m;
	limit_conn_zone $server_name zone=perserver:10m;
	#rate=10r/s 的意思是允许1秒钟不超过1000个请求
	limit_req_zone $binary_remote_addr zone=allips:100m rate=1000r/s;  
	server {
		limit_conn one 100;
		limit_conn perserver 3000;
		limit_req zone=allips  burst=5  nodelay;
	}
}
\end{lstlisting}

HttpLimitConnModule用来限制单个ip的并发连接数，使用limit\_zone和limit\_conn指令，这两个模块的区别前一个是对一段时间内的连接数限制，后者是对同一时刻的连接数限制。

\subsection{URI长度限制}

在Http1.1协议中并没有提出针对URL的长度进行限制，RFC协议里面是这样描述的，HTTP协议并不对URI的长度做任何的限制，服务器端必须能够处理任何它们所提供服务多能接受的URI，并且能够处理无限长度的URI,如果服务器不能处理过长的URI,那么应该返回414状态码。接触的最多的服务器类型就是Nginx和Tomcat,对于url的长度限制，它们都是通过控制http请求头的长度来进行限制的，nginx的配置参数为large\_client\_header\_buffers，tomcat的请求配置参数为maxHttpHeaderSize。

\begin{lstlisting}[language=bash]
# Nginx配置HTTP请求头长度
client_header_buffer_size 2048k;
\end{lstlisting}

项目采用Spring Boot，所使用的Tomcat为内嵌的Tomcat。在application.properties中，增加如下配置:

\begin{lstlisting}[language=bash]
#单位为KB,如果没有指定，默认为8192(8KB)
server.max-http-header-size=1024
\end{lstlisting}

奇怪的是，在本机设置的1024KB可以查询，部署到服务器上，参数必须再调大才能够查询，否则提示请求头太大的错误。真的遇到鬼了。在不同的操作系统上单位不一样吗？见鬼

\subsection{开启gzip压缩}

Nginx开启Gzip压缩配置如下：

\begin{lstlisting}[language=Bash]
gzip on;
gzip_min_length 1k;
gzip_buffers 4 16k;
gzip_comp_level 2;
gzip_types application/javascript application/json text/plain application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;
gzip_vary off;
gzip_disable "MSIE [1-6]";
\end{lstlisting}

用curl测试是否开启了压缩：

\begin{lstlisting}[language=Bash]
curl -I -H "Accept-Encoding: gzip, deflate" "http://creditsystem.test/main"
curl -I -H "Accept-Encoding: gzip, deflate" "http://10.10.1.11/main"
\end{lstlisting}

开启压缩成功后，可以看到一句：Content-Encoding: gzip。

\subsection{Nginx配置}

简单的配置：

\begin{lstlisting}[language=bash]
user www-data;
worker_processes 4;
pid /run/nginx.pid;

events {
	worker_connections 768;
}

http {
	server{
		listen 80;
		# 定义网站默认根目录
		root /opt/dolphin/frontend/dist;
		
		location /api {
			proxy_pass http://localhost:8011;
		}
		
		location / {
			root /opt/dolphin/frontend/dist;
			index index.html;
		} 
	}
}
\end{lstlisting}

\subsection{常见问题}

\paragraph{open() "/opt/tengine/conf/nginx.conf" failed (2: No such file or directory)}

以上的路径是编译时Nginx配置文件的路径，如果在启动时不指定配置文件和日志文件的路径，那么Nginx默认使用的时编译时的文件路径，但是在这里已经将编译文件移动到另外的文件夹里面了，所以需要显示的指定配置文件的路径：

\begin{lstlisting}[language=bash]
./nginx -c /opt/app/local/tengine/conf/nginx.conf
\end{lstlisting}

也可以指定prefix文件目录，这样就会更改默认的Nginx主目录了：

\begin{lstlisting}[language=bash]
/opt/app/local/tengine/sbin/nginx -c /opt/app/local/tengine/conf/nginx.conf -p /opt/app/local/tengine
\end{lstlisting}

其中opt目录是编译时指定的目录，/opt/app/local是新的目录，用p参数指定新目录即可。同理，在刷新配置的时候也需要显示的指定Nginx的主目录：

\begin{lstlisting}[language=bash]
./nginx -p /opt/app/local/tengine -s reload
./nginx -p /home/app/local/tengine -V
\end{lstlisting}

\paragraph{error while loading shared libraries: libpcre.so.3: cannot open shared object file}

找不到libpcre.so.3文件，在本机搜索了之后，发现文件在目录下：

\begin{lstlisting}[language=bash]
/lib/i386-linux-gnu/libpcre.so.3
/lib/x86_64-linux-gnu/libpcre.so.3
\end{lstlisting}

\paragraph{error while loading shared libraries: libssl.so.1.0.0: cannot open shared object file}

有可能是由于openssl库的位置不对导致的，可以尝试创建软链接解决：

\begin{lstlisting}[language=bash]
ln -s /usr/local/lib64/libssl.so.1.1 /usr/lib64/libssl.so.1.1  
ln -s /usr/local/lib64/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1  
\end{lstlisting}

.so 为共享库,是shared object,用于动态连接的,和dll差不多。openssl：多用途的命令行工具，各功能分别使用子命令实现，libcrypto：公共加密库（存放了各种加密算法），libssl：ssl协议的实现。在Ubuntu下可以通过如下命令安装libssl.so.1.0.0:

\begin{lstlisting}[language=bash]
sudo apt-get install libssl1.0.0:amd64
sudo apt-get install libssl1.0.0 libssl-dev

#查看OpenSSL版本
openssl version

#查看系统中对libssl
find ／ -name libssl.s0*

# 列出openssl包
yum list \*openssl\*
\end{lstlisting}

不过此处不能通过命令安装，只能通过源码编译安装。这里有包： \url{https://pkgs.org/download/libssl1.0.0}。查看当前操作系统支持的openssl：

\begin{lstlisting}[language=bash]
yum --showduplicates list openssl
\end{lstlisting}

寻找包含libssl.so.1.0.0的安装包：

\begin{lstlisting}[language=bash]
yum provides */libssl.so.1.0.0
\end{lstlisting}

在http://rpm.pbone.net/上搜索 openssl1-1.0.0 ,搜到openssl1-1.0.0-4.fc24.x86\_64.rpm。

\url{http://rpmfind.net/linux/rpm2html/search.php?query=openssl&submit=Search+...&system=CentOS&arch=}

\section{Ansible}

\subsection{Ansible模块}

Ansible在Mac下的安装路径/usr/local/Cellar/ansible/2.3.1.0。在Mac下没有默认创建Ansible配置文件，手动创建即可。

\paragraph{Ansible配置}

Ansible主机配置：

\begin{lstlisting}[language=bash]
[pi]
192.168.31.25 ansible_ssh_port=22 ansible_ssh_user=pi
\end{lstlisting}


\paragraph{shell模块}使用shell模块，在远程命令通过/bin/sh来执行；所以，我们在终端输入的各种命令方式，都可以使用； 但是我们自己定义在.bashrc/.bash\_profile中的环境变量shell模块由于没有加载，所以无法识别；如果需要使用自定义的环境变量，就需要在最开始，执行加载自定义脚本的语句；对shell模块的使用可以分成两块：如果待执行的语句少，可以直接写在一句话中：

\begin{lstlisting}[language=bash]
ansible myservers  -a ". .bash_profile;ps -fe |grep sa_q" -m shell
\end{lstlisting}

如果在远程待执行的语句比较多，可写成一个脚本，通过copy模块传到远端，然后再执行；但这样就又涉及到两次ansible调用；对于这种需求，ansible已经为我们考虑到了，script模块就是干这事的；

\paragraph{copy模块}copy模块是将本机中的文件复制到远程主机当中。如果在复制文件过程中需要使用变量，可以使用template模块。

\begin{lstlisting}[language=bash]
ansible shuitu-front-webservers -m copy -a "src=./dist.tar.gz dest=/root/app-soft/"
\end{lstlisting}

\section{ssh}

\subsection{免密登陆}

免密登录需要注意的是，.ssh文件夹下的authorize\_key文件的权限需要是600。

\begin{lstlisting}[language=Bash]
ssh -p 22 pi@192.168.31.25 'mkdir -p .ssh && cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub
\end{lstlisting}

\subsection{ssh连接慢}

在使用SSH时，每次连接建立都相当的慢啊，要20秒以上才能够登录上去，严重影响工作效率。在服务端的/etc/ssh/sshd\_config文件中，修改配置，将GSSAPIAuthentication默认设置为关闭即可，配置文件修改后需要重新启动sshd守护进程配置才能够生效：

\begin{lstlisting}[language=Bash]
GSSAPIAuthentication　no
# 重启sshd守护进程
sudo service sshd restart
\end{lstlisting}

GSSAPI ( Generic Security Services Application Programming Interface) 是一套类似Kerberos 5 的通用网络安全系统接口。该接口是对各种不同的客户端服务器安全机制的封装，以消除安全接口的不同，降低编程难度。但该接口在目标机器无域名解析时会有问题
使用strace查看后发现，ssh在验证完key之后，进行authentication gssapi-with-mic，此时先去连接DNS服务器，在这之后会进行其他操作。

\subsection{Permission denied (publickey)}

在ssh登陆机器事，提示Permission denied (publickey)。可以尝试的方法，在连接命令中加上-vvv参数，观察详细的调试输出:

\begin{lstlisting}[language=Bash]
ssh -p 2222 -vvv hldev@10.0.0.22
\end{lstlisting}

设置文件夹对应的权限：

\begin{lstlisting}[language=Bash]
sudo chmod 700 .ssh
sudo chmod 600 .ssh/authorized_keys
\end{lstlisting}

登陆服务器观察相应的日志输出：

\begin{lstlisting}[language=Bash]
tail -f /var/log/auth.log
\end{lstlisting}

是不是服务器关闭了密码登陆？只能使用公钥认证登陆。后面检查确实如此，服务器端为了安全考虑，关闭了基于密码登录的方式，但是需要登录的主机并没有将自己的公钥拷贝到服务器上。所以服务器直接提示了Permission denied (publickey)，解决的办法就是在服务器端暂时开启密码登录，在/etc/ssh/sshd\_config中调整配置：

\begin{lstlisting}[language=Bash]
#允许使用基于密钥认证的方式登陆
PubkeyAuthentication yes 
\end{lstlisting}


\subsection{Session时间}

在使用ssh的过程中，经常会遇到一会儿没有操作就自动断开了，不是非常方便。

\paragraph{ClientAliveInterval}

修改/etc/ssh/sshd\_config配置文件 ClientAliveInterval 300（默认为0），参数的是意思是每5分钟，服务器向客户端发一个消息，用于保持连接，使用service sshd reload 让其修改后生效。如果发现还是有问题，可以试着把300设置小一点，例如60。

\paragraph{ClientAliveCountMax}

另外,至于ClientAliveCountMax, 使用默认值3即可.ClientAliveCountMax表示服务器发出请求后客户端没有响应的次数达到一定值, 就自动断开。

\paragraph{ControlPersist 4h}

在./ssh/config中添加一行：

\begin{lstlisting}[language=Bash]
ControlPersist 4h
\end{lstlisting}

When used in conjunction with ControlMaster, specifies that the master connection should remain open in the background (waiting for future client connections) after the initial client connection has been closed. If set to no, then the master connection will not be placed into the background, and will close as soon as the initial client connection is closed. If set to yes or 0, then the master connection will remain in the background indefinitely (until killed or closed via a mechanism such as the “ssh -O exit”). If set to a time in seconds, or a time in any of the formats documented in sshd\_config, then the backgrounded master connection will automatically terminate after it has remained idle (with no client connections) for the specified time\footnote{\url{http://man.openbsd.org/ssh_config.5}}.现在你每次通过SSH与服务器建立连接之后，这条连接将被保持4个小时，即使在你退出服务器之后，这条连接依然可以重用，因此，在你下一次（4小时之内）登录服务器时，你会发现连接以闪电般的速度建立完成，这个选项对于通过scp拷贝多个文件提速尤其明显，因为你不在需要为每个文件做单独的认证了。

\subsection{代理转发}

\paragraph{本地转发Local Forward}将本地机(客户机)的某个端口转发到远端指定机器的指定端口. 工作原理是这样的, 本地机器上分配了一个 socket 侦听 port 端口, 一旦这个端口上有了连接, 该连接就经过安全通道转发出去, 同时远程主机和 host 的 hostport 端口建立连接. 可以在配置文件中指定端口的转发. 只有 root 才能转发特权端口. 

\paragraph{远程转发Remote Forward}

\section{VisualVM}

使用nmap扫描1099端口，看jstatd是否生效。

\subsection{VisualVM远程监控}

jstatd是一个监控 JVM 从创建到销毁过程中资源占用情况并提供远程监控接口的 RMI （ Remote Method Invocation ，远程方法调用）服务器程序，它是一个 Daemon 程序，要保证远程监控软件连接到本地的话需要 jstatd 始终保持运行。jstatd运行需要通过 -J-Djava.security.policy=*** 指定安全策略，因此我们需要在服务器上建立一个指定安全策略的文件jstatd.all.policy ，文件内容如下：

\begin{lstlisting}[language=Bash]
grant codebase "file:${java.home}/../lib/tools.jar" {  
	permission java.security.AllPermission;  
}; 
\end{lstlisting}


在服务端运行jstatd守护程序：

\begin{lstlisting}[language=Bash]
./bin/jstatd -J-Djava.security.policy=jstatd.all.policy -J-Djava.rmi.server.hostname=192.168.1.101 -J-Djava.rmi.server.logCalls=true -p 1011
#在树莓派上启动jstatd
sudo /usr/bin/jstatd -J-Djava.security.policy=/opt/dolphin/backend/jstatd.all.policy -J-Djava.rmi.server.hostname=192.168.31.25 -J-Djava.rmi.server.logCalls=true -p 1011
\end{lstlisting}

使用VisualVM监控远程主机上JAVA应用程序时，需要开启远程主机上的远程监控访问，或者在远程JAVA应用程序启动时，开启远程监控选项，两种方法，选择其中一种就可以开启远程监控功能，配置完成后就可以在本地对远程主机上的JAVA应用程序进行监控。注意下载地址\footnote{\url{https://visualvm.github.io/uc/release139/updates.xml.gz}}需要手动修改一下，原装的地址已经失效。VisualVM中做相应配置即可。连接后如图\ref{fig:visualvmconnectremote}所示.

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{visualvmconnectremote.png}
	\caption{VisualVM远程监控}
	\label{fig:visualvmconnectremote}
\end{figure}

目前可以通过VisualVM远程监控实现查看远程应用的PID、查看远程应用的启动参数、所使用的配置文件、JVM的版本以及路径、GC信息等等。


\subsection{autojump}

autojump可以让你一键跳转到目标目录，一般情况下，很难准确的记忆清楚目标目录的准确路径，特别是目录嵌套比较深的时候。不停的重复使用cd命令一级或多级跳转显得有点笨拙。此时就可以使用autojump命令，只需要输入路径的关键字即可，比如只记得路径中隐约包含有关键字“jiangxiaoqiang”，那么仅仅输入命令autojump jiang，就可以自动跳转到对应的路径下。安装autojump：

\begin{lstlisting}[language=Bash]
sudo apt-get install autojump -y
# 给autojump命令设置别名
alias j='autojump'
# 从源代码安装autojump
git clone git://github.com/joelthelion/autojump.git
chmod 755 install.py
./install.py

# 将当前路径添加到autojump的搜索库
autojump -a 'pwd'
\end{lstlisting}

为了进一步简化目录跳转命令，可以直接添加命令autojump的别名：

\begin{lstlisting}[language=Bash]
alias j='autojump'
\end{lstlisting}

有时会出现如下的提示：Please source the correct autojump file in your shell's startup file. For more information, please reinstall autojump and read the post installation instructions.为了暂时激活autojump应用，即直到你关闭当前会话或打开一个新的会话之前让autojump均有效，需要以常规用户身份运行下面的命令:

\begin{lstlisting}[language=Bash]
# 在Linux系统下
source /usr/share/autojump/autojump.sh on startup
# 在Mac OS X下
source ~/.autojump/etc/profile.d/autojump.sh
# 也可运行如下命令
[[ -s /Users/dolphin/.autojump/etc/profile.d/autojump.sh ]] && source /Users/dolphin/.autojump/etc/profile.d/autojump.sh
\end{lstlisting}

为了使得autojump在BASH shell中永久有效，需要运行下面的命令。

\begin{lstlisting}[language=Bash]
echo '. /usr/share/autojump/autojump.sh'>>~/.bashrc
# 在Mac OS X下
echo '. ~/.autojump/etc/profile.d/autojump.sh'>>~/.bashrc
\end{lstlisting}

\section{Graphviz}

\subsection{编译}

一个简单的图形示例：

\begin{lstlisting}
digraph G {
	node [peripheries=2 style=filled color="#eecc80"]
	edge [color="sienna" fontcolor="green"]
	"componentWillMount" -> "serviceComponent";
	"serviceComponent" -> "axios";
	"axios" -> "dishatch data to store"[label="XHR(GET/POST/PUT/DELETE)"] ;
	"dishatch data to store" -> "props get data";
}
\end{lstlisting}

\begin{lstlisting}[language=Bash]
dot -Tjpg -Gdpi=1024 maven-lifecycle.dot -o maven-lifecycle.jpg
\end{lstlisting}

\section{MyBatis Generator}

\subsection{title}

\section{Curl}

内部接口测试：

\begin{lstlisting}[language=Bash]

\end{lstlisting}

外部接口测试：


\begin{lstlisting}[language=Bash]

\end{lstlisting}

\chapter{Git}

\section{基础}

\subsection{配置}

常用语句。

\begin{lstlisting}[language=Bash]
#增加远程库
git remote add company-internal http://xiaoqiang.jiang@gitlab.data.com/backend/system.git
#删除远程库
git remote rm company-
#修改远程库地址
git remote set-url origin http://gitlab.hualongdata.com/internal/work-record.git
#列出git配置
git config --list
\end{lstlisting}

\subsection{返回到指定版本}

查看提交历史：

\begin{lstlisting}[language=Bash]
git log --all --graph -12
\end{lstlisting}

回滚到指定版本：

\begin{lstlisting}[language=Bash]
git reset --hard e4177ea9061a9726efd42d289f73553409df793d
\end{lstlisting}

结果如图\ref{fig:filemodifyhistory}所示，即可看到由谁修改。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{filemodifyhistory.png}
	\caption{Git查看文件修改历史}
	\label{fig:filemodifyhistory}
\end{figure}


\subsection{查看修改历史}

在项目中看到有几行代码，百思不得其解，也没有注释，所以想看一看作者，与作者沟通，使用如下命令即可：

\begin{lstlisting}[language=Bash]
git blame PubapiOrgController.java
\end{lstlisting}

会显示每一行代码的作者(最后修改者)和最后的修改时间。有的时候，不仅仅需要看某(几)行代码最后的修改，也希望看到最近几个版本的修改。比如有一次部署上去后，发现数据库链接被改掉了，那么可以使用如下命令查看文件的修改历史：

\begin{lstlisting}[language=Bash]
git log --follow application-jjxxzx-test.properties
\end{lstlisting}

会显示出指定文件的修改历史记录，使用如下命令查看详细的修改内容：

\begin{lstlisting}[language=Bash]
git show e78c8f6561060799a3db1aeab3cd00b8d30b9820
\end{lstlisting}

\subsection{常见问题}


\paragraph{error setting certificate verify location}

当你通过HTTPS访问Git远程仓库，如果服务器的SSL证书未经过第三方机构签署，那么Git就会报错。这是十分合理的设计，毕竟未知的没有签署过的证书意味着很大安全风险。但是，如果你正好在架设Git服务器，而正式的SSL证书没有签发下来，你为了赶时间生成了自签署的临时证书。

\begin{lstlisting}[language=Bash]
env GIT_SSL_NO_VERIFY=true git clone https://<host_name/git/project.git
\end{lstlisting}

或者将Git设置为不允许ssl认证：

\begin{lstlisting}[language=Bash]
git config --system http.sslverify false
\end{lstlisting}

\subsection{rsync}

在使用scp命令时，无法实现断点续传。rsync是类unix系统下的数据镜像备份工具——remote sync。一款快速增量备份工具 Remote Sync，远程同步 支持本地复制，或者与其他SSH、rsync主机同步。




\subsection{merge}

配置Meld为默认合并工具：

\begin{lstlisting}[language=Bash]
git config --global merge.tool meld
\end{lstlisting}

\chapter{Google Chrome}

\section{DevTools}

\begin{tabular}{|c|p{6cm}|}
	\hline
	\multirow{1}{*}{快捷键}
	& \multicolumn{1}{c|}{备注}\\			
	\hline
	Ctrl+Shift+J (Windows/Linux)  & 打开DevTools Console面板 \\
	\hline
	Ctrl+Shift+I (Windows/Linux)  & 打开DevTools Network面板，I是Inspect的缩写 \\
	\hline
	Ctrl+Shift+P (Windows/Linux)  & 打开所有窗口 \\
	\hline
\end{tabular}

\subsection{Console}

\paragraph{Log XMLHttpRequests}

想要看到使用中应用发送的 XHR 请求，可以打开 settings 面板（译注：打开调试面板按 F1 呼出）选中 “Log XMLHttpRequests” 选项，然后就可以在 “Console” 面板中看到请求了。点击此请求可以导航到Network选项卡上，当然也可以直接在Network选项卡中查看XHR HttpRequest请求，不过使用Console打印出来到导航还是要方便一些，多了一个入口。

\paragraph{脚本添加到黑盒}

当你使用 “Event listener breakpoint :: Mouse :: Click” 调试事件时，有很大的可能是代码在第三方的库中中断了（例如 jquery ），这时如果你想找到属于自己项目的调用需要在调试器里面点很多次的下一步才能看到。 一种很棒的方式就是把这些第三方的脚本放到黑盒里面，调试器永远不会在黑盒中的脚本内停止，它会一直向后运行，直到运行的代码行位于黑盒之外的文件。 你可以在调用栈面板中右键点击第三方脚本的文件名，然后左键在下拉菜单中点击 “Blackbox Script”。

\paragraph{定位按钮触发代码}

快速定位某个特定按钮或者链接点击后所触发的代码，只需要在你真正点击按钮之前，激活一个 “Event listener breakpoint”, 选中右侧面板下 mouse 下面的 click 确认框即可。

\paragraph{Async调试}

在Promise被广泛应用的今天，我们都知道，Promise的回调是异步执行的，没有开启Async模式前，调动栈只记录到回调函数本身，我们无法找到代码执行的顺序，这给我们调试带来巨大的困难。Async模式可以解决这个问题。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.9]{chromeasyncdebugging.png}
	\caption{Google Chrome异步调试}
	\label{fig:chromeasyncdebugging}
\end{figure}

\paragraph{查找成员函数}

Ctrl+Shif+O文件中定位成员函数。

\paragraph{打印对象数组}

在Javascript中可能返回有数组，有的时候可能某一个数组元素感觉数据不对，在Javascript调试跟踪时，输出了一些Object对象，如果是几个元素，还可以一个一个点开查看，如果有成百上千的元素，一个一个点开查看几乎是不可能的，至今没有发现在结果中按关键字搜索的功能，此时可以使用如下方法：

\begin{lstlisting}[language=Bash]
console.table(data);
\end{lstlisting}

即可以表格的形式显示所有元素，其中data表示一个对象或者数组，如图\ref{fig:googlechromeconsoletable}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{googlechromeconsoletable.png}
	\caption{Google Chrome查看对象数组}
	\label{fig:googlechromeconsoletable}
\end{figure}


\subsection{Source Map}

截止目前几乎没有浏览器完全原生支持es6标准，对于这种情况，Chrome引入了source-map文件，标识es5代码对应的转码前的es6代码哪一行，唯一要做的就是配置webpack自动生成source-map文件，这也很简单，在webpack.config.js中增加一行配置即可（需要重新启动webpack-dev-server使配置生效）


\subsection{Network}

\paragraph{Replay XHR}

有时在开发时，可以直接在Network中的网络请求列表中点击右键，在菜单中重复发送XHR请求(Replay XHR)，不需要再次进入界面模拟请求，可以提高不少效率。

\paragraph{Disable cache}

勾选上Disable cache后，浏览器不会利用本地的Javascript缓存，每次都会到服务器上重新取。

\paragraph{Preserve Log}

保留请求日志，勾选后，即使在当前页面刷新后，日志也不会清空。在使用large request row后，size和time会出现2行内容，如图\ref{fig:networksizeandtime}所示。其中时间行，上面的时间代表从请求开始到接收完最后一个byte为止的时间，下面一行是Latency，代表从请求接收完后到读取该资源第一个byte之间的等待时间。按照这个逻辑，那么上面一行的时间减去下面一行的时间就是数据的下载时间，将鼠标移动到右侧waterfall图标上，可以看到下载时间(Content Download)正好是上下两行的时间差。另外Size列也有2行，上面一行代表网络上实际传输的文件大小，下面一行代表原始的未经过压缩的文件大小。一般服务器会对资源使用Gzip进行压缩。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{networksizeandtime.png}
	\caption{Network宽行显示}
	\label{fig:networksizeandtime}
\end{figure}

\paragraph{DOMContentLoaded和load事件信息}

DOMContentLoaded和load这两个事件会高亮显示。DOMContentLoaded事件会在页面上DOM完全加载并解析完毕之后触发，不会等待CSS、图片、子框架加载完成。load事件会在页面上所有DOM、CSS、JS、图片完全加载完毕之后触发。DOMContentLoaded事件在Overview上用一条蓝色竖线标记，并且在Summary以蓝色文字显示确切的时间。load事件同样会在Overview和Requests Table上用一条红色竖线标记，在Summary也会以红色文字显示确切的时间。

\paragraph{资源的发起者(请求源)和依赖项}

通过按住Shift并且把光标移到资源名称上，可以查看该资源是由哪个对象或进程发起的（请求源）和对该资源的请求过程中引发了哪些资源（依赖资源）。在该资源的上方第一个标记为绿色的资源就是该资源的发起者（请求源），有可能会有第二个标记为绿色的资源是该资源的发起者的发起者，以此类推。在该资源的下方标记为红色的资源是该资源的依赖资源，如图\ref{fig:resourcedependency}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{resourcedependency.png}
	\caption{依赖资源}
	\label{fig:resourcedependency}
\end{figure}


\subsection{Elements}

在元素上，按住Ctrl + Alt + 鼠标左键，可以展开元素及下级元素的所有节点。


\subsection{Performance}



\subsection{Sources}

\paragraph{面板中查看变量}

在调试的过程中，虽然鼠标可以移动到变量上会自动Pop出对应的变量，但是有时候鼠标移出区域后变量也会自动消失。那么此时可以在右侧面板中的Scope查看变量，如图\ref{fig:sourcepanelscopevariable}所示：

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.4]{sourcepanelscopevariable.png}
	\caption{Devtool Scope中查看变量}
	\label{fig:sourcepanelscopevariable}
\end{figure}



\paragraph{Snippets}

\paragraph{Others}

\subparagraph{查看Render区域}

有时需要查看浏览器重绘制的区域，页面的绘制时间（paint time）是每一个前端开发都需要关注的的重要指标，它决定了你的页面流畅程度。而如何去观察页面的绘制时间，找到性能瓶颈，可以借助Chrome的开发者工具，激活页面绘制区域显示如图\ref{fig:pagepainting}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{pagepainting.png}
	\caption{激活页面绘制区域高亮}
	\label{fig:pagepainting}
\end{figure}

\subsection{Audits}



\subsection{Google Chrome SSH}

\section{Zentao}

\subsection{常见问题}

运行禅道需要PHP环境，在Ubuntu 16.04上面安装PHP：

\begin{lstlisting}[language=Bash]
sudo apt-get install php7.0 -y
\end{lstlisting}

由于80端口已经被占用，修改禅道的默认端口，在文件文件中。

\begin{lstlisting}
/opt/zbox/etc/apache/http.conf
\end{lstlisting}

由于以前电脑有安装MySQL，所以3306端口被占用了。所以在启动禅道是会提示MySQL端口已经被占用，执行如下命令修改禅道的MySQL端口即可：

\begin{lstlisting}[language=Bash]
/opt/zbox/zbox -mp 3307
\end{lstlisting}




\chapter{LaTex}

\subsection{安装}

\begin{lstlisting}[language=Bash]
# Ubuntu下安装Texlive
sudo apt-get install texlive -y
\end{lstlisting}

\subsection{字体}

Computer Modern是自由软件TeX的默认字体，为美国计算机科学家高德纳（Donald Knuth）使用METAFONT软件创造。但是此字体不是很漂亮，所以考虑换一个字体 。

\subsection{版面}

\paragraph{换行}Latex的自动拆单词换行是依靠字典的。如果遇到\LaTeX{}不认识的单词，他就不会换行了。后面的内容就会溢出。

\subsection{支持语言}

Latex原生不支持JavaScript语言，添加如下定义：

\begin{lstlisting}[language=Tex]
%lstlisting包JavaScript语言设置
\lstdefinelanguage{JavaScript}{
	keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
	keywordstyle=\color{blue}\bfseries,
	ndkeywords={class, export, boolean, throw, implements, import, this},
	ndkeywordstyle=\color{darkgray}\bfseries,
	identifierstyle=\color{black},
	sensitive=false,
	comment=[l]{//},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{purple}\ttfamily,
	stringstyle=\color{red}\ttfamily,
	morestring=[b]',
	morestring=[b]"
}
\end{lstlisting}

即可在文档中定义JavaScript，并支持关键字颜色区分。

\part{DB}

\chapter{性能(Performance)}

\section{索引(Index)}

\subsection{索引常识}

绝大多数情况下，数据库只使用一个索引。在开发中遇到有时根据时间查询很快：

\begin{lstlisting}[language=SQL]
--在列org\_id和jdrq上都建立了索引
--同样都查询语句，不同都org\_id值会有不同都查询性能
--当org\_id结果集很少当时候，数据库会使用org\_id索引
--在使用了org\_id索引后，不会使用jdrq索引
--同理，当org\_id结果集更大，根据jdrq索引效率更高时
--数据库会使用jdrq列作为索引，而不会使用org\_id索引
select *
from table
where org_id = '1000'
order by jdrq desc
\end{lstlisting}

所以需要同时利用多个列的索引，或者查询条件包含多列时，应该使用联合索引。

\begin{lstlisting}[language=SQL]
create index idx_ts_b_xzxk_jdrq_org_id on xzxk(jdrq desc,org_id);
\end{lstlisting}

\paragraph{B+树}

IO次数取决于b+数的高度h(height)，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有

\begin{equation}
h=\log_{(m+1)} N
\end{equation}


当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。

当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性\footnote{\url{参考链接:http://tech.meituan.com/mysql-index.html}}。

\paragraph{索引原则}

\subparagraph{最左前缀匹配原则}最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

\subparagraph{=和in可以乱序}=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式

\subparagraph{尽量选择区分度高的列作为索引}尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录

\subparagraph{索引列不能参与计算}索引列不能参与计算，保持列“干净”，比如from\_unixtime(create\_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create\_time = unix\_timestamp(’2014-05-29’);

\subparagraph{尽量的扩展索引}尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可

\subsection{索引类型}

\subparagraph{位图索引(Bitmap Index)}

相对于B*Tree索引,占用的空间非常小,创建和使用非常快。位图索引由于只存储键值的起止Rowid和位图,占用的空间非常少。B*Tree索引由于不记录空值,当基于is null的查询时,会使用全表扫描,而对位图索引列进行is null查询时,则可以使用索引。当select count(XX) 时,可以直接访问索引中一个位图就快速得出统计数据。当根据键值做and,or或 in(x,y,..)查询时,直接用索引的位图进行或运算,快速得出结果行数据。

\subparagraph{散列索引Or哈希索引(Hash Index)}

散列索引是根据HASH算法来构建的索引，所以检索速度很快，但不能范围查询。散列索引的特点，只适合等值查询（包括= <> 和in），不适合模糊或范围查询

\subparagraph{聚集索引(Cluster Index)}

聚集索引确定表中数据的物理顺序。聚集索引类似于电话簿，后者按姓氏排列数据。由于聚集索引规定数据在表中的物理存储顺序，因此一个表只能包含一个聚集索引。但该索引可以包含多个列（组合索引），就像电话簿按姓氏和名字进行组织一样。 

\subparagraph{非聚集索引(Non-Cluster Index)}

它并不决定数据在磁盘上的物理排序，索引上只包含被建立索引的数据，以及一个行定位符row-locator，这个行定位符，可以理解为一个聚集索引物理排序的指针，通过这个指针，可以找到行数据。

\subparagraph{全文索引(Full-text Search)}

全文索引是基于要编制索引的文本中的各个标记来生成倒排序、堆积且压缩的索引结构。每个表或索引视图只允许有一个全文索引。该索引最多可包含 1024 列。，该对象中必须有一唯一并且非空的列。 

\subparagraph{联合索引}

联合索引能够满足最左侧查询需求，例如(a, b, c)三列的联合索引，能够加速a | (a, b) | (a, b, c) 三组查询需求。这也就是为何不建立(passwd, loginName)这样联合索引的原因，业务上几乎没有passwd的单条件查询需求，而有很多loginName的单条件查询需求。

\subsection{like查询优化}

目前数据量将近千万，模糊查询效率比较低。模糊查询通常有下面三种，前匹配：like \%abc，可以使用btree索引优化，后匹配：like abc\%，可以使用reverse函数btree索引，全匹配：like \%abc\%，可以使用pg\_trgm的gin索引。后面的2种优化方式是在PostgreSQL数据库中。还有全文索引技术也可以用来优化基于like查询的匹配。

\paragraph{采用函数}

采用函数来优化查询(没有效果)：

\begin{lstlisting}[language=SQL]
--120W数据平均2.3秒左右，比原生查询稍慢
SELECT `column` FROM `table` WHERE LOCATE('keyword', `field`)>0

select * 
from "table" 
where locate('陶然居',mc)>0
and code is not null
limit 0,10;

--120W数据平均1.8秒左右，与原生查询时间相仿
SELECT `column` FROM `table` WHERE POSITION('keyword' IN `filed`)

select * 
from "table" 
where position('西南铝业' in mc)
and code is not null
limit 0,10;

--120W数据平均1.8秒左右，与原生查询时间相仿
SELECT `column` FROM `table` WHERE INSTR(`field`, 'keyword' )>0

select * 
from "table" 
where INSTR('西南铝业' in mc)>0
and code is not null
limit 0,10;
\end{lstlisting}

\paragraph{全文索引(Full-text Search)}

在达梦数据库中创建全文索引：

\begin{lstlisting}[language=SQL]
create context index qymcindex on user.TS_F_CORPORATION_TEST(qymc) lexer CHINESE_VGRAM_LEXER;
\end{lstlisting}

填充索引：

\begin{lstlisting}[language=SQL]
alter context index qymcindex1 on user_xycq.TS_F_CORPORATION rebuild;
\end{lstlisting}

删除索引：

\begin{lstlisting}[language=SQL]
drop context index qymcindex on user_xycq.TS_F_CORPORATION_TEST;
\end{lstlisting}

达梦数据库中，在写全文索引查询时，全文索引的条件需要放到前面，这样才能够利用到索引。

\section{Redis}

\subsection{安装}

这里是在Raspberry Pi上安装Redis：

\begin{lstlisting}[language=Bash]
# 下载源码
wget -c http://download.redis.io/releases/redis-3.2.9.tar.gz
# 编译安装
make
make test
\end{lstlisting}

\chapter{PostgreSQL}

PostgreSQL是自由的对象-关系型数据库服务器（数据库管理系统），在灵活的BSD-风格许可证下发行。它在其他开放源代码数据库系统（比如MySQL和Firebird），和专有系统比如Oracle、Sybase、IBM的DB2和Microsoft SQL Server之外，为用户又提供了一种选择。

\section{基础操作}

登录：

\begin{lstlisting}[language=Bash]
sudo -u postgres psql
\end{lstlisting}

这里是以超级用户的身份登录，否则在修改密码时会提示：must be superuser to alter superusers。修改密码：

\begin{lstlisting}[language=Bash]
\password postgres
\end{lstlisting}

Gradle引用驱动：

\begin{lstlisting}[language=Bash]
compile "org.postgresql:postgresql:42.1.1"
\end{lstlisting}
       
PostgreSQL新建列时，列名推荐使用小写，列名使用小写时，select里大小写都可以查询，如果列名为大写，查询语句中列名需要添加双引号才能查询。

\chapter{MySQL}

\subsection{查询}

\paragraph{MySQL远程连接}

本来以为安装完毕MySQL就可以自动进行远程连接，结果竟然花了半天时间才搞定。安装完毕后MySQL默认是不能远程连接的，由于在其他主机上用nmap无法扫描到端口3306，开始还以为是防火墙的缘故，在错误的道路上研究了许久，最后竟然在本机也无法扫描到端口。才明白MySQL是与localhost绑定而不是与IP绑定。在目录/etc/mysql/目录中修改配置文件my.cnf：

\begin{lstlisting}[language=Bash]
# Instead of skip-networking the default is now to listen only on
# localhost which is more compatible and is not less secure.
bind-address            = 192.168.31.25
\end{lstlisting}

默认是127.0.0.1,是一个回环地址,只有本机请求本机,目标地址才会是127.0.0.1,此时需要在其他主机请求，需要将绑定IP改为网卡地址, 也就是外网IP, 因为远程连接数据库目标IP肯定是公网IP. 不过还可以改成0.0.0.0,这样就表示监听所有IP, 只要端口一样, 网卡都会把请求传给进程。如果不在配置文件中修改地址，则需要在启动时指定绑定地址：

\begin{lstlisting}[language=Bash]
sudo mysqld --bind-address=192.168.31.25
\end{lstlisting}

绑定好地址后，使用命令sudo lsof:3306查看。如果填写的绑定地址是192开头，那么最终只有192开头的局域网里的机器能够远程连接MySQL，有时一台设备可能会处于多个局域网中，那么此时另一个局域网的设备就无法远程连接MySQL了。此时可以将绑定地址修改为0.0.0.0，即允许所有的IP连接到MySQL，相应的MySQL的安全性相应的就降低了。

\subsection{Mac中MySQL初始密码}

step1：

苹果->系统偏好设置->最下边点mysql 在弹出页面中 关闭mysql服务（点击stop mysql server）

step2：

进入终端输入：cd /usr/local/mysql/bin/
回车后 登录管理员权限 sudo su
回车后输入以下命令来禁止mysql验证功能 ./mysqld\_safe --skip-grant-tables \&
回车后mysql会自动重启（偏好设置中mysql的状态会变成running）

step3. 
输入命令 ./mysql
回车后，输入命令 FLUSH PRIVILEGES; 
回车后，输入命令 SET PASSWORD FOR 'root'@'localhost' = PASSWORD('你的新密码');

\subsection{基础}

启动MySQL：

\begin{lstlisting}[language=Bash]
mysqld

# Mac中启动MySQL，同理停止的参数为stop，重启的参数为restart
sudo /usr/local/MySQL/support-files/mysql.server start

# 防火墙添加端口
sudo iptables -A INPUT -p tcp --dport 3306 -j ACCEPT /*允许包从3306端口进入*/
sudo iptables -A OUTPUT -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT /*允许从3306端口进入的包返回*/

# 查看iptables
sudo iptables --list
SHOW DATABASES                                //列出 MySQL Server 数据库。
SHOW TABLES [FROM db_name]                    //列出数据库数据表。
SHOW TABLE STATUS [FROM db_name]              //列出数据表及表状态信息。
SHOW COLUMNS FROM tbl_name [FROM db_name]     //列出资料表字段
SHOW FIELDS FROM tbl_name [FROM db_name]，DESCRIBE tbl_name [col_name]。
SHOW FULL COLUMNS FROM tbl_name [FROM db_name]//列出字段及详情
SHOW FULL FIELDS FROM tbl_name [FROM db_name] //列出字段完整属性
SHOW INDEX FROM tbl_name [FROM db_name]       //列出表索引。
SHOW STATUS                                  //列出 DB Server 状态。
SHOW VARIABLES                               //列出 MySQL 系统环境变量。
SHOW PROCESSLIST                             //列出执行命令。
SHOW GRANTS FOR user                         //列出某用户权限
GRANT ALL PRIVILEGES ON *.* TO 'root'@'192.168.31.25'IDENTIFIED BY 'dolphin' WITH GRANT OPTION;
\end{lstlisting}

在使用MySQL会出现中文乱码问题，MySQL的字符集支持(Character Set Support)有两个方面：字符集(Character set)和排序方式(Collation)。对于字符集的支持细化到四个层次: 服务器(server)，数据库(database)，数据表(table)和连接(connection)。在Mac中，在etc目录下新建my.cnf文件，指定数据库编码：

\begin{lstlisting}[language=Bash]
[mysqld]
character-set-server=utf8
init_connect='SET NAMES utf8'
[mysql]
default-character-set=utf8
\end{lstlisting}

修改后，重新启动数据库。但是以前新建的表有可能还是采用的原来的编码，使用如下语句进行修改：

\begin{lstlisting}[language=SQL]
--检查数据表所有字段的状态	
show full columns from book; 
--发现address字段的Collation项非utf8，改成utf8
alter table book change publisher publisher varchar(512) character set utf8 collate utf8_unicode_ci not null;
\end{lstlisting}

\subsection{备份}

netstat -ln | grep mysql

mysqldump --sock=/var/run/mysqld/mysqld.sock -u root -p dolphin>dolphin.sql

\subsection{存储过程}

\begin{lstlisting}[language=SQL]
DELIMITER $$

CREATE DEFINER=`root`@`%` PROCEDURE `insert_initial_id`()
BEGIN  

DECLARE i INT DEFAULT 1;# can not be 0  
DECLARE j INT DEFAULT 10000000;# can not be 0  


WHILE i<99999999 && j < 10000050
DO  
insert into douban_book_id(isscapy,douban_book_id) values (0,j);  
SET i=i+1;  
set j=j+1;
END WHILE ;  
commit;  

END$$
DELIMITER ;
\end{lstlisting}


\subsection{mycli}

MyCli 是一个 MySQL 命令行工具，支持自动补全和语法高亮。也可用于 MariaDB 和 Percona。安装mycli：

\begin{lstlisting}[language=Bash]
pip install mycli
brew install mycli
\end{lstlisting}

使用MyCli连接数据库示例如下：

\begin{lstlisting}[language=Bash]
mycli -h 10.0.0.14 -u root -p dolphin
\end{lstlisting}

在Ubuntu下远程配置如图\label{raspberryremtoeconfig}所示：

\subsection{常见问题}

\paragraph{ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock'}

mysql使用unix socket或者tcp来连接数据库进行通讯，默认不加 -h选项时使用的就是localhost即unixsocket，此时会通过/tmp/mysql.sock来通讯，但是在配置文件中默认生成的socket文件是在/var/lib/mysql/mysql.sock(不同安装可能不同，建议查看/etc/my.cnf确认),所以要想mysql使用这个文件通讯，最简单的方法就是建立软链接，一劳永逸，此为方法一

方法二就是强制mysql使用tcp通讯，因为127.0.0.1对于mysql来说走的是tcp协议而非unixsocket，这种方法的弊端就是每次都要指明本地地址127.0.0.1

\subsection{中文查询}

在用MyBatis查询中文时，出来的结果却不是中文匹配的结果，将SQL在客户端工具中执行却可以正确查询。在MySQL配置文件中修改相应的编码。在服务端/etc/mysql/my.cnf修改相应编码：

\begin{lstlisting}[language=Bash]
[mysqld]

character-set-server=utf8
\end{lstlisting}




\chapter{达梦}

\section{SQL}

\subsection{索引}

全文索引未生效时，可以到CTISYS模式下查看系统所有的全文索引：

\begin{lstlisting}[language=SQL]
select * 
from "CTISYS"."SYSCONTEXTINDEXES";
\end{lstlisting}

新建索引时，选择独立的索引存储空间，最好是索引存储与数据存储独立，便于管理。每个用户建立一个数据表空间，一个索引表空间。采用函数的列应该保存在索引中。添加上(a,b,c)的组合索引相当于添加了，超过3个列的联合索引不合适，否则虽然减少了回表动作，但索引块过多，查询时就要遍历更多的索引块了，建索引动作应谨慎，因为建索引的过程会产生锁，不是行级锁，而是锁住整个表，任何该表的DML操作都将被阻止，在生产环境中的繁忙时段建索引是一件非常危险的事情。

在达梦客户端中，按下Ctrl + Alt + L会出来所有快捷键的面板，不得不说这个设计还是非常贴心的，开始真的不敢相信自己的眼睛，反复确认了真的不是Intellij Idea调出的面板，是达梦调出来的，非常不错的设计。注意平台是Ubuntu 16.04 LTS。

\subsection{函数}

根据条件来计算：

\begin{lstlisting}[language=SQL]
SELECT ORG_ID,
		ORG_NAME,
		INTERFACE_NAME,
		COUNT(*) AS TOTAL,
		SUM(CASE WHEN RETURN_COUNT > 0 THEN 1 ELSE 0 END) AS VALID_TIMES,
		ROUND(CONVERT(DOUBLE,SUM(CASE WHEN RETURN_COUNT > 0 THEN 1 ELSE 0 END))/CONVERT(DOUBLE,COUNT(*)),3) AS RATIO
FROM TD_M_APPACCESS_LOG
GROUP BY ORG_ID,ORG_NAME,INTERFACE_NAME
\end{lstlisting}

如上语句统计接口的有效调用次数，返回数量(RETURN\_COUNT)大于0时，视为有效调用次数(VALID\_TIMES)。数量默认为整数，所以要得到小数的比率(RATIO)，就需要使用函数convert将数据转换为Double类型。

\subsection{常用SQL记录}

\begin{lstlisting}[language=SQL]
select GNMK,
max(ORG_NAME) as ORG_NAME,
ORG_ID,
COUNT(*) AS USING_TIMES,
SUM(CASE WHEN ISOKRED > 0 OR ISBLACKRED > 0 THEN 1 ELSE 0 END) AS VALID_TIMES,
SUM(CASE WHEN ISOKRED > 0 THEN 1 ELSE 0 END) AS VALID_COUNT_RED,
SUM(CASE WHEN ISBLACKRED > 0 THEN 1 ELSE 0 END) AS VALID_COUNT_BLACK
from (
	SELECT GNMK,
	MAX(ORG_NAME) AS ORG_NAME,
	ORG_ID,
	sum(ISOKRED) as ISOKRED,
	sum(ISOKBALCK) AS ISBLACKRED,
	RESPONSEKEY	
	FROM TI_B_REDBALCKLOG  
	GROUP BY RESPONSEKEY,ORG_ID,GNMK 
)
GROUP BY ORG_ID,GNMK 
order by ORG_ID DESC limit 0, 10
\end{lstlisting}


\section{通用知识}

\subsection{查询优化}

最近使用数据库中，遇到查询性能低下的问题。数据量大概在800W左右，但是分页查询需要40s左右才能够查询出来结果，测试库200W数据量左右，分页查询也需要10s才能出结果。

使用MySQL，有的人BB说5.1以前300w就会很明显，5.5-5.6已经有很大优化，如果是5.7千万也不是问题。随着互联网的发展，数据的量级也是撑指数的增长，从GB到TB到PB。对数据的各种操作也是愈加的困难，传统的关系性数据库已经无法满足快速查询与插入数据的需求。这个时候NoSQL的出现暂时解决了这一危机。它通过降低数据的安全性，减少对事务的支持，减少对复杂查询的支持，来获取性能上的提升。但是，在有些场合NoSQL一些折衷是无法满足使用场景的，就比如有些使用场景是绝对要有事务与安全指标的。这个时候NoSQL肯定是无法满足的，所以还是需要使用关系性数据库。

\paragraph{增加缓存}增加缓存可以提高查询效率，但是在对数据表做任何的更新操作(update/insert/delete)等操作，server为了保证缓存与数据库的一致性，会强制刷新缓存数据，导致缓存数据全部失效。

\paragraph{分库分表}

分库分表应该算是查询优化的杀手锏了。上述各种措施在数据量达到一定等级之后，能起到优化的作用已经不明显了。这个时候就必须对数据量进行分流。分流一般有分库与分表两种措施。而分表又有垂直切分与水平切分两种方式。下面我们就针对每一种方式简单介绍。

对于mysql，其数据文件是以文件形式存储在磁盘上的。当一个数据文件过大的时候，操作系统对大文件的操作就会比较麻烦与耗时，而且有的操作系统就不支持大文件，所以这个时候就必须分表了。另外对于mysql常用的存储引擎是Innodb，它的底层数据结构是B+树。当其数据文件过大的时候，B+树就会从层次和节点上比较多，当查询一个节点的时候可能会查询很多层次，而这必定会导致多次IO操作进行装载进内存，肯定会耗时的。除此之外还有Innodb对于B+树的锁机制。对每个节点进行加锁，那么当更改表结构的时候，这时候就会树进行加锁，当表文件大的时候，这可以认为是不可实现的。

\paragraph{分区表}

分区表是MySql 5.1引入的特性。根据官网alter-table-partition-operations的介绍，其本质是将分库分表直接集成到MySql中。我们知道，传统的分库分表功能，存在业务层、中间件、数据库三层：业务层通过调用中间件的API访问数据库，不知道具体的物理存储细节；中间件将一张很大的逻辑表映射到数据库中多张较小的物理表，并对业务层的访问请求进行分解后分别放到对应物理库中执行，再将执行结果在中间件合并后返回给业务层，从而对业务层屏蔽物理存储细节；数据库则提供实际的物理存储。而MySql的分区表，借助MySql本身的逻辑架构，将分库分表功能进行了下沉。MySql逻辑架构中的客户端即对应业务层，Server层对应中间件层，存储引擎层对应物理存储层。简单的说，分库表就是我们在数据库层面看到是一张表，但物理上是分成多个文件独立存储。逻辑上分析，分区表的优点很明显：既能解决大数据量的性能问题，又能对应用层无缝切换。

\paragraph{采用NoSQL数据库}

Hbase读的性能比较强，官方定义为适合PB级数据的秒级查询，就是说在巨海量的数据中查询，响应速度非常快。

\subsection{系统架构}

从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing) 

\subsection{注意事项}

\paragraph{不要ORDER BY RAND()}MySQL会不得 不去执行RAND()函数（很耗CPU时间），而且这是为了每一行记录去记行，然后再对其排序。就算是你用了Limit 1也无济于事。自己在做爬虫时，写过这样的语句：

\begin{lstlisting}[language=SQL]
select * from douban_book_id where isscapy = 0 limit 1
\end{lstlisting}

难怪后来变的很慢，把中间停顿的sleep函数去掉之后还是很慢，开始还一位是sleep函数暂停的问题。

\part{Network}

\chapter{WebService}

\section{Axis2}

目前开发Web Service的几个框架，分别为Axis，axis2，Xfire，CXF以及JWS(也就是前面所述的JAX-WS，这是Java6发布所提供的对Web Service服务的一种实现。)前面几项都为开源项目，而其中又以axis2与cxf所最为常用，Axis与XFire已随着技术不断的更替慢慢落幕，而目前也只有axis2和cxf官方有更新，Axis与XFire都已不再更新。

\subsection{Axis2 Web Service}

在填写Web Service地址时，提示url is not valid,地址是\url{http://www.webxml.com.cn/WebServices/WeatherWebService?wsdl}，而地址\url{http://www.webxml.com.cn/WebServices/WeatherWebService.asmx?wsdl}是有效地址。

\chapter{HTTP}


\subsection{性能分析}

\paragraph{慢的SQL查询}

在项目中报表打印页面非常慢，需要10几秒才能出结果。打开Network，找到所有请求按照时间排列发现耗时最多的一个请求，针对此请求进行优化。在SQL中添加相应的索引可以解决。

\paragraph{大的Javascript脚本}在项目中，发现有的JavaScript脚本特别大，有6MB+之巨，明显是造成性能瓶颈的原因之一，如图\ref{fig:largejavasript}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{largejavasript.png}
	\caption{较大的Javascript脚本}
	\label{fig:largejavasript}
\end{figure}

\subparagraph{去掉Javascript注释}

在webpack.config.js文件的plugins数组里面添加及配置插件即可。

\begin{lstlisting}[language=Javascript]
new UglifyJsPlugin({
	output: {
		comments: false,//去掉Javascript中的注释
	},
	compress: {
		warnings: false
	}
})
\end{lstlisting}

去掉注释后小了400KB左右，但是还是有6MB，没有从根本上解决问题。

\subparagraph{添加插件}

添加一些插件压缩包。

\begin{lstlisting}[language=Javascript]
plugins: [
	new webpack.DefinePlugin({ // <-- key to reducing React's size
		'process.env': {
		'NODE_ENV': JSON.stringify('production')
	}
	}),
	new webpack.optimize.DedupePlugin(), //dedupe similar code 
	new webpack.optimize.UglifyJsPlugin(), //minify everything
	new webpack.optimize.AggressiveMergingPlugin()//Merge chunks 
]
\end{lstlisting}

\subparagraph{webpack-bundle-analyzer}

\url{http://www.reactpeixun.com/reactganhuo/2017-06-05/304.html?nsukey=Oif9ieNi7kisvaq%2B0aDH46Tl37niZVItuq2VKmhdkz7Z%2F%2F2PmtYFX6EdwDLkcMg8ajYX9XxOUXO5ykezqUerb45blihPXZ3BgEZSQUlk25d4qkfLibGwVsIWSBdG9ByTmNG11Da14liYOc%2BB9tmOVI1FbAE%2FohPpzfc2kVRHQMk%3D}

\subparagraph{React切换到产品环境}

\subparagraph{查看Webpack输出}

最最直接的，可以直接阅读Webpack的输出，里面有模块的文件大小，在package.json文件中添加--display-chunks参数，配置如下：

\begin{lstlisting}[language=Javascript]
"scripts": {
	"dev": "webpack --display-chunks --watch --progress --colors --display-error-details --config webpack/dev.config.js",
}
\end{lstlisting}


输出如图\ref{fig:webpackcheckingfilesize}所示。也可以在输出中查找是否包含重复依赖。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{webpackcheckingfilesize.png}
	\caption{Webpack查看输出文件大小}
	\label{fig:webpackcheckingfilesize}
\end{figure}


\subparagraph{利用浏览器缓存}

有时优化之后Javascript的大小还是不够理想，此时可以考虑将Javascript的文件名用Hash进行标识，Hash是经过文件计算出来的唯一值，在文件有改动后，会自动更新，浏览器仅仅在首次请求时下载较大的Javascript脚本，以后就可以直接利用本地缓存。此种方式不需要频繁的访问服务器下载Javascript脚本，提升 本地加载速度的同时也减轻了服务端的压力，如图\ref{fig:scriptusingdiskcache}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.45]{scriptusingdiskcache.png}
	\caption{Javascript脚本利用浏览器缓存}
	\label{fig:scriptusingdiskcache}
\end{figure}

由于应用是SPA(Single Page Application)，上图中单单一个main.js脚本都会有5MB之巨，加上其他的大大小小的Javascript脚本，在未经过服务端压缩的情况下会有10MB以上的大小，在普通的网络环境下肯定是非常慢。如果下载速度时1MB/s，打开应用至少需要10s，将是用户无法忍受的。

\subparagraph{开启Gzip压缩}

同时可以考虑在服务端开启Gzip压缩，開啓Gzip壓縮配置如下：

\begin{lstlisting}[language=Bash]
#僅僅開啓Gzip還不行，需要各項詳細配置
gzip on;
gzip_min_length 1k;
gzip_buffers 4 16k;
gzip_comp_level 2;
gzip_types application/javascript application/json text/plain application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;
gzip_vary off;
gzip_disable "MSIE [1-6]\.";
\end{lstlisting}

开启服务端压缩后效果非常显著，如图\ref{fig:openservercompress}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.5]{openservercompress.png}
	\caption{开启服务端压缩效果}
	\label{fig:openservercompress}
\end{figure}

图中，globalInfo请求，压缩之前返回的Json有1.4MB大小，压缩之后只有179KB，压缩之前传输时间在1.5s左右，压缩之后传输时间缩短了三分之二。另外比较大的6MB的Javascript脚本也变成了1.2MB大小。

\paragraph{字体文件加载}

發現一個字體文件(fontawesome-webfont.woff2)在Linux下的Chrone中是從磁盤中加載(from disk cache)的，花費了220ms左右。在Windows平臺下的360浏览器中是從内存中加載(from cache)的。diskCache顾名思义，就是将资源缓存到磁盘中，等待下次访问时不需要重新下载资源，而直接从磁盘中获取，它的直接操作对象为CurlCacheManager。它与memoryCache最大的区别在于，当退出进程时，内存中的数据会被清空，而磁盘的数据不会，所以，当下次再进入该进程时，该进程仍可以从diskCache中获得数据，而memoryCache则不行。diskCache与memoryCache相似之处就是也只能存储一些派生类资源文件。它的存储形式为一个index.dat文件，记录存储数据的url，然后再分别存储该url的response信息和content内容。Response信息最大作用就是用于判断服务器上该url的content内容是否被修改。

\paragraph{耗时长的请求}

查看首页中一个耗时最长的请求，一般情况下耗时都稳定在1s左右。如图\ref{fig:longtimerequest}所示。可以看出挂起(stalled)304ms，等待服务端响应581ms，此两项的时间是最长的。Stalled从HTTP连接建立到请求能够被发出送出去(真正传输数据)之间的时间花费。包含用于处理代理的时间，如果有已经建立好的连接，这个时间还包括等待已建立连接被复用的时间。挂起时间长是由于浏览器对同一个主机域名的并发连接数有限制，因此如果当前的连接数已经超过上限，那么其余请求就会被阻塞，等待新的可用连接；此外脚本也会阻塞其他组件的下载。另外在不同的平台上挂起时间时长也不一定，在Windows平台的360浏览器上挂起时长(stalled)仅仅为0.458ms。

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.6]{longtimerequest.png}
	\caption{耗时长的请求}
	\label{fig:longtimerequest}
\end{figure}

针对服务器端耗时长的请求，使用VisualVM查看服务端调用的耗时最长的方法。VisualVM 是一款免费的性能分析工具。它通过 jvmstat、JMX、SA（Serviceability Agent）以及 Attach API 等多种方式从程序运行时获得实时数据，从而进行动态的性能分析。同时，它能自动选择更快更轻量级的技术尽量减少性能分析对应用程序造成的影响，提高性能分析的精度。

\subsection{Session}

当浏览器第一次请求时，服务器创建一个session对象，同时生成一个sessionId，并在此次响应中将sessionId 以响应报文的方式传回客户端浏览器内存或以重写url方式送回客户端，来保持整个会话。关闭此浏览器窗口，其内存中的sessionId也就随之销毁。session.invalidate()是将session设置为失效，一般在退出时使用，但要注意的是：session失效的同时 浏览器会立即创建一个新的session的，你第一个session已经失效了 所以调用它的getAttribute方法时候一定会抛出NullPointerException。jsessionid只是tomcat中对session id的叫法，在其它容器里面，不一定就是叫jsessionid。


\section{PAC}

\subsection{PAC简介}

PAC是proxy auto-config的缩写。PAC文件是纯文本格式的，实际上就是JavaScript文件。PAC最简单的格式就是包含一个叫FindProxyForURL的JavaScript函数，IE通过传入两个变量来调用这个函数。一个是用户流量的地址的URL全路径，一个是这个URL中的主机名host。在日常生活中浏览器中设置代理很简单，但是当你来回切换时总是觉得很烦，你可以使用pac脚本自动判断是否走代理，方便省去了来回手动切换的烦恼。

\subsection{PAC实例}

一个最简单的PAC脚本：

\begin{lstlisting}[language=JavaScript]
function FindProxyForURL(url,host){
	return "DIRECT";
}
\end{lstlisting}

参数url是用户输入的url，参数host是url中的主机名。PAC文件返回值有三种类型：

\begin{itemize}
	\item{DIRECT直连不通过代理}
	\item{PROXY www.lybbn.cn:8080 http通过8080端口代理上网，也可以使用ip:port的形式}
	\item{SOCKS5 www.lybbn.cn:8080 socks通过8080端口代理上网，可以使用ip:port形式}
\end{itemize}

\begin{lstlisting}[language=VBScript]
var FindProxyForURL = function(init, profiles) {
	return function(url, host) {
		"use strict";
		var result = init, scheme = url.substr(0, url.indexOf(":"));
		do {
			result = profiles[result];
			if (typeof result === "function") result = result(url, host, scheme);
		} while (typeof result !== "string" || result.charCodeAt(0) === 43);
		return result;
	};
}("+dolphin2", {
	"+dolphin2": function(url, host, scheme) {
		"use strict";
		if (/^59\.214\.215\.6$/.test(host)) return "+dolphin-proxy";
		if (/^10\.10\.1\.11$/.test(host)) return "+dolphin-proxy";
		return "DIRECT";
	},
	"+dolphin-proxy": function(url, host, scheme) {
		"use strict";
		if (/^127\.0\.0\.1$/.test(host) || /^::1$/.test(host) || /^localhost$/.test(host)) return "DIRECT";
		return "PROXY 10.55.10.2:8888";
	}
});
\end{lstlisting}

以上脚本说明，当IP为59.214.215.6或10.10.1.11时，使用代理dolphin-proxy，而代理dolphin-proxy设置的是代理机器的相关信息，代理机器的IP为10.55.10.2，代理的端口是8888。

\section{dddd}


\part{Engneering}

\section{通用规范}

\subsection{版本管理}

在项目的开发过程中，要求有明确的版本号，以前只是在正式发布版本的时候才会生成带版本号的文件，目前测试需要每次发布版本修改时定义版本号，方便测试进行问题识别。以前直接使用脚本生成的带有年月日的版本标志，但是还是需要进一步规范化。由于发布采用的自动化脚本，在启动程序时需要指定明确的文件名称，文件名称与版本号有关联，所以考虑将版本号保存到一个文件里面，使用Gradle构建程序和发布程序时都读取此文件中的版本号。Given a version number MAJOR.MINOR.PATCH, increment the:

\begin{itemize}
	\item{MAJOR version when you make incompatible API changes,}
    \item{MINOR version when you add functionality in a backwards-compatible manner}
	\item{PATCH version when you make backwards-compatible bug fixes.}
\end{itemize}

Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.在项目的根目录下新建version.properties文件，文件中定义版本号：

\begin{lstlisting}
VERSION_CODE=1.0.2
\end{lstlisting}

在Gradle中读取版本号：

\begin{lstlisting}
version = getVersionCode()

def getVersionCode() {
	def versionFile = file('/Users/dolphin/source/credit-system/gradle/version.properties')
	if (versionFile.canRead()) {
		def Properties versionProps = new Properties()
		versionProps.load(new FileInputStream(versionFile))
		def versionCode = versionProps['VERSION_CODE'].toString()
		/*def runTasks = gradle.startParameter.taskNames        //仅在assembleRelease任务是增加版本号
		if ('assembleRelease' in runTasks) {
			versionProps['VERSION_CODE'] = (++versionCode).toString()
			versionProps.store(versionFile.newWriter(), null)
		}*/
		return versionCode
	} else {
		throw new GradleException("Could not find version.properties!")
	}
}
\end{lstlisting}

此处文件路径写的是绝对路径，如果仅仅在本机进行构建问题不大，但是一般都是团队协作，不同的成员使用的操作系统有区别，项目存放的路径也不一样，所以写成相对路径会比较妥当。可以写成如下：

\begin{lstlisting}
File versionFile = file("$rootDir/cc-web-boot/src/main/resource/version.properties")
\end{lstlisting}

其中\$rootDir表示项目的根目录，比如有一个叫做dolphin的项目，那么rootDir就是dolphin目录，另外还有一个projectDir的变量和buildDir的变量。projectDir就是dolphin/src目录，buildDir就是dolphin/src/build目录。如果是多个Project组成的项目，那么projectDir就是rootDir/projectName，buildDir就是dolphin/projectName/build。




\end{document}
\theend
